{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Furac\u00f5es no Atl\u00e2ntico No momento de confec\u00e7\u00e3o desse trabalho, o furac\u00e3o Laura estava atingindo Lousiana, o estado americano, causando a morte de pelo menos 8 pessoas e um preju\u00edzo de aproximadamente 9 bilh\u00f5es de dol\u00e1res. Olhando esse caso e considerando que casos como esse acontecem todo ano, tempestades tropicais s\u00e3o, variando com a intensidade, um problema para a humanidade tanto do ponto de vista econ\u00f4mico quanto do humanit\u00e1rio. Colocado esse problema, nossa proposta \u00e9 analisar o comportamento de curto prazo de furac\u00f5es e achar dire\u00e7\u00f5es que nos levem a entender como podemos minimizar esses constantes danos de furac\u00f5es.","title":"Home"},{"location":"#furacoes-no-atlantico","text":"No momento de confec\u00e7\u00e3o desse trabalho, o furac\u00e3o Laura estava atingindo Lousiana, o estado americano, causando a morte de pelo menos 8 pessoas e um preju\u00edzo de aproximadamente 9 bilh\u00f5es de dol\u00e1res. Olhando esse caso e considerando que casos como esse acontecem todo ano, tempestades tropicais s\u00e3o, variando com a intensidade, um problema para a humanidade tanto do ponto de vista econ\u00f4mico quanto do humanit\u00e1rio. Colocado esse problema, nossa proposta \u00e9 analisar o comportamento de curto prazo de furac\u00f5es e achar dire\u00e7\u00f5es que nos levem a entender como podemos minimizar esses constantes danos de furac\u00f5es.","title":"Furac\u00f5es no Atl\u00e2ntico"},{"location":"Analises_variaveis/","text":"Aplica\u00e7\u00e3o de Modelos de Regress\u00e3o Notebook refer\u00eancia Nesta p\u00e1gina iremos mostrar os diversos algoritmos aplicados aos dados visando ajustar uma boa regress\u00e3o que ajudasse a prever as velocidades dos ventos de um determinado evento (Tropical Storm ou Hurricane) ou a dura\u00e7\u00e3o dos mesmos. Algumas pequenas transforma\u00e7\u00f5es foram necess\u00e1rias para ajuste das vari\u00e1veis preditoras a serem consideradas em cada modelo. Para detalhes sobre o carregamento e prepara\u00e7\u00e3o dos dados, veja o notebook refer\u00eancia. An\u00e1lises Iniciais Segue abaixo o resultado da aplica\u00e7\u00e3o de uma regress\u00e3o linear simples usando como vari\u00e1vel alvo a velocidade de vento, dado usado em v\u00e1rios indicadores de destrutibilidade. Interessante notar os par\u00e2metros com coeficientes positivos. C\u00f3digo X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) print ( f 'Par\u00e2metro_Month = { OLSModel . params [ 2 ] } ' ) print ( f 'Par\u00e2metro_Latitude = { OLSModel . params [ 3 ] } ' ) print ( f 'Par\u00e2metro_Longitude = { OLSModel . params [ 4 ] } ' ) print ( f 'Par\u00e2metro_sst = { OLSModel . params [ 5 ] } ' ) print ( f 'Par\u00e2metro_rhum = { OLSModel . params [ 6 ] } ' ) print ( f 'Par\u00e2metro_wspd = { OLSModel . params [ 7 ] } ' ) print ( f 'Par\u00e2metro_slp = { OLSModel . params [ 8 ] } ' ) print ( f 'Par\u00e2metro_cldc = { OLSModel . params [ 9 ] } ' ) R^2_train = 0.02385621171100183 Par\u00e2metro_const = 47.35464784180999 Par\u00e2metro_Year = 0.09050506606867095 Par\u00e2metro_Month = -0.05872870746380546 Par\u00e2metro_Latitude = -0.08576970286238517 Par\u00e2metro_Longitude = 1.880119707824508 Par\u00e2metro_sst = 0.15179194438994867 Par\u00e2metro_rhum = 0.028283243122749335 Par\u00e2metro_wspd = 0.028283243122749446 Par\u00e2metro_slp = 0.14971356534654948 Par\u00e2metro_cldc = -1.5161434590996923 Os c\u00f3digos abaixos nos gera uma visualiza\u00e7\u00e3o que pode trazer insights a respeito da rela\u00e7\u00e3o entre as vari\u00e1veis. A escolha das vari\u00e1veis preditoras que servem de entrada para os modelos mais a frente foram pensadas tamb\u00e9m pela observa\u00e7\u00e3o destes gr\u00e1ficos. C\u00f3digo df = pd . concat ([ X_train , y_train_mw , y_train_mp ], axis = 1 ) scatter_matrix ( df , alpha = 0.8 , figsize = ( 15 , 15 ), diagonal = 'kde' ); C\u00f3digo # A princ\u00edpio, n\u00e3o queremos que se fa\u00e7a alguma previs\u00e3o com base no valor num\u00e9rico do ano # Al\u00e9m disso, a vari\u00e1vel wspd est\u00e1 altamente correlacionada com a rhum, podendo ser mantida apenas a \u00faltima X_train = data_atl_merged . drop ([ 'Year' , 'wspd' ], 1 ) # M\u00eas Latitude Longitude Temperatura, Umidade, Sea Level Pressure, Cloudiness] # ['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] fig , ax = plt . subplots ( 1 , 7 ) #, figsize=(16,10)) fig . suptitle ( 'Velocidade M\u00e1xima vs Vari\u00e1veis Preditoras (1950-2015)' , fontsize = 28 , y = 1.06 ) ax [ 0 ] . scatter ( X_train [ 'Month' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 1 ] . scatter ( X_train [ 'Latitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 2 ] . scatter ( X_train [ 'Longitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 3 ] . scatter ( X_train [ 'sst' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 4 ] . scatter ( X_train [ 'rhum' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 5 ] . scatter ( X_train [ 'slp' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 6 ] . scatter ( X_train [ 'cldc' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Uma primeira tentativa de ajuste foi feito atrav\u00e9s da centraliza\u00e7\u00e3o das vari\u00e1veis preditoras em rela\u00e7\u00e3o \u00e0 m\u00e9dia, adicionando tamb\u00e9m termos polinomiais de segunda ordem. No entanto, os resultados do ajuste n\u00e3o mostraram ganhos significativos para o modelo de Regress\u00e3o Linear M\u00faltipla, e at\u00e9 prejudicaram modelos mais complexos, como Random Forest, Multi Layer Perceptron, entre outros utilizados mais a frente. Detalhes desta parte do c\u00f3digo acesse o notebook refer\u00eancia, no link do in\u00edcio desta p\u00e1gina. Modelos com Separa\u00e7\u00e3o em Conjuntos de Treino e Teste Separamos os dados em conjuntos de treino e de teste. Deste modo, podemos ajustar o algoritmo utilizando os dados de treino, e tentar utilizar esses dados de teste para previs\u00e3o de outros dados, inclusive futuros. Regress\u00e3o Linear C\u00f3digo X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) X_test2 = sm . add_constant ( X_test ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared r2_test = 1 - (( OLSModel . predict ( X_test2 ) - y_test_mw ) * ( OLSModel . predict ( X_test2 ) - y_test_mw )) . sum () / (( y_test_mw . mean () - y_test_mw ) * ( y_test_mw . mean () - y_test_mw )) . sum () print ( f 'R^2_train = { r2_train } ' ) print ( f 'R^2_test = { r2_test } ' ) ''' print(f'Par\u00e2metro_const = {OLSModel.params[0]}') print(f'Par\u00e2metro_Month = {OLSModel.params[1]}') print(f'Par\u00e2metro_Latitude = {OLSModel.params[2]}') print(f'Par\u00e2metro_Longitude = {OLSModel.params[3]}') print(f'Par\u00e2metro_sst = {OLSModel.params[4]}') print(f'Par\u00e2metro_rhum = {OLSModel.params[5]}') print(f'Par\u00e2metro_slp = {OLSModel.params[6]}') print(f'Par\u00e2metro_cldc = {OLSModel.params[7]}') print(f'Par\u00e2metro_Month^2 = {OLSModel.params[8]}') print(f'Par\u00e2metro_Latitude^2 = {OLSModel.params[9]}') print(f'Par\u00e2metro_Longitude^2 = {OLSModel.params[10]}') print(f'Par\u00e2metro_sst^2 = {OLSModel.params[11]}') print(f'Par\u00e2metro_rhum^2 = {OLSModel.params[12]}') print(f'Par\u00e2metro_slp^2 = {OLSModel.params[13]}') print(f'Par\u00e2metro_cldc^2 = {OLSModel.params[14]}') ''' R^2_train = 0.019806236602926464 R^2_test = 0.01874952522766249 Veja abaixo as previs\u00f5es: Random Forest Pelos ajustes feitos no notebook com os dados completos usando Random Forest, vimos que esse algoritmo promove um bom ajuste nos dados. Um novo ajuste com aplica\u00e7\u00e3o de par\u00e2metros melhor sintonizados com os dados \u00e9 buscado pelo c\u00f3digo abaixo. C\u00f3digo - Random Forest # Par\u00e2metros com bom ajuste para Random Forest: n_estimators = 50, max_depth = 75 for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) O R2 Score obtido abaixo mostra o melhor ajuste do modelo quando tentamos prever a Velocidade M\u00e1xima Sustentada pelo algoritmo do Random Forest. O ajuste aos dados de treino fica bem superior aos de teste. Isso se deve em parte porque os dados se d\u00e3o em grupos de registros, associados aos eventos de furac\u00f5es. Assim, um algoritmo acaba por detectar a correla\u00e7\u00e3o dos dados dentro de um mesmo evento e busca ajust\u00e1-los de modo espec\u00edfico no conjunto de treinamento. No conjunto de teste, quando algum dado se encontra \"distante\" dos registros que foram utilizados, ele acaba n\u00e3o tendo o mesmo ajuste. regr_rf = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) print ( regr_rf . score ( X_train , y_train_mw )) print ( regr_rf . score ( X_test , y_test_mw )) 0.9298001112559356 0.5227899508473133 X_train_red = X_train . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) X_test_red = X_test . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) Retirando os dados clim\u00e1ticos, observamos que o ajuste fica bem pior, mostrando a import\u00e2ncia dos mesmos para a predi\u00e7\u00e3o regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.8598571752407663 0.06143697855472363 X_train_red = X_train . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) X_test_red = X_test . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9072010288584739 0.3833348196160016 #['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] X_train_red = X_train . drop ([ 'Month' ], 1 ) X_test_red = X_test . drop ([ 'Month' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9230537875398801 0.4840455607978509 Ajuste da predi\u00e7\u00e3o em rela\u00e7\u00e3o \u00e0 vari\u00e1vel sst (temperatura mensal m\u00e9dia) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_rf . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_rf . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Demais Previs\u00f5es com Random Forest (Melhor Ajuste) Adicionando as vari\u00e1veis Ano e Dia, conseguimos melhorar significativamente a capacidade de previs\u00e3o do nosso modelo. Se adicionarmos primeiramente apenas a vari\u00e1vel Ano, percebemos que cada vari\u00e1vel contribui um pouco para a melhoria da previs\u00e3o. data_train_sd = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' , 'Day' ], 1 ) data_train_mw_sd = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train_sd , data_test_sd , data_train_mw_sd , data_test_mw_sd = train_test_split ( data_train_sd , data_train_mw_sd , random_state = 1 ) regr_rf2_sd = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2_sd . fit ( data_train_sd , data_train_mw_sd ) print ( regr_rf2_sd . score ( data_train_sd , data_train_mw_sd )) print ( regr_rf2_sd . score ( data_test_sd , data_test_mw_sd )) 0.9502630426894276 0.6609429559863542 data_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) data_train_mw = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train , data_test , data_train_mw , data_test_mw = train_test_split ( data_train , data_train_mw , random_state = 1 ) Ajuste fino dos par\u00e2metros do Random Forest for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf2 = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( f ' \\n n_estimators= { i } , max_depth= { j } ' ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) n_estimators = 25 , max_depth = 25 0 . 957264562824329 0 . 7444676687346707 n_estimators = 25 , max_depth = 50 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 75 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 100 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 125 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 50 , max_depth = 25 0 . 9617003376388379 0 . 7583441990969142 n_estimators = 50 , max_depth = 50 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 75 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 100 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 125 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 75 , max_depth = 25 0 . 9637876668338223 0 . 7615184996888411 n_estimators = 75 , max_depth = 50 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 75 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 100 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 125 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 100 , max_depth = 25 0 . 9643667689055675 0 . 7618762745120209 n_estimators = 100 , max_depth = 50 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 75 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 100 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 125 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 125 , max_depth = 25 0 . 9647081307547872 0 . 762938280257875 n_estimators = 125 , max_depth = 50 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 75 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 100 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 125 0 . 9661419630929642 0 . 7643016255435418 Melhor ajuste para Previs\u00e3o de Maximal Wind regr_rf2 = RandomForestRegressor ( n_estimators = 50 , max_depth = 50 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) 0.9629951225598822 0.7593567448937373 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( data_train [ 'sst' ], data_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_train [ 'sst' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], data_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Previs\u00e3o da dura\u00e7\u00e3o dos eventos de Furac\u00e3o data_train2 = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) #data_train_mw = data_atl_merged['Maximum Wind'] data_train_dur = data_atl_merged [ 'Duration' ] #print(len(data_train)) #data_train.head() data_train2 , data_test2 , data_train_dur , data_test_dur = train_test_split ( data_train2 , data_train_dur , random_state = 1 ) Abaixo, faremos tamb\u00e9m a previs\u00e3o da dura\u00e7\u00e3o de um Furac\u00e3o. O ajuste fica bem preciso, como se pode ver pelo R2 Score regr_rf3 = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf3 . fit ( data_train2 , data_train_dur ) print ( regr_rf3 . score ( data_train2 , data_train_dur )) print ( regr_rf3 . score ( data_test2 , data_test_dur )) 0.9883397102866289 0.9289716458290775 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw , data_train_dur ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw , data_test_dur ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf3 . predict ( data_train2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf3 . predict ( data_test2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Multi Layer Perceptron regr_mlp = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp . score ( X_train , y_train_mw )) print ( regr_mlp . score ( X_test , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) 0 . 09520915346048042 0 . 08725614810614979 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_mlp . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_mlp . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Support Vector Machine regr_svr = svm . SVR () regr_svr . fit ( X_train , y_train_mw ) print ( regr_svr . score ( X_train , y_train_mw )) print ( regr_svr . score ( X_test , y_test_mw )) -0.06514630260669341 -0.058505019763586796 Modelos com Escala Padronizada # Padroniza\u00e7\u00e3o da Escala scaler = StandardScaler () # doctest: +SKIP scaler . fit ( X_train ) # doctest: +SKIP X_train_std = scaler . transform ( X_train ) # doctest: +SKIP X_test_std = scaler . transform ( X_test ) regr_svr_std = svm . SVR () regr_svr_std . fit ( X_train_std , y_train_mw ) print ( regr_svr_std . score ( X_train_std , y_train_mw )) print ( regr_svr_std . score ( X_test_std , y_test_mw )) 0.07772469466765619 0.07604328404370786 regr_mlp_std = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp_std . score ( X_train_std , y_train_mw )) print ( regr_mlp_std . score ( X_test_std , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) - 4 . 16270156850149 - 4 . 153096948726193 regr_rf_std = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_std . fit ( X_train_std , y_train_mw ) print ( regr_rf_std . score ( X_train_std , y_train_mw )) print ( regr_rf_std . score ( X_test_std , y_test_mw )) 0.9298162379440262 0.5220914993160997","title":"Destrutividade no curto prazo"},{"location":"Analises_variaveis/#aplicacao-de-modelos-de-regressao","text":"Notebook refer\u00eancia Nesta p\u00e1gina iremos mostrar os diversos algoritmos aplicados aos dados visando ajustar uma boa regress\u00e3o que ajudasse a prever as velocidades dos ventos de um determinado evento (Tropical Storm ou Hurricane) ou a dura\u00e7\u00e3o dos mesmos. Algumas pequenas transforma\u00e7\u00f5es foram necess\u00e1rias para ajuste das vari\u00e1veis preditoras a serem consideradas em cada modelo. Para detalhes sobre o carregamento e prepara\u00e7\u00e3o dos dados, veja o notebook refer\u00eancia.","title":"Aplica\u00e7\u00e3o de Modelos de Regress\u00e3o"},{"location":"Analises_variaveis/#analises-iniciais","text":"Segue abaixo o resultado da aplica\u00e7\u00e3o de uma regress\u00e3o linear simples usando como vari\u00e1vel alvo a velocidade de vento, dado usado em v\u00e1rios indicadores de destrutibilidade. Interessante notar os par\u00e2metros com coeficientes positivos. C\u00f3digo X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) print ( f 'Par\u00e2metro_Month = { OLSModel . params [ 2 ] } ' ) print ( f 'Par\u00e2metro_Latitude = { OLSModel . params [ 3 ] } ' ) print ( f 'Par\u00e2metro_Longitude = { OLSModel . params [ 4 ] } ' ) print ( f 'Par\u00e2metro_sst = { OLSModel . params [ 5 ] } ' ) print ( f 'Par\u00e2metro_rhum = { OLSModel . params [ 6 ] } ' ) print ( f 'Par\u00e2metro_wspd = { OLSModel . params [ 7 ] } ' ) print ( f 'Par\u00e2metro_slp = { OLSModel . params [ 8 ] } ' ) print ( f 'Par\u00e2metro_cldc = { OLSModel . params [ 9 ] } ' ) R^2_train = 0.02385621171100183 Par\u00e2metro_const = 47.35464784180999 Par\u00e2metro_Year = 0.09050506606867095 Par\u00e2metro_Month = -0.05872870746380546 Par\u00e2metro_Latitude = -0.08576970286238517 Par\u00e2metro_Longitude = 1.880119707824508 Par\u00e2metro_sst = 0.15179194438994867 Par\u00e2metro_rhum = 0.028283243122749335 Par\u00e2metro_wspd = 0.028283243122749446 Par\u00e2metro_slp = 0.14971356534654948 Par\u00e2metro_cldc = -1.5161434590996923 Os c\u00f3digos abaixos nos gera uma visualiza\u00e7\u00e3o que pode trazer insights a respeito da rela\u00e7\u00e3o entre as vari\u00e1veis. A escolha das vari\u00e1veis preditoras que servem de entrada para os modelos mais a frente foram pensadas tamb\u00e9m pela observa\u00e7\u00e3o destes gr\u00e1ficos. C\u00f3digo df = pd . concat ([ X_train , y_train_mw , y_train_mp ], axis = 1 ) scatter_matrix ( df , alpha = 0.8 , figsize = ( 15 , 15 ), diagonal = 'kde' ); C\u00f3digo # A princ\u00edpio, n\u00e3o queremos que se fa\u00e7a alguma previs\u00e3o com base no valor num\u00e9rico do ano # Al\u00e9m disso, a vari\u00e1vel wspd est\u00e1 altamente correlacionada com a rhum, podendo ser mantida apenas a \u00faltima X_train = data_atl_merged . drop ([ 'Year' , 'wspd' ], 1 ) # M\u00eas Latitude Longitude Temperatura, Umidade, Sea Level Pressure, Cloudiness] # ['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] fig , ax = plt . subplots ( 1 , 7 ) #, figsize=(16,10)) fig . suptitle ( 'Velocidade M\u00e1xima vs Vari\u00e1veis Preditoras (1950-2015)' , fontsize = 28 , y = 1.06 ) ax [ 0 ] . scatter ( X_train [ 'Month' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 1 ] . scatter ( X_train [ 'Latitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 2 ] . scatter ( X_train [ 'Longitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 3 ] . scatter ( X_train [ 'sst' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 4 ] . scatter ( X_train [ 'rhum' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 5 ] . scatter ( X_train [ 'slp' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) ax [ 6 ] . scatter ( X_train [ 'cldc' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Uma primeira tentativa de ajuste foi feito atrav\u00e9s da centraliza\u00e7\u00e3o das vari\u00e1veis preditoras em rela\u00e7\u00e3o \u00e0 m\u00e9dia, adicionando tamb\u00e9m termos polinomiais de segunda ordem. No entanto, os resultados do ajuste n\u00e3o mostraram ganhos significativos para o modelo de Regress\u00e3o Linear M\u00faltipla, e at\u00e9 prejudicaram modelos mais complexos, como Random Forest, Multi Layer Perceptron, entre outros utilizados mais a frente. Detalhes desta parte do c\u00f3digo acesse o notebook refer\u00eancia, no link do in\u00edcio desta p\u00e1gina.","title":"An\u00e1lises Iniciais"},{"location":"Analises_variaveis/#modelos-com-separacao-em-conjuntos-de-treino-e-teste","text":"Separamos os dados em conjuntos de treino e de teste. Deste modo, podemos ajustar o algoritmo utilizando os dados de treino, e tentar utilizar esses dados de teste para previs\u00e3o de outros dados, inclusive futuros.","title":"Modelos com Separa\u00e7\u00e3o em Conjuntos de Treino e Teste"},{"location":"Analises_variaveis/#regressao-linear","text":"C\u00f3digo X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) X_test2 = sm . add_constant ( X_test ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared r2_test = 1 - (( OLSModel . predict ( X_test2 ) - y_test_mw ) * ( OLSModel . predict ( X_test2 ) - y_test_mw )) . sum () / (( y_test_mw . mean () - y_test_mw ) * ( y_test_mw . mean () - y_test_mw )) . sum () print ( f 'R^2_train = { r2_train } ' ) print ( f 'R^2_test = { r2_test } ' ) ''' print(f'Par\u00e2metro_const = {OLSModel.params[0]}') print(f'Par\u00e2metro_Month = {OLSModel.params[1]}') print(f'Par\u00e2metro_Latitude = {OLSModel.params[2]}') print(f'Par\u00e2metro_Longitude = {OLSModel.params[3]}') print(f'Par\u00e2metro_sst = {OLSModel.params[4]}') print(f'Par\u00e2metro_rhum = {OLSModel.params[5]}') print(f'Par\u00e2metro_slp = {OLSModel.params[6]}') print(f'Par\u00e2metro_cldc = {OLSModel.params[7]}') print(f'Par\u00e2metro_Month^2 = {OLSModel.params[8]}') print(f'Par\u00e2metro_Latitude^2 = {OLSModel.params[9]}') print(f'Par\u00e2metro_Longitude^2 = {OLSModel.params[10]}') print(f'Par\u00e2metro_sst^2 = {OLSModel.params[11]}') print(f'Par\u00e2metro_rhum^2 = {OLSModel.params[12]}') print(f'Par\u00e2metro_slp^2 = {OLSModel.params[13]}') print(f'Par\u00e2metro_cldc^2 = {OLSModel.params[14]}') ''' R^2_train = 0.019806236602926464 R^2_test = 0.01874952522766249 Veja abaixo as previs\u00f5es:","title":"Regress\u00e3o Linear"},{"location":"Analises_variaveis/#random-forest","text":"Pelos ajustes feitos no notebook com os dados completos usando Random Forest, vimos que esse algoritmo promove um bom ajuste nos dados. Um novo ajuste com aplica\u00e7\u00e3o de par\u00e2metros melhor sintonizados com os dados \u00e9 buscado pelo c\u00f3digo abaixo. C\u00f3digo - Random Forest # Par\u00e2metros com bom ajuste para Random Forest: n_estimators = 50, max_depth = 75 for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) O R2 Score obtido abaixo mostra o melhor ajuste do modelo quando tentamos prever a Velocidade M\u00e1xima Sustentada pelo algoritmo do Random Forest. O ajuste aos dados de treino fica bem superior aos de teste. Isso se deve em parte porque os dados se d\u00e3o em grupos de registros, associados aos eventos de furac\u00f5es. Assim, um algoritmo acaba por detectar a correla\u00e7\u00e3o dos dados dentro de um mesmo evento e busca ajust\u00e1-los de modo espec\u00edfico no conjunto de treinamento. No conjunto de teste, quando algum dado se encontra \"distante\" dos registros que foram utilizados, ele acaba n\u00e3o tendo o mesmo ajuste. regr_rf = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) print ( regr_rf . score ( X_train , y_train_mw )) print ( regr_rf . score ( X_test , y_test_mw )) 0.9298001112559356 0.5227899508473133 X_train_red = X_train . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) X_test_red = X_test . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) Retirando os dados clim\u00e1ticos, observamos que o ajuste fica bem pior, mostrando a import\u00e2ncia dos mesmos para a predi\u00e7\u00e3o regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.8598571752407663 0.06143697855472363 X_train_red = X_train . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) X_test_red = X_test . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9072010288584739 0.3833348196160016 #['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] X_train_red = X_train . drop ([ 'Month' ], 1 ) X_test_red = X_test . drop ([ 'Month' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9230537875398801 0.4840455607978509 Ajuste da predi\u00e7\u00e3o em rela\u00e7\u00e3o \u00e0 vari\u00e1vel sst (temperatura mensal m\u00e9dia) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_rf . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_rf . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Random Forest"},{"location":"Analises_variaveis/#demais-previsoes-com-random-forest-melhor-ajuste","text":"Adicionando as vari\u00e1veis Ano e Dia, conseguimos melhorar significativamente a capacidade de previs\u00e3o do nosso modelo. Se adicionarmos primeiramente apenas a vari\u00e1vel Ano, percebemos que cada vari\u00e1vel contribui um pouco para a melhoria da previs\u00e3o. data_train_sd = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' , 'Day' ], 1 ) data_train_mw_sd = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train_sd , data_test_sd , data_train_mw_sd , data_test_mw_sd = train_test_split ( data_train_sd , data_train_mw_sd , random_state = 1 ) regr_rf2_sd = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2_sd . fit ( data_train_sd , data_train_mw_sd ) print ( regr_rf2_sd . score ( data_train_sd , data_train_mw_sd )) print ( regr_rf2_sd . score ( data_test_sd , data_test_mw_sd )) 0.9502630426894276 0.6609429559863542 data_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) data_train_mw = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train , data_test , data_train_mw , data_test_mw = train_test_split ( data_train , data_train_mw , random_state = 1 ) Ajuste fino dos par\u00e2metros do Random Forest for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf2 = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( f ' \\n n_estimators= { i } , max_depth= { j } ' ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) n_estimators = 25 , max_depth = 25 0 . 957264562824329 0 . 7444676687346707 n_estimators = 25 , max_depth = 50 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 75 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 100 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 125 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 50 , max_depth = 25 0 . 9617003376388379 0 . 7583441990969142 n_estimators = 50 , max_depth = 50 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 75 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 100 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 125 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 75 , max_depth = 25 0 . 9637876668338223 0 . 7615184996888411 n_estimators = 75 , max_depth = 50 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 75 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 100 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 125 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 100 , max_depth = 25 0 . 9643667689055675 0 . 7618762745120209 n_estimators = 100 , max_depth = 50 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 75 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 100 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 125 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 125 , max_depth = 25 0 . 9647081307547872 0 . 762938280257875 n_estimators = 125 , max_depth = 50 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 75 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 100 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 125 0 . 9661419630929642 0 . 7643016255435418 Melhor ajuste para Previs\u00e3o de Maximal Wind regr_rf2 = RandomForestRegressor ( n_estimators = 50 , max_depth = 50 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) 0.9629951225598822 0.7593567448937373 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( data_train [ 'sst' ], data_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_train [ 'sst' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], data_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Demais Previs\u00f5es com Random Forest (Melhor Ajuste)"},{"location":"Analises_variaveis/#previsao-da-duracao-dos-eventos-de-furacao","text":"data_train2 = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) #data_train_mw = data_atl_merged['Maximum Wind'] data_train_dur = data_atl_merged [ 'Duration' ] #print(len(data_train)) #data_train.head() data_train2 , data_test2 , data_train_dur , data_test_dur = train_test_split ( data_train2 , data_train_dur , random_state = 1 ) Abaixo, faremos tamb\u00e9m a previs\u00e3o da dura\u00e7\u00e3o de um Furac\u00e3o. O ajuste fica bem preciso, como se pode ver pelo R2 Score regr_rf3 = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf3 . fit ( data_train2 , data_train_dur ) print ( regr_rf3 . score ( data_train2 , data_train_dur )) print ( regr_rf3 . score ( data_test2 , data_test_dur )) 0.9883397102866289 0.9289716458290775 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw , data_train_dur ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw , data_test_dur ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf3 . predict ( data_train2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf3 . predict ( data_test2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Previs\u00e3o da dura\u00e7\u00e3o dos eventos de Furac\u00e3o"},{"location":"Analises_variaveis/#multi-layer-perceptron","text":"regr_mlp = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp . score ( X_train , y_train_mw )) print ( regr_mlp . score ( X_test , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) 0 . 09520915346048042 0 . 08725614810614979 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_mlp . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_mlp . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Multi Layer Perceptron"},{"location":"Analises_variaveis/#support-vector-machine","text":"regr_svr = svm . SVR () regr_svr . fit ( X_train , y_train_mw ) print ( regr_svr . score ( X_train , y_train_mw )) print ( regr_svr . score ( X_test , y_test_mw )) -0.06514630260669341 -0.058505019763586796","title":"Support Vector Machine"},{"location":"Analises_variaveis/#modelos-com-escala-padronizada","text":"# Padroniza\u00e7\u00e3o da Escala scaler = StandardScaler () # doctest: +SKIP scaler . fit ( X_train ) # doctest: +SKIP X_train_std = scaler . transform ( X_train ) # doctest: +SKIP X_test_std = scaler . transform ( X_test ) regr_svr_std = svm . SVR () regr_svr_std . fit ( X_train_std , y_train_mw ) print ( regr_svr_std . score ( X_train_std , y_train_mw )) print ( regr_svr_std . score ( X_test_std , y_test_mw )) 0.07772469466765619 0.07604328404370786 regr_mlp_std = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp_std . score ( X_train_std , y_train_mw )) print ( regr_mlp_std . score ( X_test_std , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) - 4 . 16270156850149 - 4 . 153096948726193 regr_rf_std = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_std . fit ( X_train_std , y_train_mw ) print ( regr_rf_std . score ( X_train_std , y_train_mw )) print ( regr_rf_std . score ( X_test_std , y_test_mw )) 0.9298162379440262 0.5220914993160997","title":"Modelos com Escala Padronizada"},{"location":"Data_Cleaning_and_EDA/","text":"Explora\u00e7\u00e3o, Limpeza dos dados e Visualiza\u00e7\u00f5es Notebook refer\u00eancia: Data_Cleaning_and_EDA.ipynb Trabalharemos com tr\u00eas datasets . O primeiro \u00e9 um subconjunto do ICOADS (International Comprehensive Ocean-Atmosphere Data Set) fornecido pela NOAA (National Oceanic and Atmospheric Administration) que possui v\u00e1rios dados clim\u00e1ticos de cobertura mensal, desde 1800 at\u00e9 o presente, e com abrang\u00eancia mar\u00edtima; Possui dados como temperatura do mar, umidade, press\u00e3o, cobertura de nuvens, velocidade de vento, etc. Caso queira trabalhar com eles, as instruc\u00f5es para baix\u00e1-los est\u00e3o no notebook refer\u00eancia. O segundo grande conjunto de dados \u00e9 o HURDAT2 cuja fonte oficial \u00e9 a NHC (National Hurricane Center), divis\u00e3o da NOAA respons\u00e1vel pelos fura\u00e7\u00f5es e tempestades. Os dados do Kaggle fornecem dados de tempestades e furac\u00f5es desde o s\u00e9culo XIX at\u00e9 2015 no pac\u00edfico e atl\u00e2ntico, mas iremos focar nossa an\u00e1lise no dadaset do atl\u00e2ntico. O terceiro, \u00e9 um dado com o \u00edndice PDI, mas ele j\u00e1 vem da fonte em boas condi\u00e7\u00f5es e n\u00e3o necessita de limpeza. Veja mais no notebook PowerDissipationIndex.ipynb . Dataset ICOADS Os dados da ICOADS vem no formato .nc que \u00e9 leg\u00edvel ao python atrav\u00e9s da biblioteca netCDF4 que \u00e9 um dos pr\u00e9-requisitos para execu\u00e7\u00e3o dos notebooks. An\u00e1lise de dados faltantes Legenda: sst: Sea Surface Temperature (Temperatura na superf\u00edcie do mar) wspd: Scalar Wind Speed (Velocidade de vento escalar) rhum: Relative Humidity (Umidade relativa) slp: Sea Level Pressure (Press\u00e3o no n\u00edvel do mar) vwnd: V-wind component (Componente V-wind) cldc: Cloudiness (Nebulosidade das nuvens) Como os continentes representam aproximadamente 29,1\\% da suferf\u00edcie terrestre e nossos dados s\u00f3 preenchem os oceanos, os continentes s\u00e3o preenchidos como dados inexistentes. Ent\u00e3o naturalmente nossa cota inferior de dados faltantes \u00e9 essa porcentagem. Os dados lidos dos arquivos .nc vem no formato de \"numpy masked array\" de 3 dimens\u00f5es (tempo,latitude,longitude) que tem um atributo \"mask\" que \u00e9 indicadora de dado faltante, isso nos ajudar\u00e1 a lidar com esses dados. Como vemos no plot acima temos v\u00e1rias d\u00e9cadas com n\u00edveis de dados faltantes acima de 90\\% mas vamos analisar focadamente e atl\u00e2ntico norte, que \u00e9 nossa regi\u00e3o de estudos. *MDR \u00e9 a abrevia\u00e7\u00e3o de Main Development Region ou regi\u00e3o central de desenvolvimento dos furac\u00f5es no Atl\u00e2ntico Norte e se refere \u00e0 faixa 10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W. Vemos que a partir de 1950-1960, os dados come\u00e7am a ficar mais completos na regi\u00e3o de estudos. Ent\u00e3o, para entender a rela\u00e7\u00e3o as vari\u00e1veis, iremos trabalhar a partir desta data. Entretanto, nada nos impede de usar os dados mais antigos, j\u00e1 que as medi\u00e7\u00f5es n\u00e3o variam muito quando est\u00e3o perto, se quisermos trabalhar com tend\u00eancias de longo prazo podemos cortar os dados a partir de 1920, trabalhar com a m\u00e9dia das regi\u00f5es estudadas, mesmo que com ~70\\% de dados faltantes. Isso pois temos a array indicadora, que pode ajudar em modelos, e tamb\u00e9m essa porcentagem \u00e9 um pouco mais baixa devido \u00e0s faixas continentais considaradas no corte de coordenadas. Visualiza\u00e7\u00e3o Abaixo temos um exemplo de como os dados de temperatura est\u00e3o distribu\u00eddos em Janeiro de 1955 Agrega\u00e7\u00e3o dos Dados MDR Criaremos um dataframe com as m\u00e9dias espaciais da regi\u00e3o MDR, para an\u00e1lise futura com o PDI. Usamos essa m\u00e9dia baseados na premissa razo\u00e1vel de que nesse corte espacial da MDR do atl\u00e2ntico os valores n\u00e3o variam muito dentro de um m\u00eas. Fazemos essa m\u00e9dias para an\u00e1lises de mais longo prazo como podem ver no notebook do PDI . Mais detalhes sobre a cria\u00e7\u00e3o desse dataframe veja no notebook refer\u00eancia. Um plot que se tornou poss\u00edvel ap\u00f3s essa agrega\u00e7\u00e3o foi o mostrado abaixo: Podemos ver que ap\u00f3s 1970-1980 inicia-se uma tend\u00eancia crescente de aumento de temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica. Temos tamb\u00e9m a matriz de correla\u00e7\u00e3o entre as vari\u00e1veis desde novo dataframe: #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col0 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col1 { background-color: #9dbdff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col2 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col3 { background-color: #b9d0f9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col4 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col5 { background-color: #f4987a; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col6 { background-color: #c9d7f0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col7 { background-color: #dfdbd9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col8 { background-color: #bcd2f7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col0 { background-color: #536edd; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col1 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col2 { background-color: #e36b54; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col3 { background-color: #f7ba9f; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col4 { background-color: #7ea1fa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col5 { background-color: #4055c8; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col6 { background-color: #e5d8d1; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col7 { background-color: #f2cbb7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col8 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col0 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col1 { background-color: #e7745b; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col2 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col3 { background-color: #ee8669; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col4 { background-color: #5977e3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col5 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col6 { background-color: #f7b194; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col7 { background-color: #f7a688; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col8 { background-color: #a7c5fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col0 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col1 { background-color: #eed0c0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col2 { background-color: #f18f71; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col3 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col4 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col5 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col6 { background-color: #d85646; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col7 { background-color: #ead5c9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col8 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col0 { background-color: #6485ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col1 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col2 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col3 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col4 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col5 { background-color: #e1dad6; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col6 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col7 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col8 { background-color: #6788ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col0 { background-color: #f7b497; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col1 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col2 { background-color: #6384eb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col3 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col4 { background-color: #f4c5ad; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col5 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col6 { background-color: #a1c0ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col7 { background-color: #b2ccfb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col8 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col0 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col1 { background-color: #c3d5f4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col2 { background-color: #f6bda2; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col3 { background-color: #d95847; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col4 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col5 { background-color: #5e7de7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col6 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col7 { background-color: #dedcdb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col8 { background-color: #3e51c5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col0 { background-color: #a2c1ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col1 { background-color: #edd2c3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col2 { background-color: #f6a283; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col3 { background-color: #f2c9b4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col4 { background-color: #688aef; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col5 { background-color: #9ebeff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col6 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col7 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col8 { background-color: #455cce; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col0 { background-color: #cbd8ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col1 { background-color: #9bbcff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col2 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col3 { background-color: #cdd9ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col4 { background-color: #e2dad5; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col5 { background-color: #a6c4fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col6 { background-color: #cedaeb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col7 { background-color: #b5cdfa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col8 { background-color: #b40426; color: #f1f1f1; } Year Month sst rhum slp wspd vwnd cldc sst_anomaly Year 1.000000 -0.010203 0.145686 -0.098235 0.048908 0.646453 -0.035567 0.241832 0.379038 Month -0.010203 1.000000 0.762722 0.392842 -0.434216 -0.385048 0.151316 0.380311 -0.017207 sst 0.145686 0.762722 1.000000 0.635370 -0.632031 -0.416486 0.432498 0.559898 0.312656 rhum -0.098235 0.392842 0.635370 1.000000 -0.768182 -0.380635 0.803748 0.305508 0.014696 slp 0.048908 -0.434216 -0.632031 -0.768182 1.000000 0.317397 -0.812037 -0.548567 0.129542 wspd 0.646453 -0.385048 -0.416486 -0.380635 0.317397 1.000000 -0.253309 0.007404 0.040588 vwnd -0.035567 0.151316 0.432498 0.803748 -0.812037 -0.253309 1.000000 0.236100 -0.002599 cldc 0.241832 0.380311 0.559898 0.305508 -0.548567 0.007404 0.236100 1.000000 0.019122 sst_anomaly 0.379038 -0.017207 0.312656 0.014696 0.129542 0.040588 -0.002599 0.019122 1.000000 Alguma correla\u00e7\u00f5es fortes interessantes: Temperatura do mar (sst) com ano (Year) Temperatura do mar (sst) e umidade (rhum) Velocidade de vento (wspd) e ano (Year) Press\u00e3o (slp) e Umidade (rhum) An\u00e1lises mais aprofundadas dessas vari\u00e1veis veremos no notebook de an\u00e1lise do PDI. Dataset HURDAT2 (Hurricane) - An\u00e1lise e Limpeza Passemos agora a analisar os dados de tempestades e furac\u00f5es. Segue a visualiza\u00e7\u00e3o da Temporada de Furac\u00f5es : Per\u00edodo de furac\u00f5es se concentra entre os meses de julho e novembro. Isso deve estar relacionado com o per\u00edodo anterior (ver\u00e3o do hemisf\u00e9rio norte, onde acontece o maior n\u00famero de casos) O aquecimento das \u00e1guas est\u00e1 intimamente ligado com a forma\u00e7\u00e3o das massas de ar que ocasionam os furac\u00f5es Isso nos d\u00e1 uma pista da forma de correla\u00e7\u00e3o temporal que devemos buscar para predizer os eventos. Veja acima que os picos de temperatura e umidade, coincidem razoavelmente com a temporada de furac\u00f5es. O pico de nebulosidade das nuvens tamb\u00e9m coincide de forma razo\u00e1vel com a temporada de furac\u00f5es. Assim como os valores mais baixos de press\u00e3o. Essas caracter\u00edsticas est\u00e3o relacionadas com a forma\u00e7\u00e3o do evento de tempestade forte ou furac\u00e3o. Veja abaixo a rela\u00e7\u00e3o quase linear entre velocidade m\u00e1xima sustendada de vento e a press\u00e3o m\u00ednima dos eventos. Essa rela\u00e7\u00e3o coincide com o que se sabe da forma\u00e7\u00e3o do evento de furac\u00e3o. Vemos abaixo que o n\u00famero de registro de fura\u00e7\u00f5es tem crescido desde 1850, mas isso se deve \u00e0 maior capacidade de detec\u00e7\u00e3o com o passar dos anos. Vemos tamb\u00e9m que a velocidade m\u00e1xima de vento sustentada pelos furac\u00f5es reduziu, mas isso tamb\u00e9m se deve \u00e0 maior capacidade de registro de eventos de pequeno porte, que acabam pesando a m\u00e9dia para baixo. Assim, para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte; Consideramos apenas tempestades cuja dura\u00e7\u00e3o em dias \u00e9 maior que 2, e cuja classifica\u00e7\u00e3o na escala Saffir-Simpson seja no m\u00ednimo Tempestade Tropical. Para essa classifica\u00e7\u00e3o, a velocidade m\u00e1xima sustentada de vento deve ultrapassar 63km/h o que equivale a 34 knots (milhas n\u00e1uticas). Ap\u00f3s algumas an\u00e1lises (veja notebook refer\u00eancia), decidimos aplicar um filtro aos dados, para reduzir vi\u00e9s de capacidade de observa\u00e7\u00e3o, e detec\u00e7\u00e3o de tempestades menores. Os filtros foram: Velocidade M\u00e1xima Sustentada > 34 milhas n\u00e1uticas Dura\u00e7\u00e3o > 2 dias Furac\u00f5es a partir de 1950 (quando a capacidade de medi\u00e7\u00e3o come\u00e7a a evoluir Latitude entre 0\u00b0 e 25\u00b0N (remover vi\u00e9s dos registros extratropicais) Fazemos tamb\u00e9m outras limpezas como formata\u00e7\u00e3o de entradas de latitude, longitude e outras vari\u00e1veis que n\u00e3o vieram da forma ideal. Uni\u00e3o dos dados via k-NN ponderado A partir dos dois grandes conjuntos de dados descritos acima criamos um novo dataframe que pega o dataframe HURDAT2 filtrado acima e busca os dados clim\u00e1ticos nos datasets da ICOADS diretamente dos arquivos .nc. Fizemos essa busca via coordenadas e para lidar com dados faltantes implementaremos um k-NN ponderado pelo inverso das dist\u00e2ncias entre as coordenadas originais e o vizinho considerado no algoritmo. Formula da dist\u00e2ncia Dados duas coordenadas (\\varphi_1,\\lambda_1) e ( \\varphi_2,\\lambda_2) em radianos, a F\u00f3rmula Haversine \u00e9 capaz de calcular a dist\u00e2ncia real entre esses dois pontos no mapa: Onde r \u00e9 o raio da Terra. Usando r aproximadamente igual a 6371 km o valor de d ser\u00e1 a dist\u00e2ncia em km dos dois pontos dados em coordenadas geogr\u00e1ficas. Detalhes sobre a implementa\u00e7\u00e3o desse c\u00e1lculo est\u00e3o no notebook refer\u00eancia. k-NN ponderado por inverso da dist\u00e2ncia Implementamos v\u00e1rias func\u00f5es auxiliares para ajudar nessa tarefa de coletar vizinhos para c\u00e1lculo de m\u00e9dia. Detalhes est\u00e3o no notebook refer\u00eancia. Por conta de dados faltantes no ICOADS, nem sempre existir\u00e1 registros para todas as coordenadas. Assim, buscamos os k vizinhos mais pr\u00f3ximos a calculamos a m\u00e9dia entre eles, por\u00e9m a m\u00e9dia \u00e9 ponderada pelo inverso da dist\u00e2ncia entre o vizinho e a coordenada original (usando a F\u00f3rmula Haversive). Sendo assim, pontos mais distantes ter\u00e3o menor peso na m\u00e9dia, enquanto pontos mais pr\u00f3ximos ser\u00e3o considerados mais importantes. Por simplicidade, usamos k=15 . Ap\u00f3s a execu\u00e7\u00e3o dos algoritmos implementados, salvamos o dataframe gerado em um csv e o usamos para outras an\u00e1lises. Outras Visualiza\u00e7\u00f5es Nos plots abaixo usaremos a biblioteca troPYcal. Internamente ela tem o dataset HURDAT2 atualizado e possui fun\u00e7\u00f5es de visualiza\u00e7\u00e3o prontas, simples de serem usadas. Refer\u00eancia: https://tropycal.github.io/tropycal/examples/index.html","title":"Data Cleaning + EDA"},{"location":"Data_Cleaning_and_EDA/#exploracao-limpeza-dos-dados-e-visualizacoes","text":"Notebook refer\u00eancia: Data_Cleaning_and_EDA.ipynb Trabalharemos com tr\u00eas datasets . O primeiro \u00e9 um subconjunto do ICOADS (International Comprehensive Ocean-Atmosphere Data Set) fornecido pela NOAA (National Oceanic and Atmospheric Administration) que possui v\u00e1rios dados clim\u00e1ticos de cobertura mensal, desde 1800 at\u00e9 o presente, e com abrang\u00eancia mar\u00edtima; Possui dados como temperatura do mar, umidade, press\u00e3o, cobertura de nuvens, velocidade de vento, etc. Caso queira trabalhar com eles, as instruc\u00f5es para baix\u00e1-los est\u00e3o no notebook refer\u00eancia. O segundo grande conjunto de dados \u00e9 o HURDAT2 cuja fonte oficial \u00e9 a NHC (National Hurricane Center), divis\u00e3o da NOAA respons\u00e1vel pelos fura\u00e7\u00f5es e tempestades. Os dados do Kaggle fornecem dados de tempestades e furac\u00f5es desde o s\u00e9culo XIX at\u00e9 2015 no pac\u00edfico e atl\u00e2ntico, mas iremos focar nossa an\u00e1lise no dadaset do atl\u00e2ntico. O terceiro, \u00e9 um dado com o \u00edndice PDI, mas ele j\u00e1 vem da fonte em boas condi\u00e7\u00f5es e n\u00e3o necessita de limpeza. Veja mais no notebook PowerDissipationIndex.ipynb .","title":"Explora\u00e7\u00e3o, Limpeza dos dados e Visualiza\u00e7\u00f5es"},{"location":"Data_Cleaning_and_EDA/#dataset-icoads","text":"Os dados da ICOADS vem no formato .nc que \u00e9 leg\u00edvel ao python atrav\u00e9s da biblioteca netCDF4 que \u00e9 um dos pr\u00e9-requisitos para execu\u00e7\u00e3o dos notebooks.","title":"Dataset ICOADS"},{"location":"Data_Cleaning_and_EDA/#analise-de-dados-faltantes","text":"Legenda: sst: Sea Surface Temperature (Temperatura na superf\u00edcie do mar) wspd: Scalar Wind Speed (Velocidade de vento escalar) rhum: Relative Humidity (Umidade relativa) slp: Sea Level Pressure (Press\u00e3o no n\u00edvel do mar) vwnd: V-wind component (Componente V-wind) cldc: Cloudiness (Nebulosidade das nuvens) Como os continentes representam aproximadamente 29,1\\% da suferf\u00edcie terrestre e nossos dados s\u00f3 preenchem os oceanos, os continentes s\u00e3o preenchidos como dados inexistentes. Ent\u00e3o naturalmente nossa cota inferior de dados faltantes \u00e9 essa porcentagem. Os dados lidos dos arquivos .nc vem no formato de \"numpy masked array\" de 3 dimens\u00f5es (tempo,latitude,longitude) que tem um atributo \"mask\" que \u00e9 indicadora de dado faltante, isso nos ajudar\u00e1 a lidar com esses dados. Como vemos no plot acima temos v\u00e1rias d\u00e9cadas com n\u00edveis de dados faltantes acima de 90\\% mas vamos analisar focadamente e atl\u00e2ntico norte, que \u00e9 nossa regi\u00e3o de estudos. *MDR \u00e9 a abrevia\u00e7\u00e3o de Main Development Region ou regi\u00e3o central de desenvolvimento dos furac\u00f5es no Atl\u00e2ntico Norte e se refere \u00e0 faixa 10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W. Vemos que a partir de 1950-1960, os dados come\u00e7am a ficar mais completos na regi\u00e3o de estudos. Ent\u00e3o, para entender a rela\u00e7\u00e3o as vari\u00e1veis, iremos trabalhar a partir desta data. Entretanto, nada nos impede de usar os dados mais antigos, j\u00e1 que as medi\u00e7\u00f5es n\u00e3o variam muito quando est\u00e3o perto, se quisermos trabalhar com tend\u00eancias de longo prazo podemos cortar os dados a partir de 1920, trabalhar com a m\u00e9dia das regi\u00f5es estudadas, mesmo que com ~70\\% de dados faltantes. Isso pois temos a array indicadora, que pode ajudar em modelos, e tamb\u00e9m essa porcentagem \u00e9 um pouco mais baixa devido \u00e0s faixas continentais considaradas no corte de coordenadas.","title":"An\u00e1lise de dados faltantes"},{"location":"Data_Cleaning_and_EDA/#visualizacao","text":"Abaixo temos um exemplo de como os dados de temperatura est\u00e3o distribu\u00eddos em Janeiro de 1955","title":"Visualiza\u00e7\u00e3o"},{"location":"Data_Cleaning_and_EDA/#agregacao-dos-dados-mdr","text":"Criaremos um dataframe com as m\u00e9dias espaciais da regi\u00e3o MDR, para an\u00e1lise futura com o PDI. Usamos essa m\u00e9dia baseados na premissa razo\u00e1vel de que nesse corte espacial da MDR do atl\u00e2ntico os valores n\u00e3o variam muito dentro de um m\u00eas. Fazemos essa m\u00e9dias para an\u00e1lises de mais longo prazo como podem ver no notebook do PDI . Mais detalhes sobre a cria\u00e7\u00e3o desse dataframe veja no notebook refer\u00eancia. Um plot que se tornou poss\u00edvel ap\u00f3s essa agrega\u00e7\u00e3o foi o mostrado abaixo: Podemos ver que ap\u00f3s 1970-1980 inicia-se uma tend\u00eancia crescente de aumento de temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica. Temos tamb\u00e9m a matriz de correla\u00e7\u00e3o entre as vari\u00e1veis desde novo dataframe: #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col0 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col1 { background-color: #9dbdff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col2 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col3 { background-color: #b9d0f9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col4 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col5 { background-color: #f4987a; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col6 { background-color: #c9d7f0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col7 { background-color: #dfdbd9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col8 { background-color: #bcd2f7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col0 { background-color: #536edd; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col1 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col2 { background-color: #e36b54; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col3 { background-color: #f7ba9f; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col4 { background-color: #7ea1fa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col5 { background-color: #4055c8; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col6 { background-color: #e5d8d1; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col7 { background-color: #f2cbb7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col8 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col0 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col1 { background-color: #e7745b; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col2 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col3 { background-color: #ee8669; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col4 { background-color: #5977e3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col5 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col6 { background-color: #f7b194; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col7 { background-color: #f7a688; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col8 { background-color: #a7c5fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col0 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col1 { background-color: #eed0c0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col2 { background-color: #f18f71; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col3 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col4 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col5 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col6 { background-color: #d85646; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col7 { background-color: #ead5c9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col8 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col0 { background-color: #6485ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col1 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col2 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col3 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col4 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col5 { background-color: #e1dad6; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col6 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col7 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col8 { background-color: #6788ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col0 { background-color: #f7b497; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col1 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col2 { background-color: #6384eb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col3 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col4 { background-color: #f4c5ad; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col5 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col6 { background-color: #a1c0ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col7 { background-color: #b2ccfb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col8 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col0 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col1 { background-color: #c3d5f4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col2 { background-color: #f6bda2; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col3 { background-color: #d95847; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col4 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col5 { background-color: #5e7de7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col6 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col7 { background-color: #dedcdb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col8 { background-color: #3e51c5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col0 { background-color: #a2c1ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col1 { background-color: #edd2c3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col2 { background-color: #f6a283; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col3 { background-color: #f2c9b4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col4 { background-color: #688aef; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col5 { background-color: #9ebeff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col6 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col7 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col8 { background-color: #455cce; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col0 { background-color: #cbd8ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col1 { background-color: #9bbcff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col2 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col3 { background-color: #cdd9ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col4 { background-color: #e2dad5; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col5 { background-color: #a6c4fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col6 { background-color: #cedaeb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col7 { background-color: #b5cdfa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col8 { background-color: #b40426; color: #f1f1f1; } Year Month sst rhum slp wspd vwnd cldc sst_anomaly Year 1.000000 -0.010203 0.145686 -0.098235 0.048908 0.646453 -0.035567 0.241832 0.379038 Month -0.010203 1.000000 0.762722 0.392842 -0.434216 -0.385048 0.151316 0.380311 -0.017207 sst 0.145686 0.762722 1.000000 0.635370 -0.632031 -0.416486 0.432498 0.559898 0.312656 rhum -0.098235 0.392842 0.635370 1.000000 -0.768182 -0.380635 0.803748 0.305508 0.014696 slp 0.048908 -0.434216 -0.632031 -0.768182 1.000000 0.317397 -0.812037 -0.548567 0.129542 wspd 0.646453 -0.385048 -0.416486 -0.380635 0.317397 1.000000 -0.253309 0.007404 0.040588 vwnd -0.035567 0.151316 0.432498 0.803748 -0.812037 -0.253309 1.000000 0.236100 -0.002599 cldc 0.241832 0.380311 0.559898 0.305508 -0.548567 0.007404 0.236100 1.000000 0.019122 sst_anomaly 0.379038 -0.017207 0.312656 0.014696 0.129542 0.040588 -0.002599 0.019122 1.000000 Alguma correla\u00e7\u00f5es fortes interessantes: Temperatura do mar (sst) com ano (Year) Temperatura do mar (sst) e umidade (rhum) Velocidade de vento (wspd) e ano (Year) Press\u00e3o (slp) e Umidade (rhum) An\u00e1lises mais aprofundadas dessas vari\u00e1veis veremos no notebook de an\u00e1lise do PDI.","title":"Agrega\u00e7\u00e3o dos Dados MDR"},{"location":"Data_Cleaning_and_EDA/#dataset-hurdat2-hurricane-analise-e-limpeza","text":"Passemos agora a analisar os dados de tempestades e furac\u00f5es. Segue a visualiza\u00e7\u00e3o da Temporada de Furac\u00f5es : Per\u00edodo de furac\u00f5es se concentra entre os meses de julho e novembro. Isso deve estar relacionado com o per\u00edodo anterior (ver\u00e3o do hemisf\u00e9rio norte, onde acontece o maior n\u00famero de casos) O aquecimento das \u00e1guas est\u00e1 intimamente ligado com a forma\u00e7\u00e3o das massas de ar que ocasionam os furac\u00f5es Isso nos d\u00e1 uma pista da forma de correla\u00e7\u00e3o temporal que devemos buscar para predizer os eventos. Veja acima que os picos de temperatura e umidade, coincidem razoavelmente com a temporada de furac\u00f5es. O pico de nebulosidade das nuvens tamb\u00e9m coincide de forma razo\u00e1vel com a temporada de furac\u00f5es. Assim como os valores mais baixos de press\u00e3o. Essas caracter\u00edsticas est\u00e3o relacionadas com a forma\u00e7\u00e3o do evento de tempestade forte ou furac\u00e3o. Veja abaixo a rela\u00e7\u00e3o quase linear entre velocidade m\u00e1xima sustendada de vento e a press\u00e3o m\u00ednima dos eventos. Essa rela\u00e7\u00e3o coincide com o que se sabe da forma\u00e7\u00e3o do evento de furac\u00e3o. Vemos abaixo que o n\u00famero de registro de fura\u00e7\u00f5es tem crescido desde 1850, mas isso se deve \u00e0 maior capacidade de detec\u00e7\u00e3o com o passar dos anos. Vemos tamb\u00e9m que a velocidade m\u00e1xima de vento sustentada pelos furac\u00f5es reduziu, mas isso tamb\u00e9m se deve \u00e0 maior capacidade de registro de eventos de pequeno porte, que acabam pesando a m\u00e9dia para baixo. Assim, para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte; Consideramos apenas tempestades cuja dura\u00e7\u00e3o em dias \u00e9 maior que 2, e cuja classifica\u00e7\u00e3o na escala Saffir-Simpson seja no m\u00ednimo Tempestade Tropical. Para essa classifica\u00e7\u00e3o, a velocidade m\u00e1xima sustentada de vento deve ultrapassar 63km/h o que equivale a 34 knots (milhas n\u00e1uticas). Ap\u00f3s algumas an\u00e1lises (veja notebook refer\u00eancia), decidimos aplicar um filtro aos dados, para reduzir vi\u00e9s de capacidade de observa\u00e7\u00e3o, e detec\u00e7\u00e3o de tempestades menores. Os filtros foram: Velocidade M\u00e1xima Sustentada > 34 milhas n\u00e1uticas Dura\u00e7\u00e3o > 2 dias Furac\u00f5es a partir de 1950 (quando a capacidade de medi\u00e7\u00e3o come\u00e7a a evoluir Latitude entre 0\u00b0 e 25\u00b0N (remover vi\u00e9s dos registros extratropicais) Fazemos tamb\u00e9m outras limpezas como formata\u00e7\u00e3o de entradas de latitude, longitude e outras vari\u00e1veis que n\u00e3o vieram da forma ideal.","title":"Dataset HURDAT2 (Hurricane) - An\u00e1lise e Limpeza"},{"location":"Data_Cleaning_and_EDA/#uniao-dos-dados-via-k-nn-ponderado","text":"A partir dos dois grandes conjuntos de dados descritos acima criamos um novo dataframe que pega o dataframe HURDAT2 filtrado acima e busca os dados clim\u00e1ticos nos datasets da ICOADS diretamente dos arquivos .nc. Fizemos essa busca via coordenadas e para lidar com dados faltantes implementaremos um k-NN ponderado pelo inverso das dist\u00e2ncias entre as coordenadas originais e o vizinho considerado no algoritmo.","title":"Uni\u00e3o dos dados via k-NN ponderado"},{"location":"Data_Cleaning_and_EDA/#formula-da-distancia","text":"Dados duas coordenadas (\\varphi_1,\\lambda_1) e ( \\varphi_2,\\lambda_2) em radianos, a F\u00f3rmula Haversine \u00e9 capaz de calcular a dist\u00e2ncia real entre esses dois pontos no mapa: Onde r \u00e9 o raio da Terra. Usando r aproximadamente igual a 6371 km o valor de d ser\u00e1 a dist\u00e2ncia em km dos dois pontos dados em coordenadas geogr\u00e1ficas. Detalhes sobre a implementa\u00e7\u00e3o desse c\u00e1lculo est\u00e3o no notebook refer\u00eancia.","title":"Formula da dist\u00e2ncia"},{"location":"Data_Cleaning_and_EDA/#k-nn-ponderado-por-inverso-da-distancia","text":"Implementamos v\u00e1rias func\u00f5es auxiliares para ajudar nessa tarefa de coletar vizinhos para c\u00e1lculo de m\u00e9dia. Detalhes est\u00e3o no notebook refer\u00eancia. Por conta de dados faltantes no ICOADS, nem sempre existir\u00e1 registros para todas as coordenadas. Assim, buscamos os k vizinhos mais pr\u00f3ximos a calculamos a m\u00e9dia entre eles, por\u00e9m a m\u00e9dia \u00e9 ponderada pelo inverso da dist\u00e2ncia entre o vizinho e a coordenada original (usando a F\u00f3rmula Haversive). Sendo assim, pontos mais distantes ter\u00e3o menor peso na m\u00e9dia, enquanto pontos mais pr\u00f3ximos ser\u00e3o considerados mais importantes. Por simplicidade, usamos k=15 . Ap\u00f3s a execu\u00e7\u00e3o dos algoritmos implementados, salvamos o dataframe gerado em um csv e o usamos para outras an\u00e1lises.","title":"k-NN ponderado por inverso da dist\u00e2ncia"},{"location":"Data_Cleaning_and_EDA/#outras-visualizacoes","text":"Nos plots abaixo usaremos a biblioteca troPYcal. Internamente ela tem o dataset HURDAT2 atualizado e possui fun\u00e7\u00f5es de visualiza\u00e7\u00e3o prontas, simples de serem usadas. Refer\u00eancia: https://tropycal.github.io/tropycal/examples/index.html","title":"Outras Visualiza\u00e7\u00f5es"},{"location":"NN-TrackPrediction/","text":"Redes Neurais para Previs\u00e3o de Trajet\u00f3ria Notebook refer\u00eancia: NN-TrackPrediction.ipynb Al\u00e9m de todas as an\u00e1lises feitas, implementamos tamb\u00e9m uma rede neural que usa informa\u00e7\u00f5es de um furac\u00e3o de algumas horas atr\u00e1s e do presente, e projeta a sua posi\u00e7\u00e3o futura em coordenadas geogr\u00e1ficas. Funcionamento da Predi\u00e7\u00e3o O input da rede \u00e9 uma matriz do tipo: \\left[\\begin{matrix} x_1^{(t-2)} & x_2^{(t-2)} & ...& x_n^{(t-2)}\\\\ x_1^{(t-1)} & x_2^{(t-1)} & ...& x_n^{(t-1)}\\\\ x_1^{(t)} & x_2^{(t)} & ...& x_n^{(t)}\\end{matrix}\\right] na qual: x_1,~x_2, ~..., ~x_n representam os n preditores usados. Testamos dois conjuntos de modelos, o primeiro com 4 preditores (Tempo, Latitude, Longitude e Velocidade de vento) e o segundo com 8 (anteriores mais Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade). Fizemos uma compara\u00e7\u00e3o da fun\u00e7\u00e3o de perda entre eles para eleger o melhor conjunto de entrada, e usar 8 preditores apresentou melhores resultados. x_i^{(k)} representa o preditor i no registro de tempo k . Cada registro dos nossos dados est\u00e1 espa\u00e7ado por 6 horas do pr\u00f3ximo e do anterior. Sendo assim, pela matriz acima, usamos um conjunto de 3 registros sequenciais, que representam 18 horas. Nossa sa\u00edda \u00e9 da forma: O vetor Y geral \u00e9 composto por: [lat^{(t+1)},~ lon^{(t+1)}], que representa a latitude e longitude no registro t+1 ou seja, + horas depois do \u00faltimo ponto de treinamento. Treino, Valida\u00e7\u00e3o e Teste Para formatar os dados de treino, teste e valida\u00e7\u00e3o, tivemos que fazer um tratamento diferenciado e manual, para que o treinamento e previs\u00e3o ocorresse tempestade por tempestade e n\u00e3o misturasse dados. Usamos uma divis\u00e3o de 70% para treino, 20% para valida\u00e7\u00e3o e 10% para teste. Para mais detalhes do processo veja o notebook NN-TrackPrediction.ipynb . Anatomia das redes e compara\u00e7\u00e3o dos modelos Fizemos v\u00e1rios modelos com anatomias diferentes apesar de parecidas, elegemos o melhor atrav\u00e9s do MSE (Erros M\u00e9dio Quadr\u00e1tico) nos dados de teste e valida\u00e7\u00e3o. A anatomia desse modelo campe\u00e3o \u00e9 a seguinte: 8 vari\u00e1veis de input (Tempo, Latitude, Longitude e Velocidade de vento, Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade) para cada um dos registros passados. Como usamos 3 registros temos ent\u00e3o 24 neur\u00f4nios de entrada. 1 \u00fanica camada interna com 9 neur\u00f4nios e fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoidal. 2 neur\u00f4nios para a camada de output, um para latitude e outro para longitude, com ativa\u00e7\u00e3o linear. Camada de dropout de 15% para evitar overfitting. C\u00f3digo model_892 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) Performance Tivemos um R2-score de aproximadamente 0.988 nos dados de teste, um resultado excelente! Abaixo h\u00e1 um c\u00f3digo oculto que plota a rela\u00e7\u00e3o bem pr\u00f3xima de linear entre os dados previstos e reais. C\u00f3digo from sklearn.metrics import r2_score ypred = model_892 . predict ( xtest2 ) lat_r2 = r2_score ( ytest2 [:, 0 ], ypred [:, 0 ]) lon_r2 = r2_score ( ytest2 [:, 1 ], ypred [:, 1 ]) tot_r2 = r2_score ( ytest2 , ypred ) print ( f \"R2 Latitude Teste - { lat_r2 } \" ) print ( f \"R2 Longitude Teste - { lon_r2 } \" ) print ( f \"R2 Total Teste - { tot_r2 } \" ) fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 18 , 6 )) ax . set_title ( \"Latitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax . set_xlabel ( \"Real\" , fontsize = 13 ) ax . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax . scatter ( ytest2 [:, 0 ], ypred [:, 0 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lat_r2 , 3 ) } \" ) ax . legend ( loc = 'best' , fontsize = 13 ) ax1 . set_title ( \"Longitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax1 . set_xlabel ( \"Real\" , fontsize = 13 ) ax1 . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax1 . scatter ( ytest2 [:, 1 ], ypred [:, 1 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lon_r2 , 3 ) } \" ) ax1 . legend ( loc = 'best' , fontsize = 13 ); plt . savefig ( '../figs/lat_lon_teste.jpg' ) R2 Latitude Teste - 0.9890722375652974 R2 Longitude Teste - 0.9879249013965508 R2 Total Teste - 0.9884985694809241 Previs\u00f5es Testamos a previs\u00e3o para algumas tempestades espec\u00edficas do conjunto de testes e tivemos bons resultados. A fun\u00e7\u00e3o predict_storm recebe um ID de tempestade, e plota as predi\u00e7\u00f5es al\u00e9m de printar os R2 scores's de Latitude e Longitude nessa ordem. predict_storm ( 'AL092011' ) 0.912396866066715 0.8587266243812284 predict_storm ( 'AL112015' ) 0.9650855905600251 0.9805961966628389 Para mais detalhes do funcionamento da fun\u00e7\u00e3o predict visite o notebook NN-TrackPrediction.ipynb .","title":"Prevendo trajet\u00f3rias"},{"location":"NN-TrackPrediction/#redes-neurais-para-previsao-de-trajetoria","text":"Notebook refer\u00eancia: NN-TrackPrediction.ipynb Al\u00e9m de todas as an\u00e1lises feitas, implementamos tamb\u00e9m uma rede neural que usa informa\u00e7\u00f5es de um furac\u00e3o de algumas horas atr\u00e1s e do presente, e projeta a sua posi\u00e7\u00e3o futura em coordenadas geogr\u00e1ficas.","title":"Redes Neurais para Previs\u00e3o de Trajet\u00f3ria"},{"location":"NN-TrackPrediction/#funcionamento-da-predicao","text":"O input da rede \u00e9 uma matriz do tipo: \\left[\\begin{matrix} x_1^{(t-2)} & x_2^{(t-2)} & ...& x_n^{(t-2)}\\\\ x_1^{(t-1)} & x_2^{(t-1)} & ...& x_n^{(t-1)}\\\\ x_1^{(t)} & x_2^{(t)} & ...& x_n^{(t)}\\end{matrix}\\right] na qual: x_1,~x_2, ~..., ~x_n representam os n preditores usados. Testamos dois conjuntos de modelos, o primeiro com 4 preditores (Tempo, Latitude, Longitude e Velocidade de vento) e o segundo com 8 (anteriores mais Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade). Fizemos uma compara\u00e7\u00e3o da fun\u00e7\u00e3o de perda entre eles para eleger o melhor conjunto de entrada, e usar 8 preditores apresentou melhores resultados. x_i^{(k)} representa o preditor i no registro de tempo k . Cada registro dos nossos dados est\u00e1 espa\u00e7ado por 6 horas do pr\u00f3ximo e do anterior. Sendo assim, pela matriz acima, usamos um conjunto de 3 registros sequenciais, que representam 18 horas. Nossa sa\u00edda \u00e9 da forma: O vetor Y geral \u00e9 composto por: [lat^{(t+1)},~ lon^{(t+1)}], que representa a latitude e longitude no registro t+1 ou seja, + horas depois do \u00faltimo ponto de treinamento.","title":"Funcionamento da Predi\u00e7\u00e3o"},{"location":"NN-TrackPrediction/#treino-validacao-e-teste","text":"Para formatar os dados de treino, teste e valida\u00e7\u00e3o, tivemos que fazer um tratamento diferenciado e manual, para que o treinamento e previs\u00e3o ocorresse tempestade por tempestade e n\u00e3o misturasse dados. Usamos uma divis\u00e3o de 70% para treino, 20% para valida\u00e7\u00e3o e 10% para teste. Para mais detalhes do processo veja o notebook NN-TrackPrediction.ipynb .","title":"Treino, Valida\u00e7\u00e3o e Teste"},{"location":"NN-TrackPrediction/#anatomia-das-redes-e-comparacao-dos-modelos","text":"Fizemos v\u00e1rios modelos com anatomias diferentes apesar de parecidas, elegemos o melhor atrav\u00e9s do MSE (Erros M\u00e9dio Quadr\u00e1tico) nos dados de teste e valida\u00e7\u00e3o. A anatomia desse modelo campe\u00e3o \u00e9 a seguinte: 8 vari\u00e1veis de input (Tempo, Latitude, Longitude e Velocidade de vento, Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade) para cada um dos registros passados. Como usamos 3 registros temos ent\u00e3o 24 neur\u00f4nios de entrada. 1 \u00fanica camada interna com 9 neur\u00f4nios e fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoidal. 2 neur\u00f4nios para a camada de output, um para latitude e outro para longitude, com ativa\u00e7\u00e3o linear. Camada de dropout de 15% para evitar overfitting. C\u00f3digo model_892 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ])","title":"Anatomia das redes e compara\u00e7\u00e3o dos modelos"},{"location":"NN-TrackPrediction/#performance","text":"Tivemos um R2-score de aproximadamente 0.988 nos dados de teste, um resultado excelente! Abaixo h\u00e1 um c\u00f3digo oculto que plota a rela\u00e7\u00e3o bem pr\u00f3xima de linear entre os dados previstos e reais. C\u00f3digo from sklearn.metrics import r2_score ypred = model_892 . predict ( xtest2 ) lat_r2 = r2_score ( ytest2 [:, 0 ], ypred [:, 0 ]) lon_r2 = r2_score ( ytest2 [:, 1 ], ypred [:, 1 ]) tot_r2 = r2_score ( ytest2 , ypred ) print ( f \"R2 Latitude Teste - { lat_r2 } \" ) print ( f \"R2 Longitude Teste - { lon_r2 } \" ) print ( f \"R2 Total Teste - { tot_r2 } \" ) fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 18 , 6 )) ax . set_title ( \"Latitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax . set_xlabel ( \"Real\" , fontsize = 13 ) ax . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax . scatter ( ytest2 [:, 0 ], ypred [:, 0 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lat_r2 , 3 ) } \" ) ax . legend ( loc = 'best' , fontsize = 13 ) ax1 . set_title ( \"Longitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax1 . set_xlabel ( \"Real\" , fontsize = 13 ) ax1 . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax1 . scatter ( ytest2 [:, 1 ], ypred [:, 1 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lon_r2 , 3 ) } \" ) ax1 . legend ( loc = 'best' , fontsize = 13 ); plt . savefig ( '../figs/lat_lon_teste.jpg' ) R2 Latitude Teste - 0.9890722375652974 R2 Longitude Teste - 0.9879249013965508 R2 Total Teste - 0.9884985694809241","title":"Performance"},{"location":"NN-TrackPrediction/#previsoes","text":"Testamos a previs\u00e3o para algumas tempestades espec\u00edficas do conjunto de testes e tivemos bons resultados. A fun\u00e7\u00e3o predict_storm recebe um ID de tempestade, e plota as predi\u00e7\u00f5es al\u00e9m de printar os R2 scores's de Latitude e Longitude nessa ordem. predict_storm ( 'AL092011' ) 0.912396866066715 0.8587266243812284 predict_storm ( 'AL112015' ) 0.9650855905600251 0.9805961966628389 Para mais detalhes do funcionamento da fun\u00e7\u00e3o predict visite o notebook NN-TrackPrediction.ipynb .","title":"Previs\u00f5es"},{"location":"PowerDissipationIndex/","text":"Power Dissipation Index (PDI) Analysis Definitions PDI \u00e9 um \u00edndice criado para medir a destrutividade de um furac\u00e3o, levando em conta sua intensidade por meio de uma transforma\u00e7\u00e3o velocidade m\u00e1xima m\u00e9dia, a sua dura\u00e7\u00e3o e, caso estejamos medindo varios furac\u00f5es, consideramos o n\u00famero deles. References: Emanuel, 2005 and Emanuel, 2007 Nas refer\u00eancias, Kerry Emanuel define o PDI como: PDI\\equiv\\int_0^{\\tau}V^3_{max}dt~, onde V_{max} \u00e9 a velocidade tangencial m\u00e1xima dos ventos do furac\u00e3o e \\tau \u00e9 o tempo de dura\u00e7\u00e3o do furac\u00e3o. The PDI Dataset Vamos usar o PDI calculado pela National Oceanic & Atmospheric Administration (NOAA) por meio do site Our World in Data . Esses dados cobrem todo o Atl\u00e2ntico Norte, o mar do Caribe e o Golfo do M\u00e9xico. Esses dados passam por smooth de fazer m\u00e9dia podendara de 5 observa\u00e7\u00f5es pr\u00f3ximas, filtro esse. Code fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 9 )) axs [ 0 , 0 ] . set_title ( \"Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 0 ] . set_ylabel ( \"PDI\" , fontsize = 16 ) axs [ 0 , 0 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 0 ] . plot ( PDI . index , PDI . PDI , lw = 4 ); axs [ 0 , 1 ] . set_title ( \"Logarithm of Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 1 ] . set_ylabel ( \"log(PDI)\" , fontsize = 16 ) axs [ 0 , 1 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 1 ] . plot ( PDI . index , np . log ( PDI . PDI ), lw = 4 ) sns . distplot ( PDI . PDI , ax = axs [ 1 , 0 ]) sns . distplot ( np . log ( PDI . PDI ), ax = axs [ 1 , 1 ]) Analysing Correlation Uma hip\u00f3tese comum diante das ideias cientificamente popularizadas \u00e9 a de que as mudan\u00e7as clim\u00e1ticas, ao provocarem aquecimento do mar, fazem com que os furac\u00f5es fiquem mais intensos. De certo modo a intui\u00e7\u00e3o poderia falar nessa dire\u00e7\u00e3o sabendo o processo de forma\u00e7\u00e3o dos furac\u00f5es; precisamos ver se essa ideia se mostra nos dados. Code # corr = mdr_annual.corr() # corr.style.background_gradient(cmap='coolwarm') # corr = df.corr() # corr.style.background_gradient(cmap='coolwarm') f , axs = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( mdr_annual . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 0 ]); df = mdr_annual . copy () df [ 'PDI' ] = np . log ( df [ 'PDI' ]) df = df . rename ( columns = { \"PDI\" : \"logPDI\" }) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( df . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 1 ]); plt . show () Tanto o PDI quando o logPDI tiveram correla\u00e7\u00e3o de aproximadamente 0.64 . Code fig , ax = plt . subplots ( figsize = ( 10 , 6 )) plt . title ( \"North Atlantic MDR SST vs PDI\" , fontsize = 18 ) plt . xlabel ( \"Year\" , fontsize = 14 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) ax2 = ax . twinx () ax . plot ( mdr_annual . sst , lw = 4 , label = \"SST\" ); ax . set_ylabel ( \"Sea Surface Temperature\" , fontsize = 14 ) ax2 . plot ( mdr_annual . PDI , lw = 4 , label = \"PDI\" , color = 'orange' ); ax2 . set_ylabel ( \"Power Dissipation Index\" , fontsize = 14 ) fig . legend ( loc = \"upper left\" , fontsize = 14 , bbox_to_anchor = ( 0 , 1 ), bbox_transform = ax . transAxes ); Modelos lineares Quanto \u00e0 trajet\u00f3ria do PDI, \u00e9 observ\u00e1vel uma leve tend\u00eancia positiva tendo nos \u00faltimos 20 anos um comportamento bem irregular, o que pode ser explicado pela excepcionalidade da ENSO (El-Ni\u00f1o Southern Oscilation) nesse per\u00edodo. Code # Aparentemente o PDI cresceu: X = mdr_annual . drop ( columns = \"PDI\" ) X = sm . add_constant ( X ) Y = np . array ( mdr_annual [ 'PDI' ]) . reshape ( - 1 , 1 ) X [ 'Year' ] = np . array ( X . index ) model = sm . OLS ( Y , sm . add_constant ( X [ 'Year' ])) . fit () plt . plot ( PDI ) plt . plot ( model . fittedvalues ) plt . title ( \"Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta\" ) Usando as covari\u00e1veis do dataset, podemos fazer um modelo onde achamos rela\u00e7\u00e3o com a press\u00e3o ao nivel do mar (slp), a humidade (rhum) e a \"cloudiness\", ou nebulosidade (cldc). Code # X['sst'] = np.log(X['sst']) ( xtrain , xtest , ytrain , ytest ) = train_test_split ( X . drop ( columns = [ \"sst\" , \"wspd\" , \"vwnd\" , \"Year\" ]), np . log ( Y )) model = sm . OLS ( ytrain , xtrain ) . fit () ypred = model . predict ( xtest ) print ( \"R2 nos dados separados para teste:\" , r2_score ( ytest , ypred )) print ( model . summary ()) R2 nos dados separados para teste: 0.2268374488496503 OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.781 Model: OLS Adj. R-squared: 0.766 Method: Least Squares F-statistic: 51.26 Date: Mon, 31 Aug 2020 Prob (F-statistic): 2.98e-14 Time: 10:22:52 Log-Likelihood: 14.019 No. Observations: 47 AIC: -20.04 Df Residuals: 43 BIC: -12.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -484.2535 146.318 -3.310 0.002 -779.332 -189.175 rhum 0.7547 0.079 9.497 0.000 0.594 0.915 slp 0.4106 0.143 2.871 0.006 0.122 0.699 cldc 1.9827 0.229 8.641 0.000 1.520 2.445 ============================================================================== Omnibus: 4.476 Durbin-Watson: 2.046 Prob(Omnibus): 0.107 Jarque-Bera (JB): 3.413 Skew: -0.468 Prob(JB): 0.182 Kurtosis: 3.932 Cond. No. 5.42e+06 ============================================================================== Pode parecer um pouco contraintuitivo a princ\u00edpio n\u00e3o achar rela\u00e7\u00e3o significativa da temperatura do mar e da intensidade (na verdade, a destrutibilidade, o PDI) dos furac\u00f5es. Uma explica\u00e7\u00e3o plausivel para isso \u00e9 que os dados de temperatura conseguem ser explicados pelas outras vari\u00e1veis: Code model = sm . OLS ( X [ 'sst' ], X . drop ( columns = [ 'vwnd' , 'wspd' , 'Year' , 'sst' ])) . fit () print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: sst R-squared: 0.749 Model: OLS Adj. R-squared: 0.736 Method: Least Squares F-statistic: 58.61 Date: Mon, 31 Aug 2020 Prob (F-statistic): 1.08e-17 Time: 10:22:52 Log-Likelihood: 53.106 No. Observations: 63 AIC: -98.21 Df Residuals: 59 BIC: -89.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -177.9485 73.852 -2.410 0.019 -325.726 -30.171 rhum 0.2643 0.040 6.528 0.000 0.183 0.345 slp 0.1764 0.072 2.446 0.017 0.032 0.321 cldc 1.3740 0.113 12.204 0.000 1.149 1.599 ============================================================================== Omnibus: 0.296 Durbin-Watson: 0.473 Prob(Omnibus): 0.862 Jarque-Bera (JB): 0.479 Skew: -0.023 Prob(JB): 0.787 Kurtosis: 2.575 Cond. No. 5.52e+06 ============================================================================== Time series Analysis N\u00e3o conseguimos modelar essas varia\u00e7\u00f5es do PDI com o tempo muito bem com os modelos de regress\u00e3o, embora ganhamos algum insight sobre como as outras vari\u00e1veis clim\u00e1ticas est\u00e3o relacionadas com ele. Sendo assim, agora vamos tratar a trajet\u00f3ria do PDI como uma s\u00e9rie temporal; a rela\u00e7\u00e3o dele com o tempo vai nos dar informa\u00e7\u00e3o sobre se \u00e9 esperado que ele cres\u00e7a nos pr\u00f3ximos anos. Inicialmente, vamos ajustar um modelo autorregressivo de ordem 2 sendo que essa ordem do modelo foi escolhida com crit\u00e9rios de informa\u00e7\u00e3o: Code mod = sm . tsa . ARIMA ( tsPDI , order = ( 2 , 0 , 0 )) modfit = mod . fit () # modfit.summary() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia das previs\u00f5es com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Uma outra forma de modelar no tempo que facilita a an\u00e1lise do modelo \u00e9 trabalhar com o log dessa s\u00e9rie. Isso ajuda a diminuir o pico no crescimento da destrutibilidade e induz na escala original do PDI a n\u00e3o ter valores negativos, o que est\u00e1 de acordo com a defini\u00e7\u00e3o do PDI. Code tsLogPDI = tsPDI . apply ( np . log ) #sm.tsa.stattools.adfuller(tsLogPDI['PDI']) ainda n\u00e3o \u00e9 estacionaria a 5% de significancia mod2 = sm . tsa . ARIMA ( tsLogPDI , order = ( 2 , 0 , 0 )) modfit2 = mod2 . fit () # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit2.resid, ax = axs[0, 0]) # plot_pacf(modfit2.resid, ax = axs[0, 1]) # sm.qqplot(modfit2.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit2.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Code fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: sims = np . zeros (( 1000 , step )) for i in range ( 1000 ): sims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( sims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 30 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 15 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Al\u00e9m disso, conseguimos achar uma certa transforma\u00e7\u00e3o nos dados que \u00e9 estacion\u00e1ria; permitindo um bom ajuste do modelo ARIMA. Passando as predi\u00e7\u00f5es para o espa\u00e7o do PDI, temos as seguintes previs\u00f5es: Code # plt.plot(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) # sm.tsa.stattools.adfuller(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) tsTr = np . log ( np . diff ( np . log ( tsPDI [ 'PDI' ])) + 1 ) # terceiro modelo: log(delta(log(PDI))+1) como ARMA(2,1) mod3 = sm . tsa . ARIMA ( tsTr , order = ( 2 , 0 , 1 )) modfit3 = mod3 . fit () # modfit3.summary() # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit3.resid, ax = axs[0, 0]) # plot_pacf(modfit3.resid, ax = axs[0, 1]) # sm.qqplot(modfit3.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit3.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsTr ) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = range ( len ( tsTr ), len ( tsTr ) + step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsTr ) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia da transforma\u00e7\u00e3o predita com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es da transforma\u00e7\u00e3o' ) axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = last + np . cumsum ( dsims [ i ]) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 50 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do log(PDI)' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = np . exp ( last + np . cumsum ( dsims [ i ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Embora todos esses m\u00e9todos tenham alguma razoabilidade, eles as previs\u00f5es claramente tem uma vari\u00e2ncia muito grande, refletindo a dificuldade em prever esse dado dado que ele n\u00e3o tem nenhum padr\u00e3o claro. Count Medir destrutibilidade do furac\u00e3o n\u00e3o \u00e9 coisa trivial; n\u00e3o basta olhar o PDI visto que os nossos dados de PDI podem estar errados por medi\u00e7\u00f5es inconsistentes de velocidade at\u00e9 os anos 90, onde come\u00e7amos a ter um comportamento an\u00f4malo do ENSO; al\u00e9m da din\u00e2mica sazonal de correntes termosal\u00ednicas gerarem sazonalidade nas temperaturas entre dec\u00e1das (Atlantic Multidecadal Oscilation, AMO). Outras alternativas comumente consideradas s\u00e3o a de contagem de furac\u00f5es no ano e de tamanho de cada furac\u00e3o; cada uma delas com seus problemas particulares. Abaixo, fazemos um modelo simples de GLM para contagem usando como preditores as covari Code data = pd . read_csv ( '../Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data = data [[ 'ID' , 'Name' , 'Date' , 'Year' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ]] def category ( mw ): if mw >= 137 : return \"Category 5\" elif mw >= 113 : return \"Category 4\" elif mw >= 96 : return \"Category 3\" elif mw >= 83 : return \"Category 2\" elif mw >= 64 : return \"Category 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" cat_id = data . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) data . loc [:, 'Category' ] = data . ID . apply ( lambda x : cat_id [ x ]) # data.groupby('Category')['ID'].count().plot.bar(); major_df = data [ data . Category . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] #major_df.groupby('Year')['ID'].count().plot() #sm.tsa.stattools.adfuller(count) count = major_df . groupby ( \"Year\" )[ \"ID\" ] . count () X = pd . pivot_table ( major_df , index = \"Year\" , values = [ \"sst\" , \"rhum\" , \"wspd\" , \"slp\" , \"cldc\" ]) X [ 'Y' ] = X . index . values countModel = sm . GLM ( count , sm . add_constant ( X ), family = sm . families . NegativeBinomial ()) countFit = countModel . fit () print ( countFit . summary ()) #descomente para ver o sumario do ajuste plt . plot ( count ) plt . plot ( countFit . predict ( sm . add_constant ( X ))) plt . show () Generalized Linear Model Regression Results ============================================================================== Dep. Variable: ID No. Observations: 60 Model: GLM Df Residuals: 54 Model Family: NegativeBinomial Df Model: 5 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -347.61 Date: Mon, 31 Aug 2020 Deviance: 26.626 Time: 10:22:58 Pearson chi2: 26.3 No. Iterations: 12 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -146.2316 147.049 -0.994 0.320 -434.443 141.980 cldc 0.1977 0.589 0.336 0.737 -0.956 1.351 rhum 0.0182 0.067 0.271 0.786 -0.113 0.150 slp 0.1338 0.142 0.940 0.347 -0.145 0.413 sst 0.0952 0.132 0.723 0.470 -0.163 0.353 wspd 0.0182 0.067 0.271 0.786 -0.113 0.150 Y 0.0048 0.008 0.602 0.547 -0.011 0.020 ============================================================================== Olhando esse gr\u00e1fico do ajuste e a tabela com as estat\u00edsticas do ajuste, vemos que o modelo da binomial negativa claramente n\u00e3o consegue capturar muitas informa\u00e7\u00f5es relevantes que o n\u00famero de furac\u00f5es nos d\u00e1. Outra medida tamb\u00e9m comum de destrutividade dos furac\u00f5es \u00e9 o tamanho do raio, que n\u00e3o ser\u00e1 analisado nesse trabalho.","title":"Destrutividade no longo prazo"},{"location":"PowerDissipationIndex/#power-dissipation-index-pdi-analysis","text":"","title":"Power Dissipation Index (PDI) Analysis"},{"location":"PowerDissipationIndex/#definitions","text":"PDI \u00e9 um \u00edndice criado para medir a destrutividade de um furac\u00e3o, levando em conta sua intensidade por meio de uma transforma\u00e7\u00e3o velocidade m\u00e1xima m\u00e9dia, a sua dura\u00e7\u00e3o e, caso estejamos medindo varios furac\u00f5es, consideramos o n\u00famero deles. References: Emanuel, 2005 and Emanuel, 2007 Nas refer\u00eancias, Kerry Emanuel define o PDI como: PDI\\equiv\\int_0^{\\tau}V^3_{max}dt~, onde V_{max} \u00e9 a velocidade tangencial m\u00e1xima dos ventos do furac\u00e3o e \\tau \u00e9 o tempo de dura\u00e7\u00e3o do furac\u00e3o.","title":"Definitions"},{"location":"PowerDissipationIndex/#the-pdi-dataset","text":"Vamos usar o PDI calculado pela National Oceanic & Atmospheric Administration (NOAA) por meio do site Our World in Data . Esses dados cobrem todo o Atl\u00e2ntico Norte, o mar do Caribe e o Golfo do M\u00e9xico. Esses dados passam por smooth de fazer m\u00e9dia podendara de 5 observa\u00e7\u00f5es pr\u00f3ximas, filtro esse. Code fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 9 )) axs [ 0 , 0 ] . set_title ( \"Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 0 ] . set_ylabel ( \"PDI\" , fontsize = 16 ) axs [ 0 , 0 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 0 ] . plot ( PDI . index , PDI . PDI , lw = 4 ); axs [ 0 , 1 ] . set_title ( \"Logarithm of Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 1 ] . set_ylabel ( \"log(PDI)\" , fontsize = 16 ) axs [ 0 , 1 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 1 ] . plot ( PDI . index , np . log ( PDI . PDI ), lw = 4 ) sns . distplot ( PDI . PDI , ax = axs [ 1 , 0 ]) sns . distplot ( np . log ( PDI . PDI ), ax = axs [ 1 , 1 ])","title":"The PDI Dataset"},{"location":"PowerDissipationIndex/#analysing-correlation","text":"Uma hip\u00f3tese comum diante das ideias cientificamente popularizadas \u00e9 a de que as mudan\u00e7as clim\u00e1ticas, ao provocarem aquecimento do mar, fazem com que os furac\u00f5es fiquem mais intensos. De certo modo a intui\u00e7\u00e3o poderia falar nessa dire\u00e7\u00e3o sabendo o processo de forma\u00e7\u00e3o dos furac\u00f5es; precisamos ver se essa ideia se mostra nos dados. Code # corr = mdr_annual.corr() # corr.style.background_gradient(cmap='coolwarm') # corr = df.corr() # corr.style.background_gradient(cmap='coolwarm') f , axs = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( mdr_annual . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 0 ]); df = mdr_annual . copy () df [ 'PDI' ] = np . log ( df [ 'PDI' ]) df = df . rename ( columns = { \"PDI\" : \"logPDI\" }) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( df . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 1 ]); plt . show () Tanto o PDI quando o logPDI tiveram correla\u00e7\u00e3o de aproximadamente 0.64 . Code fig , ax = plt . subplots ( figsize = ( 10 , 6 )) plt . title ( \"North Atlantic MDR SST vs PDI\" , fontsize = 18 ) plt . xlabel ( \"Year\" , fontsize = 14 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) ax2 = ax . twinx () ax . plot ( mdr_annual . sst , lw = 4 , label = \"SST\" ); ax . set_ylabel ( \"Sea Surface Temperature\" , fontsize = 14 ) ax2 . plot ( mdr_annual . PDI , lw = 4 , label = \"PDI\" , color = 'orange' ); ax2 . set_ylabel ( \"Power Dissipation Index\" , fontsize = 14 ) fig . legend ( loc = \"upper left\" , fontsize = 14 , bbox_to_anchor = ( 0 , 1 ), bbox_transform = ax . transAxes );","title":"Analysing Correlation"},{"location":"PowerDissipationIndex/#modelos-lineares","text":"Quanto \u00e0 trajet\u00f3ria do PDI, \u00e9 observ\u00e1vel uma leve tend\u00eancia positiva tendo nos \u00faltimos 20 anos um comportamento bem irregular, o que pode ser explicado pela excepcionalidade da ENSO (El-Ni\u00f1o Southern Oscilation) nesse per\u00edodo. Code # Aparentemente o PDI cresceu: X = mdr_annual . drop ( columns = \"PDI\" ) X = sm . add_constant ( X ) Y = np . array ( mdr_annual [ 'PDI' ]) . reshape ( - 1 , 1 ) X [ 'Year' ] = np . array ( X . index ) model = sm . OLS ( Y , sm . add_constant ( X [ 'Year' ])) . fit () plt . plot ( PDI ) plt . plot ( model . fittedvalues ) plt . title ( \"Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta\" ) Usando as covari\u00e1veis do dataset, podemos fazer um modelo onde achamos rela\u00e7\u00e3o com a press\u00e3o ao nivel do mar (slp), a humidade (rhum) e a \"cloudiness\", ou nebulosidade (cldc). Code # X['sst'] = np.log(X['sst']) ( xtrain , xtest , ytrain , ytest ) = train_test_split ( X . drop ( columns = [ \"sst\" , \"wspd\" , \"vwnd\" , \"Year\" ]), np . log ( Y )) model = sm . OLS ( ytrain , xtrain ) . fit () ypred = model . predict ( xtest ) print ( \"R2 nos dados separados para teste:\" , r2_score ( ytest , ypred )) print ( model . summary ()) R2 nos dados separados para teste: 0.2268374488496503 OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.781 Model: OLS Adj. R-squared: 0.766 Method: Least Squares F-statistic: 51.26 Date: Mon, 31 Aug 2020 Prob (F-statistic): 2.98e-14 Time: 10:22:52 Log-Likelihood: 14.019 No. Observations: 47 AIC: -20.04 Df Residuals: 43 BIC: -12.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -484.2535 146.318 -3.310 0.002 -779.332 -189.175 rhum 0.7547 0.079 9.497 0.000 0.594 0.915 slp 0.4106 0.143 2.871 0.006 0.122 0.699 cldc 1.9827 0.229 8.641 0.000 1.520 2.445 ============================================================================== Omnibus: 4.476 Durbin-Watson: 2.046 Prob(Omnibus): 0.107 Jarque-Bera (JB): 3.413 Skew: -0.468 Prob(JB): 0.182 Kurtosis: 3.932 Cond. No. 5.42e+06 ============================================================================== Pode parecer um pouco contraintuitivo a princ\u00edpio n\u00e3o achar rela\u00e7\u00e3o significativa da temperatura do mar e da intensidade (na verdade, a destrutibilidade, o PDI) dos furac\u00f5es. Uma explica\u00e7\u00e3o plausivel para isso \u00e9 que os dados de temperatura conseguem ser explicados pelas outras vari\u00e1veis: Code model = sm . OLS ( X [ 'sst' ], X . drop ( columns = [ 'vwnd' , 'wspd' , 'Year' , 'sst' ])) . fit () print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: sst R-squared: 0.749 Model: OLS Adj. R-squared: 0.736 Method: Least Squares F-statistic: 58.61 Date: Mon, 31 Aug 2020 Prob (F-statistic): 1.08e-17 Time: 10:22:52 Log-Likelihood: 53.106 No. Observations: 63 AIC: -98.21 Df Residuals: 59 BIC: -89.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -177.9485 73.852 -2.410 0.019 -325.726 -30.171 rhum 0.2643 0.040 6.528 0.000 0.183 0.345 slp 0.1764 0.072 2.446 0.017 0.032 0.321 cldc 1.3740 0.113 12.204 0.000 1.149 1.599 ============================================================================== Omnibus: 0.296 Durbin-Watson: 0.473 Prob(Omnibus): 0.862 Jarque-Bera (JB): 0.479 Skew: -0.023 Prob(JB): 0.787 Kurtosis: 2.575 Cond. No. 5.52e+06 ==============================================================================","title":"Modelos lineares"},{"location":"PowerDissipationIndex/#time-series-analysis","text":"N\u00e3o conseguimos modelar essas varia\u00e7\u00f5es do PDI com o tempo muito bem com os modelos de regress\u00e3o, embora ganhamos algum insight sobre como as outras vari\u00e1veis clim\u00e1ticas est\u00e3o relacionadas com ele. Sendo assim, agora vamos tratar a trajet\u00f3ria do PDI como uma s\u00e9rie temporal; a rela\u00e7\u00e3o dele com o tempo vai nos dar informa\u00e7\u00e3o sobre se \u00e9 esperado que ele cres\u00e7a nos pr\u00f3ximos anos. Inicialmente, vamos ajustar um modelo autorregressivo de ordem 2 sendo que essa ordem do modelo foi escolhida com crit\u00e9rios de informa\u00e7\u00e3o: Code mod = sm . tsa . ARIMA ( tsPDI , order = ( 2 , 0 , 0 )) modfit = mod . fit () # modfit.summary() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia das previs\u00f5es com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Uma outra forma de modelar no tempo que facilita a an\u00e1lise do modelo \u00e9 trabalhar com o log dessa s\u00e9rie. Isso ajuda a diminuir o pico no crescimento da destrutibilidade e induz na escala original do PDI a n\u00e3o ter valores negativos, o que est\u00e1 de acordo com a defini\u00e7\u00e3o do PDI. Code tsLogPDI = tsPDI . apply ( np . log ) #sm.tsa.stattools.adfuller(tsLogPDI['PDI']) ainda n\u00e3o \u00e9 estacionaria a 5% de significancia mod2 = sm . tsa . ARIMA ( tsLogPDI , order = ( 2 , 0 , 0 )) modfit2 = mod2 . fit () # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit2.resid, ax = axs[0, 0]) # plot_pacf(modfit2.resid, ax = axs[0, 1]) # sm.qqplot(modfit2.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit2.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Code fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: sims = np . zeros (( 1000 , step )) for i in range ( 1000 ): sims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( sims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 30 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 15 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Al\u00e9m disso, conseguimos achar uma certa transforma\u00e7\u00e3o nos dados que \u00e9 estacion\u00e1ria; permitindo um bom ajuste do modelo ARIMA. Passando as predi\u00e7\u00f5es para o espa\u00e7o do PDI, temos as seguintes previs\u00f5es: Code # plt.plot(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) # sm.tsa.stattools.adfuller(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) tsTr = np . log ( np . diff ( np . log ( tsPDI [ 'PDI' ])) + 1 ) # terceiro modelo: log(delta(log(PDI))+1) como ARMA(2,1) mod3 = sm . tsa . ARIMA ( tsTr , order = ( 2 , 0 , 1 )) modfit3 = mod3 . fit () # modfit3.summary() # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit3.resid, ax = axs[0, 0]) # plot_pacf(modfit3.resid, ax = axs[0, 1]) # sm.qqplot(modfit3.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit3.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsTr ) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = range ( len ( tsTr ), len ( tsTr ) + step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsTr ) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia da transforma\u00e7\u00e3o predita com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es da transforma\u00e7\u00e3o' ) axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = last + np . cumsum ( dsims [ i ]) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 50 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do log(PDI)' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = np . exp ( last + np . cumsum ( dsims [ i ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Embora todos esses m\u00e9todos tenham alguma razoabilidade, eles as previs\u00f5es claramente tem uma vari\u00e2ncia muito grande, refletindo a dificuldade em prever esse dado dado que ele n\u00e3o tem nenhum padr\u00e3o claro.","title":"Time series Analysis"},{"location":"PowerDissipationIndex/#count","text":"Medir destrutibilidade do furac\u00e3o n\u00e3o \u00e9 coisa trivial; n\u00e3o basta olhar o PDI visto que os nossos dados de PDI podem estar errados por medi\u00e7\u00f5es inconsistentes de velocidade at\u00e9 os anos 90, onde come\u00e7amos a ter um comportamento an\u00f4malo do ENSO; al\u00e9m da din\u00e2mica sazonal de correntes termosal\u00ednicas gerarem sazonalidade nas temperaturas entre dec\u00e1das (Atlantic Multidecadal Oscilation, AMO). Outras alternativas comumente consideradas s\u00e3o a de contagem de furac\u00f5es no ano e de tamanho de cada furac\u00e3o; cada uma delas com seus problemas particulares. Abaixo, fazemos um modelo simples de GLM para contagem usando como preditores as covari Code data = pd . read_csv ( '../Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data = data [[ 'ID' , 'Name' , 'Date' , 'Year' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ]] def category ( mw ): if mw >= 137 : return \"Category 5\" elif mw >= 113 : return \"Category 4\" elif mw >= 96 : return \"Category 3\" elif mw >= 83 : return \"Category 2\" elif mw >= 64 : return \"Category 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" cat_id = data . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) data . loc [:, 'Category' ] = data . ID . apply ( lambda x : cat_id [ x ]) # data.groupby('Category')['ID'].count().plot.bar(); major_df = data [ data . Category . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] #major_df.groupby('Year')['ID'].count().plot() #sm.tsa.stattools.adfuller(count) count = major_df . groupby ( \"Year\" )[ \"ID\" ] . count () X = pd . pivot_table ( major_df , index = \"Year\" , values = [ \"sst\" , \"rhum\" , \"wspd\" , \"slp\" , \"cldc\" ]) X [ 'Y' ] = X . index . values countModel = sm . GLM ( count , sm . add_constant ( X ), family = sm . families . NegativeBinomial ()) countFit = countModel . fit () print ( countFit . summary ()) #descomente para ver o sumario do ajuste plt . plot ( count ) plt . plot ( countFit . predict ( sm . add_constant ( X ))) plt . show () Generalized Linear Model Regression Results ============================================================================== Dep. Variable: ID No. Observations: 60 Model: GLM Df Residuals: 54 Model Family: NegativeBinomial Df Model: 5 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -347.61 Date: Mon, 31 Aug 2020 Deviance: 26.626 Time: 10:22:58 Pearson chi2: 26.3 No. Iterations: 12 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -146.2316 147.049 -0.994 0.320 -434.443 141.980 cldc 0.1977 0.589 0.336 0.737 -0.956 1.351 rhum 0.0182 0.067 0.271 0.786 -0.113 0.150 slp 0.1338 0.142 0.940 0.347 -0.145 0.413 sst 0.0952 0.132 0.723 0.470 -0.163 0.353 wspd 0.0182 0.067 0.271 0.786 -0.113 0.150 Y 0.0048 0.008 0.602 0.547 -0.011 0.020 ============================================================================== Olhando esse gr\u00e1fico do ajuste e a tabela com as estat\u00edsticas do ajuste, vemos que o modelo da binomial negativa claramente n\u00e3o consegue capturar muitas informa\u00e7\u00f5es relevantes que o n\u00famero de furac\u00f5es nos d\u00e1. Outra medida tamb\u00e9m comum de destrutividade dos furac\u00f5es \u00e9 o tamanho do raio, que n\u00e3o ser\u00e1 analisado nesse trabalho.","title":"Count"}]}