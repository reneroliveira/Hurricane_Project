{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Oi (\u00e9 para mudar) No momento de confec\u00e7\u00e3o desse trabalho, o furac\u00e3o Laura estava atingindo Lousiana, o estado americano, causando a morte de pelo menos 8 pessoas e um preju\u00edzo de aproximadamente 9 bilh\u00f5es de dol\u00e1res. Olhando esse caso e considerando que casos como esse acontecem todo ano, tempestades tropicais s\u00e3o, variando com a intensidade, um problema para a humanidade tanto do ponto de vista econ\u00f4mico quanto do humanit\u00e1rio. Colocado esse problema, nossa proposta \u00e9 analisar o comportamento de curto prazo de furac\u00f5es e achar dire\u00e7\u00f5es que nos levem a entender como podemos minimizar esses constantes danos de furac\u00f5es.","title":"Home"},{"location":"#oi-e-para-mudar","text":"No momento de confec\u00e7\u00e3o desse trabalho, o furac\u00e3o Laura estava atingindo Lousiana, o estado americano, causando a morte de pelo menos 8 pessoas e um preju\u00edzo de aproximadamente 9 bilh\u00f5es de dol\u00e1res. Olhando esse caso e considerando que casos como esse acontecem todo ano, tempestades tropicais s\u00e3o, variando com a intensidade, um problema para a humanidade tanto do ponto de vista econ\u00f4mico quanto do humanit\u00e1rio. Colocado esse problema, nossa proposta \u00e9 analisar o comportamento de curto prazo de furac\u00f5es e achar dire\u00e7\u00f5es que nos levem a entender como podemos minimizar esses constantes danos de furac\u00f5es.","title":"Oi (\u00e9 para mudar)"},{"location":"Analises_variaveis/","text":"Aplica\u00e7\u00e3o de Modelos de Regress\u00e3o Neste Notebook, iremos importar os dados j\u00e1 limpados/filtrados em etapas anteriores e iremos aplicar diversos algoritmos visando ajustar uma boa regress\u00e3o que nos ajude a prever as velocidades dos ventos de um determinado evento (Tropical Storm ou Hurricane) ou a dura\u00e7\u00e3o dos mesmos. Algumas pequenas transforma\u00e7\u00f5es ser\u00e3o necess\u00e1rias para ajuste das vari\u00e1veis preditoras a serem consideradas em cada modelo. Instala\u00e7\u00e3o e Importa\u00e7\u00e3o de Pacotes Os c\u00f3digos de instala\u00e7\u00e3o abaixo est\u00e3o comentados. Na primeira utiliza\u00e7\u00e3o do Notebook, pode-se descoment\u00e1-los para instalar os pacotes que ainda n\u00e3o estejam instalados no computador que se esteja utilizando. Al\u00e9m disso, alguns pacotes abaixo s\u00e3o para utiliza\u00e7\u00e3o nos demais notebooks do projeto. # !apt-get install libgeos-3.5.0 # !apt-get install libgeos-dev # !pip install https://github.com/matplotlib/basemap/archive/master.zip # !pip install pyproj==1.9.6 # !pip install netcdf4 # Dependencies for Tropycal Package: https://pypi.org/project/tropycal/ # matplotlib >= 2.2.2 # numpy >= 1.14.3 # scipy >= 1.1.0 # pandas >= 0.23.0 # geopy >= 1.18.1 # xarray >= 0.10.7 # networkx >= 2.0.0 # requests >= 2.22.0 # To fully leverage tropycal's plotting capabilities, it is strongly recommended to have cartopy >= 0.17.0 installed. # !pip install numpy # !pip install pandas # !pip install matplotlib # !pip install scipy # !pip install geopy # !pip install xarray # !pip install networkx # !pip install requests # !pip install proj # !pip install proj-data # !pip install geos # #!pip uninstall cartopy # !apt-get -qq install python-cartopy python3-cartopy # !pip install cartopy # !pip install tropycal # #!pip freeze % matplotlib inline import numpy as np import matplotlib import matplotlib.pyplot as plt import matplotlib.colors as c #from mpl_toolkits.basemap import Basemap,shiftgrid import pandas as pd import netCDF4 as nc from itertools import chain from netCDF4 import Dataset from netCDF4 import date2index from datetime import datetime import scipy import geopy import xarray import networkx import requests import cartopy import tropycal import tropycal.tracks as tracks import tropycal.tornado as tornado import datetime as dt import statsmodels.api as sm from statsmodels.api import OLS # Modelos para Testes Futuros de Regress\u00e3o from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.preprocessing import PolynomialFeatures from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold import math from scipy.special import gamma import seaborn as sns sns . set () alpha = 0.5 from sklearn import preprocessing from pandas.plotting import scatter_matrix from sklearn.ensemble import RandomForestRegressor from sklearn.neural_network import MLPRegressor from sklearn.model_selection import train_test_split from sklearn import svm #from sklearn.naive_bayes import GaussianNB from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GroupShuffleSplit from sklearn.model_selection import GroupKFold from sklearn.datasets import make_friedman2 from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import DotProduct , WhiteKernel Carregamento e Prepara\u00e7\u00e3o dos Dados #Importa\u00e7\u00e3o dos dados previamente estruturados data_atl_merged = pd . read_csv ( '../Datasets/data_atl_merged2.csv' ) print ( data_atl_merged . columns ) data_atl_merged . head () Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Date_c', 'Year', 'Month', 'Day', 'Latitude_c', 'Longitude_c', 'Duration', 'sst', 'rhum', 'wspd', 'slp', 'cldc'], dtype='object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Month Day Latitude_c Longitude_c Duration sst rhum wspd slp cldc 0 AL011951 UNNAMED 1951-01-02 1200 NaN EX 30.5 -58.0 50 -999 ... 1 2 30.5 -58.0 10 12.371029 72.457094 72.457094 1027.446055 5.219256 1 AL011951 UNNAMED 1951-01-02 1800 NaN EX 29.9 -56.8 45 -999 ... 1 2 29.9 -56.8 10 16.217764 80.423854 80.423854 1022.672978 5.756614 2 AL011951 UNNAMED 1951-01-03 0 NaN EX 29.0 -55.7 45 -999 ... 1 3 29.0 -55.7 10 15.491124 81.121590 81.121590 1023.151909 5.782474 3 AL011951 UNNAMED 1951-01-03 600 NaN EX 27.5 -54.8 45 -999 ... 1 3 27.5 -54.8 10 22.268075 84.264761 84.264761 1019.325138 5.651842 4 AL011951 UNNAMED 1951-01-03 1200 NaN EX 26.5 -54.5 45 -999 ... 1 3 26.5 -54.5 10 22.336746 84.458617 84.458617 1019.305866 5.645406 5 rows \u00d7 22 columns y_train_mw = data_atl_merged [ 'Maximum Wind' ] y_train_mp = data_atl_merged [ 'Minimum Pressure' ] y_train_mp [ y_train_mp < 0 ] = - 1 y_train_dur = data_atl_merged [ 'Duration' ] X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' ], 1 ) #print(y_train_rad) print ( X_train . columns ) X_train . head () Index ([ 'Latitude' , 'Longitude' , 'Year' , 'Month' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ], dtype = 'object' ) < ipython - input - 16 - 44 f50b930158 > : 3 : SettingWithCopyWarning : A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation : https : // pandas . pydata . org / pandas - docs / stable / user_guide / indexing . html # returning - a - view - versus - a - copy y_train_mp [ y_train_mp < 0 ] = - 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Latitude Longitude Year Month sst rhum wspd slp cldc 0 30.5 -58.0 1951 1 12.371029 72.457094 72.457094 1027.446055 5.219256 1 29.9 -56.8 1951 1 16.217764 80.423854 80.423854 1022.672978 5.756614 2 29.0 -55.7 1951 1 15.491124 81.121590 81.121590 1023.151909 5.782474 3 27.5 -54.8 1951 1 22.268075 84.264761 84.264761 1019.325138 5.651842 4 26.5 -54.5 1951 1 22.336746 84.458617 84.458617 1019.305866 5.645406 Modelos X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) print ( f 'Par\u00e2metro_Month = { OLSModel . params [ 2 ] } ' ) print ( f 'Par\u00e2metro_Latitude = { OLSModel . params [ 3 ] } ' ) print ( f 'Par\u00e2metro_Longitude = { OLSModel . params [ 4 ] } ' ) print ( f 'Par\u00e2metro_sst = { OLSModel . params [ 5 ] } ' ) print ( f 'Par\u00e2metro_rhum = { OLSModel . params [ 6 ] } ' ) print ( f 'Par\u00e2metro_wspd = { OLSModel . params [ 7 ] } ' ) print ( f 'Par\u00e2metro_slp = { OLSModel . params [ 8 ] } ' ) print ( f 'Par\u00e2metro_cldc = { OLSModel . params [ 9 ] } ' ) R^2_train = 0.02385621171100183 Par\u00e2metro_const = 47.35464784180999 Par\u00e2metro_Year = 0.09050506606867095 Par\u00e2metro_Month = -0.05872870746380546 Par\u00e2metro_Latitude = -0.08576970286238517 Par\u00e2metro_Longitude = 1.880119707824508 Par\u00e2metro_sst = 0.15179194438994867 Par\u00e2metro_rhum = 0.028283243122749335 Par\u00e2metro_wspd = 0.028283243122749446 Par\u00e2metro_slp = 0.14971356534654948 Par\u00e2metro_cldc = -1.5161434590996923 Os c\u00f3digos abaixos nos geras uma visualiza\u00e7\u00e3o que pode trazer insights a respeito da rela\u00e7\u00e3o entre as vari\u00e1veis. A escolha das vari\u00e1veis preditoras que servem de entrada para os modelos mais a frente foram pensadas tamb\u00e9m pela observa\u00e7\u00e3o destes gr\u00e1ficos. df = pd . concat ([ X_train , y_train_mw , y_train_mp ], axis = 1 ) scatter_matrix ( df , alpha = 0.8 , figsize = ( 15 , 15 ), diagonal = 'kde' ); # A princ\u00edpio, n\u00e3o queremos que se fa\u00e7a alguma previs\u00e3o com base no valor num\u00e9rico do ano # Al\u00e9m disso, a vari\u00e1vel wspd est\u00e1 altamente correlacionada com a rhum, podendo ser mantida apenas a \u00faltima X_train = data_atl_merged . drop ([ 'Year' , 'wspd' ], 1 ) # M\u00eas Latitude Longitude Temperatura, Umidade, Sea Level Pressure, Cloudiness] # ['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] fig , ax = plt . subplots ( 1 , 7 ) #, figsize=(16,10)) fig . suptitle ( 'Velocidade M\u00e1xima vs Vari\u00e1veis Preditoras (1950-2015)' , fontsize = 28 , y = 1.06 ) ax [ 0 ] . scatter ( X_train [ 'Month' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 1 ] . scatter ( X_train [ 'Latitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 2 ] . scatter ( X_train [ 'Longitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 3 ] . scatter ( X_train [ 'sst' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 4 ] . scatter ( X_train [ 'rhum' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 5 ] . scatter ( X_train [ 'slp' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 6 ] . scatter ( X_train [ 'cldc' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') #ax.plot([1850, 2015], [w0, w1], ls = '-.') #, label=r'$Furac\u00f5es$ $=$ $0$') ''' ax.tick_params(labelsize=24) ax.set_title(f'N\u00famero de Furac\u00f5es Anuais (1851-2015)', fontsize=24) ax.set_xlabel(r'$Ano$', fontsize=16) ax.set_ylabel(r'$Quantidade$', fontsize=16) ''' #fig.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'Year' , 'wspd' ], 1 ) X_train . mean () Latitude 27.466819 Longitude -62.607058 Month 8.709729 sst 26.416134 rhum 81.284651 slp 1009.678574 cldc 5.137222 dtype: float64 Uma primeira tentativa de ajuste foi feito atrav\u00e9s da centraliza\u00e7\u00e3o das vari\u00e1veis preditoras em rela\u00e7\u00e3o \u00e0 m\u00e9dia, adicionando tamb\u00e9m termos polinomiais de segunda ordem. No entanto, os resultados do ajuste n\u00e3o mostraram ganhos significativos para o modelo de Regress\u00e3o Linear M\u00faltipla, e at\u00e9 prejudicaram modelos mais complexos, como Random Forest, Multi Layer Perceptron, entre outros utilizados mais a frente. # Tentativa de ajuste atrav\u00e9s de transforma\u00e7\u00e3o de vari\u00e1veis e inclus\u00e3o de termos polinomiais X_train [ 'Month_mod' ] = ( X_train [ 'Month' ] - 8.788781 ) X_train [ 'Latitude_mod' ] = ( X_train [ 'Latitude' ] - 17.943038 ) X_train [ 'Longitude_mod' ] = ( X_train [ 'Longitude' ] + 62.555538 ) X_train [ 'sst_mod' ] = ( X_train [ 'sst' ] - 28.888685 ) X_train [ 'rhum_mod' ] = ( X_train [ 'rhum' ] - 82.051702 ) X_train [ 'slp_mod' ] = ( X_train [ 'slp' ] - 1008.611661 ) X_train [ 'cldc_mod' ] = ( X_train [ 'cldc' ] - 5.223518 ) X_train [ 'Month_mod^2' ] = X_train [ 'Month_mod' ] ** 2 X_train [ 'Latitude_mod^2' ] = X_train [ 'Latitude_mod' ] ** 2 X_train [ 'Longitude_mod^2' ] = X_train [ 'Longitude_mod' ] ** 2 X_train [ 'sst_mod^2' ] = X_train [ 'sst_mod' ] ** 2 X_train [ 'rhum_mod^2' ] = X_train [ 'rhum_mod' ] ** 2 X_train [ 'slp_mod^2' ] = X_train [ 'slp_mod' ] ** 2 X_train [ 'cldc_mod^2' ] = X_train [ 'cldc_mod' ] ** 2 X_train = X_train . drop ([ 'Month' , 'Latitude' , 'Longitude' , 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) X_train . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Month_mod Latitude_mod Longitude_mod sst_mod rhum_mod slp_mod cldc_mod Month_mod^2 Latitude_mod^2 Longitude_mod^2 sst_mod^2 rhum_mod^2 slp_mod^2 cldc_mod^2 0 -7.788781 12.556962 4.555538 -16.517656 -9.594608 18.834394 -0.004262 60.665109 157.677295 20.752926 272.832952 92.056509 354.734396 0.000018 1 -7.788781 11.956962 5.755538 -12.670921 -1.627848 14.061317 0.533096 60.665109 142.968940 33.126218 160.552231 2.649888 197.720622 0.284192 2 -7.788781 11.056962 6.855538 -13.397561 -0.930112 14.540248 0.558956 60.665109 122.256409 46.998401 179.494647 0.865108 211.418806 0.312431 3 -7.788781 9.556962 7.755538 -6.620610 2.213059 10.713477 0.428324 60.665109 91.335523 60.148370 43.832480 4.897629 114.778588 0.183461 4 -7.788781 8.556962 8.055538 -6.551939 2.406915 10.694205 0.421888 60.665109 73.221599 64.891692 42.927902 5.793238 114.366028 0.177989 Regress\u00e3o Linear X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Month = { OLSModel . params [ 1 ] } ' ) print ( f 'Par\u00e2metro_Latitude = { OLSModel . params [ 2 ] } ' ) print ( f 'Par\u00e2metro_Longitude = { OLSModel . params [ 3 ] } ' ) print ( f 'Par\u00e2metro_sst = { OLSModel . params [ 4 ] } ' ) print ( f 'Par\u00e2metro_rhum = { OLSModel . params [ 5 ] } ' ) print ( f 'Par\u00e2metro_slp = { OLSModel . params [ 6 ] } ' ) print ( f 'Par\u00e2metro_cldc = { OLSModel . params [ 7 ] } ' ) print ( f 'Par\u00e2metro_Month^2 = { OLSModel . params [ 8 ] } ' ) print ( f 'Par\u00e2metro_Latitude^2 = { OLSModel . params [ 9 ] } ' ) print ( f 'Par\u00e2metro_Longitude^2 = { OLSModel . params [ 10 ] } ' ) print ( f 'Par\u00e2metro_sst^2 = { OLSModel . params [ 11 ] } ' ) print ( f 'Par\u00e2metro_rhum^2 = { OLSModel . params [ 12 ] } ' ) print ( f 'Par\u00e2metro_slp^2 = { OLSModel . params [ 13 ] } ' ) print ( f 'Par\u00e2metro_cldc^2 = { OLSModel . params [ 14 ] } ' ) R^2_train = 0.057796565423244184 Par\u00e2metro_const = 55.44314399378881 Par\u00e2metro_Month = 0.8453623447457261 Par\u00e2metro_Latitude = 0.3457362098822069 Par\u00e2metro_Longitude = -0.02891517619557605 Par\u00e2metro_sst = 0.02964116885731194 Par\u00e2metro_rhum = 0.09964668732077198 Par\u00e2metro_slp = 0.289107953297979 Par\u00e2metro_cldc = -0.5559643046414668 Par\u00e2metro_Month^2 = -0.48473747846473236 Par\u00e2metro_Latitude^2 = -0.017728720785750357 Par\u00e2metro_Longitude^2 = -0.00855022784566639 Par\u00e2metro_sst^2 = 0.051566532947100174 Par\u00e2metro_rhum^2 = 0.002209669018392528 Par\u00e2metro_slp^2 = -0.03379242085061111 Par\u00e2metro_cldc^2 = -0.16931736469014314 X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'Year' , 'wspd' ], 1 ) y_train_mw = data_atl_merged [ 'Maximum Wind' ] X_train , X_test , y_train_mw , y_test_mw = train_test_split ( X_train , y_train_mw , random_state = 1 ) print ( len ( X_train )) X_train . head () 16789 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Latitude Longitude Month sst rhum slp cldc 10353 26.2 -78.2 8 28.981304 82.394811 1006.801278 4.477195 19374 14.5 -69.6 10 28.904502 82.054564 1010.190954 5.367316 12393 30.7 -86.3 7 28.347659 82.523253 1000.820020 6.652682 3373 40.9 -70.7 9 24.981041 77.434502 1012.284426 4.131097 21173 25.8 -42.5 9 28.643885 78.809962 1013.124650 4.610140 Random Forest Primeira tentativa de ajuste j\u00e1 nos parece promissor em rela\u00e7\u00e3o aos demais. #X_train, y_train_mw = make_regression(n_features=7, n_informative=2, random_state=0, shuffle=False) regr = RandomForestRegressor ( n_estimators = 7 , max_depth = 20 , random_state = 0 ) regr . fit ( X_train , y_train_mw ) print ( regr . score ( X_train , y_train_mw )) print ( regr . score ( X_test , y_test_mw )) 0.8272253680614361 0.4358801020027704 #X_train = data_atl_merged.drop(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Maximum Wind', 'Minimum Pressure', 'Date_c', 'Day', 'Latitude_c', 'Longitude_c', 'Duration', 'Year', 'wspd'], 1) #X_train.head() Multi Layer Perceptron #X_train, y_train_mw = make_regression(n_samples=200, random_state=1) regr = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr . score ( X_train , y_train_mw )) print ( regr . score ( X_test , y_test_mw )) 0 . 09520915346048042 0 . 08725614810614979 / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) Modelos com Separa\u00e7\u00e3o em Conjuntos de Treino e Teste Separamos os dados em conjuntos de treino e de teste. Deste modo, podemos ajustar o algoritmo utilizando os dados de treino, e tentar utilizar esses dados de teste para previs\u00e3o de outros dados, inclusive futuros. X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'Year' , 'wspd' ], 1 ) y_train_mw = data_atl_merged [ 'Maximum Wind' ] #print(len(X_train)) #X_train.head() X_train , X_test , y_train_mw , y_test_mw = train_test_split ( X_train , y_train_mw , random_state = 1 ) Regress\u00e3o Linear X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) X_test2 = sm . add_constant ( X_test ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared r2_test = 1 - (( OLSModel . predict ( X_test2 ) - y_test_mw ) * ( OLSModel . predict ( X_test2 ) - y_test_mw )) . sum () / (( y_test_mw . mean () - y_test_mw ) * ( y_test_mw . mean () - y_test_mw )) . sum () print ( f 'R^2_train = { r2_train } ' ) print ( f 'R^2_test = { r2_test } ' ) ''' print(f'Par\u00e2metro_const = {OLSModel.params[0]}') print(f'Par\u00e2metro_Month = {OLSModel.params[1]}') print(f'Par\u00e2metro_Latitude = {OLSModel.params[2]}') print(f'Par\u00e2metro_Longitude = {OLSModel.params[3]}') print(f'Par\u00e2metro_sst = {OLSModel.params[4]}') print(f'Par\u00e2metro_rhum = {OLSModel.params[5]}') print(f'Par\u00e2metro_slp = {OLSModel.params[6]}') print(f'Par\u00e2metro_cldc = {OLSModel.params[7]}') print(f'Par\u00e2metro_Month^2 = {OLSModel.params[8]}') print(f'Par\u00e2metro_Latitude^2 = {OLSModel.params[9]}') print(f'Par\u00e2metro_Longitude^2 = {OLSModel.params[10]}') print(f'Par\u00e2metro_sst^2 = {OLSModel.params[11]}') print(f'Par\u00e2metro_rhum^2 = {OLSModel.params[12]}') print(f'Par\u00e2metro_slp^2 = {OLSModel.params[13]}') print(f'Par\u00e2metro_cldc^2 = {OLSModel.params[14]}') ''' R ^ 2 _train = 0 . 019806236602926464 R ^ 2 _test = 0 . 01874952522766249 \"\\nprint(f'Par\u00e2metro_const = {OLSModel.params[0]}')\\nprint(f'Par\u00e2metro_Month = {OLSModel.params[1]}')\\nprint(f'Par\u00e2metro_Latitude = {OLSModel.params[2]}')\\nprint(f'Par\u00e2metro_Longitude = {OLSModel.params[3]}')\\nprint(f'Par\u00e2metro_sst = {OLSModel.params[4]}')\\nprint(f'Par\u00e2metro_rhum = {OLSModel.params[5]}')\\nprint(f'Par\u00e2metro_slp = {OLSModel.params[6]}')\\nprint(f'Par\u00e2metro_cldc = {OLSModel.params[7]}')\\n\\nprint(f'Par\u00e2metro_Month^2 = {OLSModel.params[8]}')\\nprint(f'Par\u00e2metro_Latitude^2 = {OLSModel.params[9]}')\\nprint(f'Par\u00e2metro_Longitude^2 = {OLSModel.params[10]}')\\nprint(f'Par\u00e2metro_sst^2 = {OLSModel.params[11]}')\\nprint(f'Par\u00e2metro_rhum^2 = {OLSModel.params[12]}')\\nprint(f'Par\u00e2metro_slp^2 = {OLSModel.params[13]}')\\nprint(f'Par\u00e2metro_cldc^2 = {OLSModel.params[14]}')\\n\" fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], OLSModel . predict ( X_train2 ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], OLSModel . predict ( X_test2 ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Regress\u00e3o Linear (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Regress\u00e3o Linear (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Random Forest Pelos ajustes anteriores, vimos que esse algoritmo promove um bom ajuste nos dados. Um novo ajuste com aplica\u00e7\u00e3o de par\u00e2metros melhor sintonizados com os dados \u00e9 buscado pelo c\u00f3digo abaixo. # Par\u00e2metros com bom ajuste para Random Forest: n_estimators = 50, max_depth = 75 for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) print ( f ' \\n n_estimators= { i } , max_depth= { j } ' ) print ( regr_rf . score ( X_train , y_train_mw )) print ( regr_rf . score ( X_test , y_test_mw )) n_estimators = 25 , max_depth = 25 0 . 9109127406378082 0 . 5049385543175804 n_estimators = 25 , max_depth = 50 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 25 , max_depth = 75 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 25 , max_depth = 100 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 25 , max_depth = 125 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 50 , max_depth = 25 0 . 9177312843005485 0 . 5190516352697632 n_estimators = 50 , max_depth = 50 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 50 , max_depth = 75 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 50 , max_depth = 100 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 50 , max_depth = 125 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 75 , max_depth = 25 0 . 9198026089627763 0 . 5227499387064152 n_estimators = 75 , max_depth = 50 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 75 , max_depth = 75 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 75 , max_depth = 100 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 75 , max_depth = 125 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 100 , max_depth = 25 0 . 9198383784088979 0 . 5224954900261807 n_estimators = 100 , max_depth = 50 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 100 , max_depth = 75 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 100 , max_depth = 100 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 100 , max_depth = 125 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 125 , max_depth = 25 0 . 9204444093451314 0 . 5234545174002654 n_estimators = 125 , max_depth = 50 0 . 9344355008748928 0 . 5280216690661794 n_estimators = 125 , max_depth = 75 0 . 9344355008748928 0 . 5280216690661794 n_estimators = 125 , max_depth = 100 0 . 9344355008748928 0 . 5280216690661794 n_estimators = 125 , max_depth = 125 0 . 9344355008748928 0 . 5280216690661794 O R2 Score obtido abaixo mostra o melhor ajuste do modelo quando tentamos prever a Velocidade M\u00e1xima Sustentada pelo algoritmo do Random Forest. O ajuste aos dados de treino ficam regr_rf = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) print ( regr_rf . score ( X_train , y_train_mw )) print ( regr_rf . score ( X_test , y_test_mw )) 0.9298001112559356 0.5227899508473133 X_train_red = X_train . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) X_test_red = X_test . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) Retirando os dados clim\u00e1ticos, observamos que o ajuste fica bem pior, mostrando a import\u00e2ncia dos mesmos para a predi\u00e7\u00e3o regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.8598571752407663 0.06143697855472363 X_train_red = X_train . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) X_test_red = X_test . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9072010288584739 0.3833348196160016 #['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] X_train_red = X_train . drop ([ 'Month' ], 1 ) X_test_red = X_test . drop ([ 'Month' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9230537875398801 0.4840455607978509 Ajuste da predi\u00e7\u00e3o em rela\u00e7\u00e3o \u00e0 vari\u00e1vel sst (temperatura mensal m\u00e9dia) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_rf . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_rf . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Demais Previs\u00f5es com Random Forest (Melhor Ajuste) Adicionando as vari\u00e1veis Ano e Dia, conseguimos melhorar significativamente a capacidade de previs\u00e3o do nosso modelo. Se adicionarmos primeiramente apenas a vari\u00e1vel Ano, percebemos que cada vari\u00e1vel contribui um pouco para a melhoria da previs\u00e3o. data_train_sd = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' , 'Day' ], 1 ) data_train_mw_sd = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train_sd , data_test_sd , data_train_mw_sd , data_test_mw_sd = train_test_split ( data_train_sd , data_train_mw_sd , random_state = 1 ) regr_rf2_sd = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2_sd . fit ( data_train_sd , data_train_mw_sd ) print ( regr_rf2_sd . score ( data_train_sd , data_train_mw_sd )) print ( regr_rf2_sd . score ( data_test_sd , data_test_mw_sd )) 0.9502630426894276 0.6609429559863542 data_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) data_train_mw = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train , data_test , data_train_mw , data_test_mw = train_test_split ( data_train , data_train_mw , random_state = 1 ) Ajuste fino dos par\u00e2metros do Random Forest for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf2 = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( f ' \\n n_estimators= { i } , max_depth= { j } ' ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) n_estimators = 25 , max_depth = 25 0 . 957264562824329 0 . 7444676687346707 n_estimators = 25 , max_depth = 50 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 75 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 100 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 125 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 50 , max_depth = 25 0 . 9617003376388379 0 . 7583441990969142 n_estimators = 50 , max_depth = 50 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 75 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 100 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 125 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 75 , max_depth = 25 0 . 9637876668338223 0 . 7615184996888411 n_estimators = 75 , max_depth = 50 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 75 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 100 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 125 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 100 , max_depth = 25 0 . 9643667689055675 0 . 7618762745120209 n_estimators = 100 , max_depth = 50 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 75 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 100 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 125 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 125 , max_depth = 25 0 . 9647081307547872 0 . 762938280257875 n_estimators = 125 , max_depth = 50 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 75 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 100 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 125 0 . 9661419630929642 0 . 7643016255435418 Melhor ajuste para Previs\u00e3o de Maximal Wind regr_rf2 = RandomForestRegressor ( n_estimators = 50 , max_depth = 50 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) 0.9629951225598822 0.7593567448937373 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( data_train [ 'sst' ], data_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_train [ 'sst' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], data_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Previs\u00e3o da dura\u00e7\u00e3o dos eventos de Furac\u00e3o data_train2 = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) #data_train_mw = data_atl_merged['Maximum Wind'] data_train_dur = data_atl_merged [ 'Duration' ] #print(len(data_train)) #data_train.head() data_train2 , data_test2 , data_train_dur , data_test_dur = train_test_split ( data_train2 , data_train_dur , random_state = 1 ) Abaixo, faremos tamb\u00e9m a previs\u00e3o da dura\u00e7\u00e3o de um Furac\u00e3o. O ajuste fica bem preciso, como se pode ver pelo R2 Score regr_rf3 = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf3 . fit ( data_train2 , data_train_dur ) print ( regr_rf3 . score ( data_train2 , data_train_dur )) print ( regr_rf3 . score ( data_test2 , data_test_dur )) 0.9883397102866289 0.9289716458290775 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw , data_train_dur ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw , data_test_dur ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf3 . predict ( data_train2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf3 . predict ( data_test2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Multi Layer Perceptron regr_mlp = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp . score ( X_train , y_train_mw )) print ( regr_mlp . score ( X_test , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) 0 . 09520915346048042 0 . 08725614810614979 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_mlp . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_mlp . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) Support Vector Machine regr_svr = svm . SVR () regr_svr . fit ( X_train , y_train_mw ) print ( regr_svr . score ( X_train , y_train_mw )) print ( regr_svr . score ( X_test , y_test_mw )) -0.06514630260669341 -0.058505019763586796 Modelos com Escala Padronizada # Padroniza\u00e7\u00e3o da Escala scaler = StandardScaler () # doctest: +SKIP scaler . fit ( X_train ) # doctest: +SKIP X_train_std = scaler . transform ( X_train ) # doctest: +SKIP X_test_std = scaler . transform ( X_test ) regr_svr_std = svm . SVR () regr_svr_std . fit ( X_train_std , y_train_mw ) print ( regr_svr_std . score ( X_train_std , y_train_mw )) print ( regr_svr_std . score ( X_test_std , y_test_mw )) 0.07772469466765619 0.07604328404370786 regr_mlp_std = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp_std . score ( X_train_std , y_train_mw )) print ( regr_mlp_std . score ( X_test_std , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) - 4 . 16270156850149 - 4 . 153096948726193 regr_rf_std = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_std . fit ( X_train_std , y_train_mw ) print ( regr_rf_std . score ( X_train_std , y_train_mw )) print ( regr_rf_std . score ( X_test_std , y_test_mw )) 0.9298162379440262 0.5220914993160997","title":"Destrutividade no curto prazo"},{"location":"Analises_variaveis/#aplicacao-de-modelos-de-regressao","text":"Neste Notebook, iremos importar os dados j\u00e1 limpados/filtrados em etapas anteriores e iremos aplicar diversos algoritmos visando ajustar uma boa regress\u00e3o que nos ajude a prever as velocidades dos ventos de um determinado evento (Tropical Storm ou Hurricane) ou a dura\u00e7\u00e3o dos mesmos. Algumas pequenas transforma\u00e7\u00f5es ser\u00e3o necess\u00e1rias para ajuste das vari\u00e1veis preditoras a serem consideradas em cada modelo.","title":"Aplica\u00e7\u00e3o de Modelos de Regress\u00e3o"},{"location":"Analises_variaveis/#instalacao-e-importacao-de-pacotes","text":"Os c\u00f3digos de instala\u00e7\u00e3o abaixo est\u00e3o comentados. Na primeira utiliza\u00e7\u00e3o do Notebook, pode-se descoment\u00e1-los para instalar os pacotes que ainda n\u00e3o estejam instalados no computador que se esteja utilizando. Al\u00e9m disso, alguns pacotes abaixo s\u00e3o para utiliza\u00e7\u00e3o nos demais notebooks do projeto. # !apt-get install libgeos-3.5.0 # !apt-get install libgeos-dev # !pip install https://github.com/matplotlib/basemap/archive/master.zip # !pip install pyproj==1.9.6 # !pip install netcdf4 # Dependencies for Tropycal Package: https://pypi.org/project/tropycal/ # matplotlib >= 2.2.2 # numpy >= 1.14.3 # scipy >= 1.1.0 # pandas >= 0.23.0 # geopy >= 1.18.1 # xarray >= 0.10.7 # networkx >= 2.0.0 # requests >= 2.22.0 # To fully leverage tropycal's plotting capabilities, it is strongly recommended to have cartopy >= 0.17.0 installed. # !pip install numpy # !pip install pandas # !pip install matplotlib # !pip install scipy # !pip install geopy # !pip install xarray # !pip install networkx # !pip install requests # !pip install proj # !pip install proj-data # !pip install geos # #!pip uninstall cartopy # !apt-get -qq install python-cartopy python3-cartopy # !pip install cartopy # !pip install tropycal # #!pip freeze % matplotlib inline import numpy as np import matplotlib import matplotlib.pyplot as plt import matplotlib.colors as c #from mpl_toolkits.basemap import Basemap,shiftgrid import pandas as pd import netCDF4 as nc from itertools import chain from netCDF4 import Dataset from netCDF4 import date2index from datetime import datetime import scipy import geopy import xarray import networkx import requests import cartopy import tropycal import tropycal.tracks as tracks import tropycal.tornado as tornado import datetime as dt import statsmodels.api as sm from statsmodels.api import OLS # Modelos para Testes Futuros de Regress\u00e3o from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.preprocessing import PolynomialFeatures from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold import math from scipy.special import gamma import seaborn as sns sns . set () alpha = 0.5 from sklearn import preprocessing from pandas.plotting import scatter_matrix from sklearn.ensemble import RandomForestRegressor from sklearn.neural_network import MLPRegressor from sklearn.model_selection import train_test_split from sklearn import svm #from sklearn.naive_bayes import GaussianNB from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GroupShuffleSplit from sklearn.model_selection import GroupKFold from sklearn.datasets import make_friedman2 from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import DotProduct , WhiteKernel","title":"Instala\u00e7\u00e3o e Importa\u00e7\u00e3o de Pacotes"},{"location":"Analises_variaveis/#carregamento-e-preparacao-dos-dados","text":"#Importa\u00e7\u00e3o dos dados previamente estruturados data_atl_merged = pd . read_csv ( '../Datasets/data_atl_merged2.csv' ) print ( data_atl_merged . columns ) data_atl_merged . head () Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Date_c', 'Year', 'Month', 'Day', 'Latitude_c', 'Longitude_c', 'Duration', 'sst', 'rhum', 'wspd', 'slp', 'cldc'], dtype='object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Month Day Latitude_c Longitude_c Duration sst rhum wspd slp cldc 0 AL011951 UNNAMED 1951-01-02 1200 NaN EX 30.5 -58.0 50 -999 ... 1 2 30.5 -58.0 10 12.371029 72.457094 72.457094 1027.446055 5.219256 1 AL011951 UNNAMED 1951-01-02 1800 NaN EX 29.9 -56.8 45 -999 ... 1 2 29.9 -56.8 10 16.217764 80.423854 80.423854 1022.672978 5.756614 2 AL011951 UNNAMED 1951-01-03 0 NaN EX 29.0 -55.7 45 -999 ... 1 3 29.0 -55.7 10 15.491124 81.121590 81.121590 1023.151909 5.782474 3 AL011951 UNNAMED 1951-01-03 600 NaN EX 27.5 -54.8 45 -999 ... 1 3 27.5 -54.8 10 22.268075 84.264761 84.264761 1019.325138 5.651842 4 AL011951 UNNAMED 1951-01-03 1200 NaN EX 26.5 -54.5 45 -999 ... 1 3 26.5 -54.5 10 22.336746 84.458617 84.458617 1019.305866 5.645406 5 rows \u00d7 22 columns y_train_mw = data_atl_merged [ 'Maximum Wind' ] y_train_mp = data_atl_merged [ 'Minimum Pressure' ] y_train_mp [ y_train_mp < 0 ] = - 1 y_train_dur = data_atl_merged [ 'Duration' ] X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' ], 1 ) #print(y_train_rad) print ( X_train . columns ) X_train . head () Index ([ 'Latitude' , 'Longitude' , 'Year' , 'Month' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ], dtype = 'object' ) < ipython - input - 16 - 44 f50b930158 > : 3 : SettingWithCopyWarning : A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation : https : // pandas . pydata . org / pandas - docs / stable / user_guide / indexing . html # returning - a - view - versus - a - copy y_train_mp [ y_train_mp < 0 ] = - 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Latitude Longitude Year Month sst rhum wspd slp cldc 0 30.5 -58.0 1951 1 12.371029 72.457094 72.457094 1027.446055 5.219256 1 29.9 -56.8 1951 1 16.217764 80.423854 80.423854 1022.672978 5.756614 2 29.0 -55.7 1951 1 15.491124 81.121590 81.121590 1023.151909 5.782474 3 27.5 -54.8 1951 1 22.268075 84.264761 84.264761 1019.325138 5.651842 4 26.5 -54.5 1951 1 22.336746 84.458617 84.458617 1019.305866 5.645406","title":"Carregamento e Prepara\u00e7\u00e3o dos Dados"},{"location":"Analises_variaveis/#modelos","text":"X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) print ( f 'Par\u00e2metro_Month = { OLSModel . params [ 2 ] } ' ) print ( f 'Par\u00e2metro_Latitude = { OLSModel . params [ 3 ] } ' ) print ( f 'Par\u00e2metro_Longitude = { OLSModel . params [ 4 ] } ' ) print ( f 'Par\u00e2metro_sst = { OLSModel . params [ 5 ] } ' ) print ( f 'Par\u00e2metro_rhum = { OLSModel . params [ 6 ] } ' ) print ( f 'Par\u00e2metro_wspd = { OLSModel . params [ 7 ] } ' ) print ( f 'Par\u00e2metro_slp = { OLSModel . params [ 8 ] } ' ) print ( f 'Par\u00e2metro_cldc = { OLSModel . params [ 9 ] } ' ) R^2_train = 0.02385621171100183 Par\u00e2metro_const = 47.35464784180999 Par\u00e2metro_Year = 0.09050506606867095 Par\u00e2metro_Month = -0.05872870746380546 Par\u00e2metro_Latitude = -0.08576970286238517 Par\u00e2metro_Longitude = 1.880119707824508 Par\u00e2metro_sst = 0.15179194438994867 Par\u00e2metro_rhum = 0.028283243122749335 Par\u00e2metro_wspd = 0.028283243122749446 Par\u00e2metro_slp = 0.14971356534654948 Par\u00e2metro_cldc = -1.5161434590996923 Os c\u00f3digos abaixos nos geras uma visualiza\u00e7\u00e3o que pode trazer insights a respeito da rela\u00e7\u00e3o entre as vari\u00e1veis. A escolha das vari\u00e1veis preditoras que servem de entrada para os modelos mais a frente foram pensadas tamb\u00e9m pela observa\u00e7\u00e3o destes gr\u00e1ficos. df = pd . concat ([ X_train , y_train_mw , y_train_mp ], axis = 1 ) scatter_matrix ( df , alpha = 0.8 , figsize = ( 15 , 15 ), diagonal = 'kde' ); # A princ\u00edpio, n\u00e3o queremos que se fa\u00e7a alguma previs\u00e3o com base no valor num\u00e9rico do ano # Al\u00e9m disso, a vari\u00e1vel wspd est\u00e1 altamente correlacionada com a rhum, podendo ser mantida apenas a \u00faltima X_train = data_atl_merged . drop ([ 'Year' , 'wspd' ], 1 ) # M\u00eas Latitude Longitude Temperatura, Umidade, Sea Level Pressure, Cloudiness] # ['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] fig , ax = plt . subplots ( 1 , 7 ) #, figsize=(16,10)) fig . suptitle ( 'Velocidade M\u00e1xima vs Vari\u00e1veis Preditoras (1950-2015)' , fontsize = 28 , y = 1.06 ) ax [ 0 ] . scatter ( X_train [ 'Month' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 1 ] . scatter ( X_train [ 'Latitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 2 ] . scatter ( X_train [ 'Longitude' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 3 ] . scatter ( X_train [ 'sst' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 4 ] . scatter ( X_train [ 'rhum' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 5 ] . scatter ( X_train [ 'slp' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax [ 6 ] . scatter ( X_train [ 'cldc' ], X_train [ 'Maximum Wind' ], alpha = 0.5 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') #ax.plot([1850, 2015], [w0, w1], ls = '-.') #, label=r'$Furac\u00f5es$ $=$ $0$') ''' ax.tick_params(labelsize=24) ax.set_title(f'N\u00famero de Furac\u00f5es Anuais (1851-2015)', fontsize=24) ax.set_xlabel(r'$Ano$', fontsize=16) ax.set_ylabel(r'$Quantidade$', fontsize=16) ''' #fig.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'Year' , 'wspd' ], 1 ) X_train . mean () Latitude 27.466819 Longitude -62.607058 Month 8.709729 sst 26.416134 rhum 81.284651 slp 1009.678574 cldc 5.137222 dtype: float64 Uma primeira tentativa de ajuste foi feito atrav\u00e9s da centraliza\u00e7\u00e3o das vari\u00e1veis preditoras em rela\u00e7\u00e3o \u00e0 m\u00e9dia, adicionando tamb\u00e9m termos polinomiais de segunda ordem. No entanto, os resultados do ajuste n\u00e3o mostraram ganhos significativos para o modelo de Regress\u00e3o Linear M\u00faltipla, e at\u00e9 prejudicaram modelos mais complexos, como Random Forest, Multi Layer Perceptron, entre outros utilizados mais a frente. # Tentativa de ajuste atrav\u00e9s de transforma\u00e7\u00e3o de vari\u00e1veis e inclus\u00e3o de termos polinomiais X_train [ 'Month_mod' ] = ( X_train [ 'Month' ] - 8.788781 ) X_train [ 'Latitude_mod' ] = ( X_train [ 'Latitude' ] - 17.943038 ) X_train [ 'Longitude_mod' ] = ( X_train [ 'Longitude' ] + 62.555538 ) X_train [ 'sst_mod' ] = ( X_train [ 'sst' ] - 28.888685 ) X_train [ 'rhum_mod' ] = ( X_train [ 'rhum' ] - 82.051702 ) X_train [ 'slp_mod' ] = ( X_train [ 'slp' ] - 1008.611661 ) X_train [ 'cldc_mod' ] = ( X_train [ 'cldc' ] - 5.223518 ) X_train [ 'Month_mod^2' ] = X_train [ 'Month_mod' ] ** 2 X_train [ 'Latitude_mod^2' ] = X_train [ 'Latitude_mod' ] ** 2 X_train [ 'Longitude_mod^2' ] = X_train [ 'Longitude_mod' ] ** 2 X_train [ 'sst_mod^2' ] = X_train [ 'sst_mod' ] ** 2 X_train [ 'rhum_mod^2' ] = X_train [ 'rhum_mod' ] ** 2 X_train [ 'slp_mod^2' ] = X_train [ 'slp_mod' ] ** 2 X_train [ 'cldc_mod^2' ] = X_train [ 'cldc_mod' ] ** 2 X_train = X_train . drop ([ 'Month' , 'Latitude' , 'Longitude' , 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) X_train . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Month_mod Latitude_mod Longitude_mod sst_mod rhum_mod slp_mod cldc_mod Month_mod^2 Latitude_mod^2 Longitude_mod^2 sst_mod^2 rhum_mod^2 slp_mod^2 cldc_mod^2 0 -7.788781 12.556962 4.555538 -16.517656 -9.594608 18.834394 -0.004262 60.665109 157.677295 20.752926 272.832952 92.056509 354.734396 0.000018 1 -7.788781 11.956962 5.755538 -12.670921 -1.627848 14.061317 0.533096 60.665109 142.968940 33.126218 160.552231 2.649888 197.720622 0.284192 2 -7.788781 11.056962 6.855538 -13.397561 -0.930112 14.540248 0.558956 60.665109 122.256409 46.998401 179.494647 0.865108 211.418806 0.312431 3 -7.788781 9.556962 7.755538 -6.620610 2.213059 10.713477 0.428324 60.665109 91.335523 60.148370 43.832480 4.897629 114.778588 0.183461 4 -7.788781 8.556962 8.055538 -6.551939 2.406915 10.694205 0.421888 60.665109 73.221599 64.891692 42.927902 5.793238 114.366028 0.177989","title":"Modelos"},{"location":"Analises_variaveis/#regressao-linear","text":"X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Month = { OLSModel . params [ 1 ] } ' ) print ( f 'Par\u00e2metro_Latitude = { OLSModel . params [ 2 ] } ' ) print ( f 'Par\u00e2metro_Longitude = { OLSModel . params [ 3 ] } ' ) print ( f 'Par\u00e2metro_sst = { OLSModel . params [ 4 ] } ' ) print ( f 'Par\u00e2metro_rhum = { OLSModel . params [ 5 ] } ' ) print ( f 'Par\u00e2metro_slp = { OLSModel . params [ 6 ] } ' ) print ( f 'Par\u00e2metro_cldc = { OLSModel . params [ 7 ] } ' ) print ( f 'Par\u00e2metro_Month^2 = { OLSModel . params [ 8 ] } ' ) print ( f 'Par\u00e2metro_Latitude^2 = { OLSModel . params [ 9 ] } ' ) print ( f 'Par\u00e2metro_Longitude^2 = { OLSModel . params [ 10 ] } ' ) print ( f 'Par\u00e2metro_sst^2 = { OLSModel . params [ 11 ] } ' ) print ( f 'Par\u00e2metro_rhum^2 = { OLSModel . params [ 12 ] } ' ) print ( f 'Par\u00e2metro_slp^2 = { OLSModel . params [ 13 ] } ' ) print ( f 'Par\u00e2metro_cldc^2 = { OLSModel . params [ 14 ] } ' ) R^2_train = 0.057796565423244184 Par\u00e2metro_const = 55.44314399378881 Par\u00e2metro_Month = 0.8453623447457261 Par\u00e2metro_Latitude = 0.3457362098822069 Par\u00e2metro_Longitude = -0.02891517619557605 Par\u00e2metro_sst = 0.02964116885731194 Par\u00e2metro_rhum = 0.09964668732077198 Par\u00e2metro_slp = 0.289107953297979 Par\u00e2metro_cldc = -0.5559643046414668 Par\u00e2metro_Month^2 = -0.48473747846473236 Par\u00e2metro_Latitude^2 = -0.017728720785750357 Par\u00e2metro_Longitude^2 = -0.00855022784566639 Par\u00e2metro_sst^2 = 0.051566532947100174 Par\u00e2metro_rhum^2 = 0.002209669018392528 Par\u00e2metro_slp^2 = -0.03379242085061111 Par\u00e2metro_cldc^2 = -0.16931736469014314 X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'Year' , 'wspd' ], 1 ) y_train_mw = data_atl_merged [ 'Maximum Wind' ] X_train , X_test , y_train_mw , y_test_mw = train_test_split ( X_train , y_train_mw , random_state = 1 ) print ( len ( X_train )) X_train . head () 16789 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Latitude Longitude Month sst rhum slp cldc 10353 26.2 -78.2 8 28.981304 82.394811 1006.801278 4.477195 19374 14.5 -69.6 10 28.904502 82.054564 1010.190954 5.367316 12393 30.7 -86.3 7 28.347659 82.523253 1000.820020 6.652682 3373 40.9 -70.7 9 24.981041 77.434502 1012.284426 4.131097 21173 25.8 -42.5 9 28.643885 78.809962 1013.124650 4.610140","title":"Regress\u00e3o Linear"},{"location":"Analises_variaveis/#random-forest","text":"Primeira tentativa de ajuste j\u00e1 nos parece promissor em rela\u00e7\u00e3o aos demais. #X_train, y_train_mw = make_regression(n_features=7, n_informative=2, random_state=0, shuffle=False) regr = RandomForestRegressor ( n_estimators = 7 , max_depth = 20 , random_state = 0 ) regr . fit ( X_train , y_train_mw ) print ( regr . score ( X_train , y_train_mw )) print ( regr . score ( X_test , y_test_mw )) 0.8272253680614361 0.4358801020027704 #X_train = data_atl_merged.drop(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Maximum Wind', 'Minimum Pressure', 'Date_c', 'Day', 'Latitude_c', 'Longitude_c', 'Duration', 'Year', 'wspd'], 1) #X_train.head()","title":"Random Forest"},{"location":"Analises_variaveis/#multi-layer-perceptron","text":"#X_train, y_train_mw = make_regression(n_samples=200, random_state=1) regr = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr . score ( X_train , y_train_mw )) print ( regr . score ( X_test , y_test_mw )) 0 . 09520915346048042 0 . 08725614810614979 / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter )","title":"Multi Layer Perceptron"},{"location":"Analises_variaveis/#modelos-com-separacao-em-conjuntos-de-treino-e-teste","text":"Separamos os dados em conjuntos de treino e de teste. Deste modo, podemos ajustar o algoritmo utilizando os dados de treino, e tentar utilizar esses dados de teste para previs\u00e3o de outros dados, inclusive futuros. X_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'Year' , 'wspd' ], 1 ) y_train_mw = data_atl_merged [ 'Maximum Wind' ] #print(len(X_train)) #X_train.head() X_train , X_test , y_train_mw , y_test_mw = train_test_split ( X_train , y_train_mw , random_state = 1 )","title":"Modelos com Separa\u00e7\u00e3o em Conjuntos de Treino e Teste"},{"location":"Analises_variaveis/#regressao-linear_1","text":"X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) X_test2 = sm . add_constant ( X_test ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train_mw , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared r2_test = 1 - (( OLSModel . predict ( X_test2 ) - y_test_mw ) * ( OLSModel . predict ( X_test2 ) - y_test_mw )) . sum () / (( y_test_mw . mean () - y_test_mw ) * ( y_test_mw . mean () - y_test_mw )) . sum () print ( f 'R^2_train = { r2_train } ' ) print ( f 'R^2_test = { r2_test } ' ) ''' print(f'Par\u00e2metro_const = {OLSModel.params[0]}') print(f'Par\u00e2metro_Month = {OLSModel.params[1]}') print(f'Par\u00e2metro_Latitude = {OLSModel.params[2]}') print(f'Par\u00e2metro_Longitude = {OLSModel.params[3]}') print(f'Par\u00e2metro_sst = {OLSModel.params[4]}') print(f'Par\u00e2metro_rhum = {OLSModel.params[5]}') print(f'Par\u00e2metro_slp = {OLSModel.params[6]}') print(f'Par\u00e2metro_cldc = {OLSModel.params[7]}') print(f'Par\u00e2metro_Month^2 = {OLSModel.params[8]}') print(f'Par\u00e2metro_Latitude^2 = {OLSModel.params[9]}') print(f'Par\u00e2metro_Longitude^2 = {OLSModel.params[10]}') print(f'Par\u00e2metro_sst^2 = {OLSModel.params[11]}') print(f'Par\u00e2metro_rhum^2 = {OLSModel.params[12]}') print(f'Par\u00e2metro_slp^2 = {OLSModel.params[13]}') print(f'Par\u00e2metro_cldc^2 = {OLSModel.params[14]}') ''' R ^ 2 _train = 0 . 019806236602926464 R ^ 2 _test = 0 . 01874952522766249 \"\\nprint(f'Par\u00e2metro_const = {OLSModel.params[0]}')\\nprint(f'Par\u00e2metro_Month = {OLSModel.params[1]}')\\nprint(f'Par\u00e2metro_Latitude = {OLSModel.params[2]}')\\nprint(f'Par\u00e2metro_Longitude = {OLSModel.params[3]}')\\nprint(f'Par\u00e2metro_sst = {OLSModel.params[4]}')\\nprint(f'Par\u00e2metro_rhum = {OLSModel.params[5]}')\\nprint(f'Par\u00e2metro_slp = {OLSModel.params[6]}')\\nprint(f'Par\u00e2metro_cldc = {OLSModel.params[7]}')\\n\\nprint(f'Par\u00e2metro_Month^2 = {OLSModel.params[8]}')\\nprint(f'Par\u00e2metro_Latitude^2 = {OLSModel.params[9]}')\\nprint(f'Par\u00e2metro_Longitude^2 = {OLSModel.params[10]}')\\nprint(f'Par\u00e2metro_sst^2 = {OLSModel.params[11]}')\\nprint(f'Par\u00e2metro_rhum^2 = {OLSModel.params[12]}')\\nprint(f'Par\u00e2metro_slp^2 = {OLSModel.params[13]}')\\nprint(f'Par\u00e2metro_cldc^2 = {OLSModel.params[14]}')\\n\" fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], OLSModel . predict ( X_train2 ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], OLSModel . predict ( X_test2 ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Regress\u00e3o Linear (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Regress\u00e3o Linear (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Regress\u00e3o Linear"},{"location":"Analises_variaveis/#random-forest_1","text":"Pelos ajustes anteriores, vimos que esse algoritmo promove um bom ajuste nos dados. Um novo ajuste com aplica\u00e7\u00e3o de par\u00e2metros melhor sintonizados com os dados \u00e9 buscado pelo c\u00f3digo abaixo. # Par\u00e2metros com bom ajuste para Random Forest: n_estimators = 50, max_depth = 75 for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) print ( f ' \\n n_estimators= { i } , max_depth= { j } ' ) print ( regr_rf . score ( X_train , y_train_mw )) print ( regr_rf . score ( X_test , y_test_mw )) n_estimators = 25 , max_depth = 25 0 . 9109127406378082 0 . 5049385543175804 n_estimators = 25 , max_depth = 50 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 25 , max_depth = 75 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 25 , max_depth = 100 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 25 , max_depth = 125 0 . 9231191839999706 0 . 5083859136052509 n_estimators = 50 , max_depth = 25 0 . 9177312843005485 0 . 5190516352697632 n_estimators = 50 , max_depth = 50 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 50 , max_depth = 75 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 50 , max_depth = 100 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 50 , max_depth = 125 0 . 9298001112559356 0 . 5227899508473133 n_estimators = 75 , max_depth = 25 0 . 9198026089627763 0 . 5227499387064152 n_estimators = 75 , max_depth = 50 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 75 , max_depth = 75 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 75 , max_depth = 100 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 75 , max_depth = 125 0 . 9323017737677339 0 . 5256624725114608 n_estimators = 100 , max_depth = 25 0 . 9198383784088979 0 . 5224954900261807 n_estimators = 100 , max_depth = 50 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 100 , max_depth = 75 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 100 , max_depth = 100 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 100 , max_depth = 125 0 . 9336736647334395 0 . 5269361337556209 n_estimators = 125 , max_depth = 25 0 . 9204444093451314 0 . 5234545174002654 n_estimators = 125 , max_depth = 50 0 . 9344355008748928 0 . 5280216690661794 n_estimators = 125 , max_depth = 75 0 . 9344355008748928 0 . 5280216690661794 n_estimators = 125 , max_depth = 100 0 . 9344355008748928 0 . 5280216690661794 n_estimators = 125 , max_depth = 125 0 . 9344355008748928 0 . 5280216690661794 O R2 Score obtido abaixo mostra o melhor ajuste do modelo quando tentamos prever a Velocidade M\u00e1xima Sustentada pelo algoritmo do Random Forest. O ajuste aos dados de treino ficam regr_rf = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf . fit ( X_train , y_train_mw ) print ( regr_rf . score ( X_train , y_train_mw )) print ( regr_rf . score ( X_test , y_test_mw )) 0.9298001112559356 0.5227899508473133 X_train_red = X_train . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) X_test_red = X_test . drop ([ 'sst' , 'rhum' , 'slp' , 'cldc' ], 1 ) Retirando os dados clim\u00e1ticos, observamos que o ajuste fica bem pior, mostrando a import\u00e2ncia dos mesmos para a predi\u00e7\u00e3o regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.8598571752407663 0.06143697855472363 X_train_red = X_train . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) X_test_red = X_test . drop ([ 'Month' , 'Latitude' , 'Longitude' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9072010288584739 0.3833348196160016 #['Month', 'Latitude', 'Longitude', 'sst', 'rhum', 'slp', 'cldc'] X_train_red = X_train . drop ([ 'Month' ], 1 ) X_test_red = X_test . drop ([ 'Month' ], 1 ) regr_rf_red = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_red . fit ( X_train_red , y_train_mw ) print ( regr_rf_red . score ( X_train_red , y_train_mw )) print ( regr_rf_red . score ( X_test_red , y_test_mw )) 0.9230537875398801 0.4840455607978509 Ajuste da predi\u00e7\u00e3o em rela\u00e7\u00e3o \u00e0 vari\u00e1vel sst (temperatura mensal m\u00e9dia) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_rf . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_rf . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Random Forest"},{"location":"Analises_variaveis/#demais-previsoes-com-random-forest-melhor-ajuste","text":"Adicionando as vari\u00e1veis Ano e Dia, conseguimos melhorar significativamente a capacidade de previs\u00e3o do nosso modelo. Se adicionarmos primeiramente apenas a vari\u00e1vel Ano, percebemos que cada vari\u00e1vel contribui um pouco para a melhoria da previs\u00e3o. data_train_sd = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' , 'Day' ], 1 ) data_train_mw_sd = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train_sd , data_test_sd , data_train_mw_sd , data_test_mw_sd = train_test_split ( data_train_sd , data_train_mw_sd , random_state = 1 ) regr_rf2_sd = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2_sd . fit ( data_train_sd , data_train_mw_sd ) print ( regr_rf2_sd . score ( data_train_sd , data_train_mw_sd )) print ( regr_rf2_sd . score ( data_test_sd , data_test_mw_sd )) 0.9502630426894276 0.6609429559863542 data_train = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) data_train_mw = data_atl_merged [ 'Maximum Wind' ] #print(len(data_train)) #data_train.head() data_train , data_test , data_train_mw , data_test_mw = train_test_split ( data_train , data_train_mw , random_state = 1 ) Ajuste fino dos par\u00e2metros do Random Forest for i in [ 25 , 50 , 75 , 100 , 125 ]: for j in [ 25 , 50 , 75 , 100 , 125 ]: regr_rf2 = RandomForestRegressor ( n_estimators = i , max_depth = j , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( f ' \\n n_estimators= { i } , max_depth= { j } ' ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) n_estimators = 25 , max_depth = 25 0 . 957264562824329 0 . 7444676687346707 n_estimators = 25 , max_depth = 50 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 75 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 100 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 25 , max_depth = 125 0 . 9582917968925033 0 . 7442935073801389 n_estimators = 50 , max_depth = 25 0 . 9617003376388379 0 . 7583441990969142 n_estimators = 50 , max_depth = 50 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 75 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 100 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 50 , max_depth = 125 0 . 9629951225598822 0 . 7593567448937373 n_estimators = 75 , max_depth = 25 0 . 9637876668338223 0 . 7615184996888411 n_estimators = 75 , max_depth = 50 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 75 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 100 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 75 , max_depth = 125 0 . 9651229272756359 0 . 7623946732426374 n_estimators = 100 , max_depth = 25 0 . 9643667689055675 0 . 7618762745120209 n_estimators = 100 , max_depth = 50 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 75 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 100 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 100 , max_depth = 125 0 . 9657927394363737 0 . 7630103360785616 n_estimators = 125 , max_depth = 25 0 . 9647081307547872 0 . 762938280257875 n_estimators = 125 , max_depth = 50 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 75 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 100 0 . 9661419630929642 0 . 7643016255435418 n_estimators = 125 , max_depth = 125 0 . 9661419630929642 0 . 7643016255435418 Melhor ajuste para Previs\u00e3o de Maximal Wind regr_rf2 = RandomForestRegressor ( n_estimators = 50 , max_depth = 50 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf2 . fit ( data_train , data_train_mw ) print ( regr_rf2 . score ( data_train , data_train_mw )) print ( regr_rf2 . score ( data_test , data_test_mw )) 0.9629951225598822 0.7593567448937373 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( data_train [ 'sst' ], data_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_train [ 'sst' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], data_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_test [ 'sst' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf2 . predict ( data_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Maximum Wind' ], alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf2 . predict ( data_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Demais Previs\u00f5es com Random Forest (Melhor Ajuste)"},{"location":"Analises_variaveis/#previsao-da-duracao-dos-eventos-de-furacao","text":"data_train2 = data_atl_merged . drop ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'wspd' ], 1 ) #data_train_mw = data_atl_merged['Maximum Wind'] data_train_dur = data_atl_merged [ 'Duration' ] #print(len(data_train)) #data_train.head() data_train2 , data_test2 , data_train_dur , data_test_dur = train_test_split ( data_train2 , data_train_dur , random_state = 1 ) Abaixo, faremos tamb\u00e9m a previs\u00e3o da dura\u00e7\u00e3o de um Furac\u00e3o. O ajuste fica bem preciso, como se pode ver pelo R2 Score regr_rf3 = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf3 . fit ( data_train2 , data_train_dur ) print ( regr_rf3 . score ( data_train2 , data_train_dur )) print ( regr_rf3 . score ( data_test2 , data_test_dur )) 0.9883397102866289 0.9289716458290775 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) data_concat_train = pd . concat ([ data_train , data_train_mw , data_train_dur ], axis = 1 ) data_concat_test = pd . concat ([ data_test , data_test_mw , data_test_dur ], axis = 1 ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], data_concat_train [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( data_concat_train [ 'Year' ], regr_rf3 . predict ( data_train2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], data_concat_test [ 'Duration' ], alpha = 0.05 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( data_concat_test [ 'Year' ], regr_rf3 . predict ( data_test2 ), alpha = 0.05 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o da Dura\u00e7\u00e3o por Random Forest (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Dura\u00e7\u00e3o$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Previs\u00e3o da dura\u00e7\u00e3o dos eventos de Furac\u00e3o"},{"location":"Analises_variaveis/#multi-layer-perceptron_1","text":"regr_mlp = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp . score ( X_train , y_train_mw )) print ( regr_mlp . score ( X_test , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) 0 . 09520915346048042 0 . 08725614810614979 fig , ax = plt . subplots ( 1 , 2 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax [ 0 ] . scatter ( X_train [ 'sst' ], y_train_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Treino$' ) ax [ 0 ] . scatter ( X_train [ 'sst' ], regr_mlp . predict ( X_train ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], y_test_mw , alpha = 0.5 , label = r '$Dados$ $de$ $Teste$' ) ax [ 1 ] . scatter ( X_test [ 'sst' ], regr_mlp . predict ( X_test ), alpha = 0.5 , label = r '$Previs\u00e3o$' ) ax [ 0 ] . tick_params ( labelsize = 24 ) ax [ 0 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Treino)' , fontsize = 24 ) ax [ 0 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 0 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 0 ] . legend ( loc = 'best' , fontsize = 12 ); ax [ 1 ] . tick_params ( labelsize = 24 ) ax [ 1 ] . set_title ( f 'Previs\u00e3o de Maximal Wind por MLPRegressor (Teste)' , fontsize = 24 ) ax [ 1 ] . set_xlabel ( r '$sst$' , fontsize = 16 ) ax [ 1 ] . set_ylabel ( r '$Maximal$ $Wind$' , fontsize = 16 ) ax [ 1 ] . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 )","title":"Multi Layer Perceptron"},{"location":"Analises_variaveis/#support-vector-machine","text":"regr_svr = svm . SVR () regr_svr . fit ( X_train , y_train_mw ) print ( regr_svr . score ( X_train , y_train_mw )) print ( regr_svr . score ( X_test , y_test_mw )) -0.06514630260669341 -0.058505019763586796","title":"Support Vector Machine"},{"location":"Analises_variaveis/#modelos-com-escala-padronizada","text":"# Padroniza\u00e7\u00e3o da Escala scaler = StandardScaler () # doctest: +SKIP scaler . fit ( X_train ) # doctest: +SKIP X_train_std = scaler . transform ( X_train ) # doctest: +SKIP X_test_std = scaler . transform ( X_test ) regr_svr_std = svm . SVR () regr_svr_std . fit ( X_train_std , y_train_mw ) print ( regr_svr_std . score ( X_train_std , y_train_mw )) print ( regr_svr_std . score ( X_test_std , y_test_mw )) 0.07772469466765619 0.07604328404370786 regr_mlp_std = MLPRegressor ( hidden_layer_sizes = ( 100 , 2 ), random_state = 1 , max_iter = 1000 , solver = 'lbfgs' , activation = 'relu' ) . fit ( X_train , y_train_mw ) #regr.predict(X_test[:2]) print ( regr_mlp_std . score ( X_train_std , y_train_mw )) print ( regr_mlp_std . score ( X_test_std , y_test_mw )) / home / gambitura / anaconda3 / lib / python3 . 8 / site - packages / sklearn / neural_network / _multilayer_perceptron . py : 471 : ConvergenceWarning : lbfgs failed to converge ( status = 1 ): STOP : TOTAL NO . of ITERATIONS REACHED LIMIT . Increase the number of iterations ( max_iter ) or scale the data as shown in : https : // scikit - learn . org / stable / modules / preprocessing . html self . n_iter_ = _check_optimize_result ( \"lbfgs\" , opt_res , self . max_iter ) - 4 . 16270156850149 - 4 . 153096948726193 regr_rf_std = RandomForestRegressor ( n_estimators = 50 , max_depth = 75 , random_state = 0 , oob_score = True , bootstrap = True ) regr_rf_std . fit ( X_train_std , y_train_mw ) print ( regr_rf_std . score ( X_train_std , y_train_mw )) print ( regr_rf_std . score ( X_test_std , y_test_mw )) 0.9298162379440262 0.5220914993160997","title":"Modelos com Escala Padronizada"},{"location":"Data_Cleaning_and_EDA/","text":"Explora\u00e7\u00e3o, Limpeza dos dados e Visualiza\u00e7\u00f5es Trabalharemos com tr\u00eas datasets. O primeiro \u00e9 um subconjunto do ICOADS (International Comprehensive Ocean-Atmosphere Data Set) fornecido pela NOAA (National Oceanic and Atmospheric Administration) que possui v\u00e1rios dados clim\u00e1ticos de cobertura mensal, desde 1800 at\u00e9 o presente, e com abrang\u00eancia mar\u00edtima; Possui dados como temperatura do mar, umidade, press\u00e3o, cobertura de nuvens, velocidade de vento, etc. Na c\u00e9lula abaixo, h\u00e1 um c\u00f3digo que baixa o subconjunto que usaremos diretamente da fonte oficial. N\u00e3o colocamos estes arquivos no GitHub por conta do limite de tamanho imposto sobre os uploads/commits. # !sh ../download_data.sh O segundo grande conjunto de dados \u00e9 o HURDAT2 cuja fonte oficial \u00e9 a NHC (National Hurricane Center), divis\u00e3o da NOAA respons\u00e1vel pelos fura\u00e7\u00f5es e tempestades. Os dados do Kaggle fornecem dados de tempestades e furac\u00f5es desde o s\u00e9culo XIX at\u00e9 2015 no pac\u00edfico e atl\u00e2ntico, mas iremos focar nossa an\u00e1lise no dadaset do atl\u00e2ntico. O terceiro, \u00e9 um dado com o \u00edndice PDI, mas ele j\u00e1 vem da fonte em boas condi\u00e7\u00f5es e n\u00e3o necessita de limpeza. Veja mais no notebook PowerDissipationIndex.ipynb . ## Imports % matplotlib inline import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as c from mpl_toolkits.basemap import Basemap , shiftgrid import pandas as pd import netCDF4 as nc from math import sin , cos , sqrt , atan2 , radians import scipy import geopy import xarray import networkx import requests import cartopy import tropycal import tropycal.tracks as tracks import tropycal.tornado as tornado import datetime as dt import statsmodels.api as sm from statsmodels.api import OLS Dataset ICOADS Vamos analisar os dados baixados acima. O formato .nc \u00e9 leg\u00edvel ao python atrav\u00e9s da biblioteca netCDF4 que est\u00e1 nos pr\u00e9-requisitos. Al\u00e9m disso, ser\u00e3o necess\u00e1rias instalar os pacotes abaixo; Descomente a c\u00e9lula, execute-a e d\u00ea um \"Restart Runtime\" no Jupyter. Certifique-se que instalou todos os pr\u00e9-requisitos do arquivo \"requirements.txt\". # !apt-get install libgeos-3.5.0 # !apt-get install libgeos-dev # !pip install https://github.com/matplotlib/basemap/archive/master.zip An\u00e1lise de dados faltantes sst_mean = nc . Dataset ( '../Datasets/sst.mean.nc' , 'r' ) rhum_mean = nc . Dataset ( '../Datasets/rhum.mean.nc' , 'r' ) wspd_mean = nc . Dataset ( '../Datasets/wspd.mean.nc' , 'r' ) slp_mean = nc . Dataset ( '../Datasets/slp.mean.nc' , 'r' ) vwnd_mean = nc . Dataset ( '../Datasets/vwnd.mean.nc' , 'r' ) cldc_mean = nc . Dataset ( \"../Datasets/cldc.mean.nc\" , 'r' ) lats = sst_mean . variables [ 'lat' ][:] lons = sst_mean . variables [ 'lon' ][:] time = sst_mean . variables [ 'time' ][:] sst = sst_mean . variables [ 'sst' ][:,:,:] rhum = rhum_mean . variables [ 'rhum' ][:,:,:] wspd = wspd_mean . variables [ 'wspd' ][:,:,:] slp = slp_mean . variables [ 'slp' ][:,:,:] vwnd = vwnd_mean . variables [ 'vwnd' ][:,:,:] cldc = cldc_mean . variables [ 'cldc' ][:,:,:] sst_mean . close () wspd_mean . close () rhum_mean . close () slp_mean . close () vwnd_mean . close () cldc_mean . close () period = pd . date_range ( start = \"1800-01-01\" , end = \"2020-07-01\" , freq = \"MS\" ) . to_pydatetime () . tolist () def get_missing ( data : list , labels : list ) -> dict : missing = {} lenght = data [ 0 ] . shape [ 0 ] for j , item in enumerate ( data ): missing [ labels [ j ]] = [] for i in range ( lenght ): missing [ labels [ j ]] . append ( 100 * np . sum ( item [ i ] . mask ) / item [ i ] . data . size ) return missing missing = get_missing ([ sst , wspd , rhum , slp , vwnd , cldc ],[ 'sst' , 'wspd' , 'rhum' , 'slp' , 'vwnd' , 'cldc' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Dados Faltantes - Global\" , fontsize = 15 ) ax . set_xlabel ( \"Ano\" , fontsize = 12 ) ax . set_ylabel ( \" % d e dados faltantes\" , fontsize = 12 ) for key , value in missing . items (): ax . plot ( period , missing [ key ], label = key ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( '../figs/missing-global.png' ) plt . show () print ( type ( sst )) <class 'numpy.ma.core.MaskedArray'> Legenda: sst: Sea Surface Temperature (Temperatura na superf\u00edcie do mar) wspd: Scalar Wind Speed (Velocidade de vento escalar) rhum: Relative Humidity (Umidade relativa) slp: Sea Level Pressure (Press\u00e3o no n\u00edvel do mar) vwnd: V-wind component (Componente V-wind) cldc: Cloudiness (Nebulosidade das nuvens) Como os continentes representam aproximadamente 29,1\\% da suferf\u00edcie terrestre e nossos dados s\u00f3 preenchem os oceanos, os continentes s\u00e3o preenchidos como dados inexistentes. Ent\u00e3o naturalmente nossa cota inferior de dados faltantes \u00e9 essa porcentagem. Note que os dados lidos vem no formato de \"numpy masked array\" que tem um atributo \"mask\" que \u00e9 indicadora de dado faltante, isso nos ajudar\u00e1 a lidar com esses dados. Como vemos no plot acima temos v\u00e1rias d\u00e9cadas com n\u00edveis de dados faltantes acima de 90\\% mas vamos analisar focadamente e atl\u00e2ntico norte, que \u00e9 nossa regi\u00e3o de estudos. sst_at = sst [:, 34 : 40 , 51 : 82 ] #10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W wspd_at = wspd [:, 34 : 40 , 51 : 82 ] rhum_at = rhum [:, 34 : 40 , 51 : 82 ] slp_at = slp [:, 34 : 40 , 51 : 82 ] vwnd_at = vwnd [:, 34 : 40 , 51 : 82 ] cldc_at = cldc [:, 34 : 40 , 51 : 82 ] # sst_pac = sst[:,14:45,0:41] #0\u00b0-60\u00b0N, 100\u00b0W-180\u00b0W # wspd_pac = wspd[:,14:45,0:41] # rhum_pac = rhum[:,14:45,0:41] missing_at = get_missing ([ sst_at , wspd_at , rhum_at , slp_at , vwnd_at , cldc_at ],[ 'sst_at' , 'wspd_at' , 'rhum_at' , 'slp_at' , 'vwnd_at' , 'cldc_at' ]) # missing_pac = get_missing([sst_pac,wspd_pac,rhum_pac],['sst_pac','wspd_pac','rhum_pac']) # fig,(ax,ax1) = plt.subplots(2,1,figsize=(10,8)) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Dados Faltantes - Atl\u00e2ntico Norte MDR*\" , fontsize = 15 ) ax . set_xlabel ( \"Ano\" , fontsize = 12 ) ax . set_ylabel ( \" % d e dados faltantes\" , fontsize = 12 ) for key , value in missing_at . items (): ax . plot ( period , missing_at [ key ], label = key [ 0 : - 3 ]) plt . axvline ( x = period [ 1860 ], label = \"Jan/1955\" , color = 'black' , lw = 3 , ls = '--' ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( '../figs/missing_mdr.png' ) *MDR \u00e9 a abrevia\u00e7\u00e3o de Main Development Region ou regi\u00e3o central de desenvolvimento dos furac\u00f5es no Atl\u00e2ntico Norte e se refere \u00e0 faixa 10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W. Vemos que a partir de 1950-1960, os dados come\u00e7am a ficar mais completos na regi\u00e3o de estudos. Ent\u00e3o, para entender a rela\u00e7\u00e3o as vari\u00e1veis, iremos trabalhar a partir desta data. Entretanto, nada nos impede de usar os dados mais antigos, j\u00e1 que as medi\u00e7\u00f5es n\u00e3o variam muito quando est\u00e3o perto, se quisermos trabalhar com tend\u00eancias de longo prazo podemos cortar os dados a partir de 1920, trabalhar com a m\u00e9dia das regi\u00f5es estudadas, mesmo que com ~70\\% de dados faltantes. Isso pois temos a array indicadora, que pode ajudar em modelos, e tamb\u00e9m essa porcentagem \u00e9 um pouco mais baixa devido \u00e0s faixas continentais considaradas no corte de coordenadas. Abaixo temos um exemplo de como os dados de temperatura est\u00e3o distribu\u00eddos em Janeiro de 1955 Visualiza\u00e7\u00e3o #Transforms longitude ranges from [0,360] para [-180,180] --> useful for plot sst [:], lonsn = shiftgrid ( 180 , sst [:], lons , start = False ) wspd [:], lonsn = shiftgrid ( 180 , wspd [:], lons , start = False ) # shum[:],lonsn = shiftgrid(180,shum[:],lons,start=False) rhum [:], lonsn = shiftgrid ( 180 , rhum [:], lons , start = False ) lons = lonsn #Reference: https://annefou.github.io/metos_python/04-plotting/ time_index = 1860 fig = plt . figure ( figsize = [ 12 , 15 ]) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . set_title ( \"sst: {} \" . format ( period [ time_index ] . date ()), fontsize = 16 ) map = Basemap ( projection = 'cyl' , llcrnrlat =- 90 , urcrnrlat = 90 , \\ llcrnrlon =- 180 , urcrnrlon = 180 , resolution = 'c' , ax = ax ) map . drawcoastlines () map . fillcontinents ( color = '#ffe2ab' ) map . drawparallels ( np . arange ( - 90. , 120. , 30. ), labels = [ 1 , 0 , 0 , 0 ]) map . drawmeridians ( np . arange ( - 180. , 180. , 60. ), labels = [ 0 , 0 , 0 , 1 ]) llons , llats = np . meshgrid ( lons , lats ) x , y = map ( llons , llats ) cmap = c . ListedColormap ([ '#35978f' , '#ffffcc' , '#ffeda0' , '#fed976' , '#feb24c' , '#fd8d3c' , '#fc4e2a' , '#e31a1c' , '#bd0026' , '#800026' ]) bounds = list ( np . arange ( - 5 , 37 , 1 )) # bounds=list(np.arange(10,100,5)) norm = c . BoundaryNorm ( bounds , ncolors = cmap . N ) cs = map . contourf ( x , y , sst [ time_index ], cmap = cmap , norm = norm , levels = bounds ) fig . colorbar ( cs , cmap = cmap , norm = norm , boundaries = bounds , ticks = bounds , ax = ax , orientation = 'horizontal' ); plt . savefig ( '../figs/sst_1955.png' ) Agrega\u00e7\u00e3o dos Dados MDR Criaremos a seguir uma dataframe com as m\u00e9dias espaciais da regi\u00e3o MDR, para an\u00e1lise futura com o PDI. Usamos essa m\u00e9dia baseados na premissa razo\u00e1vel de que nesse corte espacial da MDR do atl\u00e2ntico os valores n\u00e3o variam muito dentro de um m\u00eas. Fazemos essa m\u00e9dias para an\u00e1lises de mais longo prazo como podem ver no notebook do PDI . print ( period [ 1860 ]) # -- Jan/1955 def get_mean ( data ): size = data . shape [ 0 ] new = np . array ([]) for i in range ( size ): new = np . append ( new , np . mean ( data [ i ,:,:])) return new #Come\u00e7aremos do \u00edndice 1788, representando Janeiro de 1949, para corresponder com os dados de PDI. data_at = pd . DataFrame ( get_mean ( sst_at [ 1788 :,:,:]), columns = [ \"sst\" ]) period_df = pd . DataFrame ( period [ 1788 :], columns = [ \"Date\" ]) period_df [ 'Year' ] = period_df . Date . map ( lambda x : x . year ) period_df [ 'Month' ] = period_df . Date . map ( lambda x : x . month ) data_at [ 'rhum' ] = pd . DataFrame ( get_mean ( rhum_at [ 1788 :,:,:]), columns = [ \"rhum\" ]) data_at [ 'slp' ] = pd . DataFrame ( get_mean ( slp_at [ 1788 :,:,:]), columns = [ \"slp\" ]) data_at [ 'wspd' ] = pd . DataFrame ( get_mean ( wspd_at [ 1788 :,:,:]), columns = [ \"wspd\" ]) data_at [ 'vwnd' ] = pd . DataFrame ( get_mean ( vwnd_at [ 1788 :,:,:]), columns = [ \"vwnd\" ]) data_at [ 'cldc' ] = pd . DataFrame ( get_mean ( cldc_at [ 1788 :,:,:]), columns = [ \"cldc\" ]) atlantic_mdr = pd . concat ([ period_df , data_at ], axis = 1 ) #C\u00f3digo que calcula desvios da temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica #Apenas para visualiza\u00e7\u00e3o cum_sum = {} for i in range ( 1 , 13 ): cum_sum [ i ] = 0 k = 0 #year count for i in range ( 0 , atlantic_mdr . shape [ 0 ] - 12 ): month = atlantic_mdr . iloc [ i ,:] . Month if month % 12 == 1 : k += 1 cum_sum [ month ] += atlantic_mdr . iloc [ i , 3 ] atlantic_mdr . loc [ atlantic_mdr . index [ i ], 'sst_anomaly' ] = atlantic_mdr . iloc [ i , 3 ] - cum_sum [ month ] / k atlantic_mdr . drop ( 'sst_anomaly' , axis = 1 ) . to_csv ( '../Datasets/atlantic_mdr.csv' , index = False ) atlantic_mdr . iloc [ 12 : 24 ,:] 1955-01-01 00:00:00 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Year Month sst rhum slp wspd vwnd cldc sst_anomaly 12 1950-01-01 1950 1 24.782794 75.675756 1012.699219 7.800527 -2.709708 4.139401 -0.138282 13 1950-02-01 1950 2 24.251776 78.633936 1012.883875 6.583213 -2.401613 3.965875 -0.129778 14 1950-03-01 1950 3 24.395219 78.334148 1013.134516 6.621354 -2.062759 4.087092 -0.020090 15 1950-04-01 1950 4 24.900423 77.248673 1011.096354 5.958877 -0.454326 3.146995 -0.010952 16 1950-05-01 1950 5 25.355377 79.638362 1010.577563 6.486837 0.516981 4.291411 -0.061999 17 1950-06-01 1950 6 26.110960 81.022388 1009.393293 6.946026 2.076482 4.650387 0.087756 18 1950-07-01 1950 7 26.516357 80.354432 1007.978285 6.342278 2.375750 5.184904 -0.070533 19 1950-08-01 1950 8 27.312561 81.914887 1007.856950 5.279884 1.956808 5.109233 -0.081636 20 1950-09-01 1950 9 27.811223 80.212305 1007.869104 4.947030 0.525868 5.305648 -0.057075 21 1950-10-01 1950 10 27.702060 80.523185 1010.697798 4.656318 -1.037250 4.423469 -0.026328 22 1950-11-01 1950 11 27.303333 81.614127 1009.745018 5.103683 -2.024746 3.765682 0.058410 23 1950-12-01 1950 12 26.351468 77.828336 1012.071718 5.696333 -4.750430 4.100716 0.117639 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 5 )) ax . plot ( np . arange ( 1949 , 2021 , 1 ), atlantic_mdr . groupby ([ 'Year' ]) . agg ({ 'sst_anomaly' : np . mean })[ 'sst_anomaly' ]) ax . set_title ( \"Desvios na temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica\" , fontsize = 14 ) ax . set_xlabel ( \"Ano\" , fontsize = 12 ) ax . set_ylabel ( \"Desvio da temperatura\" , fontsize = 12 ); Podemos ver que ap\u00f3s 1970-1980 inicia-se uma tend\u00eancia crescente de aumento de temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica. corr = atlantic_mdr . corr () corr . style . background_gradient ( cmap = 'coolwarm' ) #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col0 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col1 { background-color: #9dbdff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col2 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col3 { background-color: #b9d0f9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col4 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col5 { background-color: #f4987a; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col6 { background-color: #c9d7f0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col7 { background-color: #dfdbd9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col8 { background-color: #bcd2f7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col0 { background-color: #536edd; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col1 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col2 { background-color: #e36b54; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col3 { background-color: #f7ba9f; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col4 { background-color: #7ea1fa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col5 { background-color: #4055c8; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col6 { background-color: #e5d8d1; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col7 { background-color: #f2cbb7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col8 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col0 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col1 { background-color: #e7745b; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col2 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col3 { background-color: #ee8669; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col4 { background-color: #5977e3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col5 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col6 { background-color: #f7b194; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col7 { background-color: #f7a688; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col8 { background-color: #a7c5fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col0 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col1 { background-color: #eed0c0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col2 { background-color: #f18f71; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col3 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col4 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col5 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col6 { background-color: #d85646; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col7 { background-color: #ead5c9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col8 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col0 { background-color: #6485ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col1 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col2 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col3 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col4 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col5 { background-color: #e1dad6; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col6 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col7 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col8 { background-color: #6788ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col0 { background-color: #f7b497; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col1 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col2 { background-color: #6384eb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col3 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col4 { background-color: #f4c5ad; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col5 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col6 { background-color: #a1c0ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col7 { background-color: #b2ccfb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col8 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col0 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col1 { background-color: #c3d5f4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col2 { background-color: #f6bda2; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col3 { background-color: #d95847; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col4 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col5 { background-color: #5e7de7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col6 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col7 { background-color: #dedcdb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col8 { background-color: #3e51c5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col0 { background-color: #a2c1ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col1 { background-color: #edd2c3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col2 { background-color: #f6a283; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col3 { background-color: #f2c9b4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col4 { background-color: #688aef; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col5 { background-color: #9ebeff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col6 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col7 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col8 { background-color: #455cce; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col0 { background-color: #cbd8ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col1 { background-color: #9bbcff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col2 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col3 { background-color: #cdd9ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col4 { background-color: #e2dad5; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col5 { background-color: #a6c4fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col6 { background-color: #cedaeb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col7 { background-color: #b5cdfa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col8 { background-color: #b40426; color: #f1f1f1; } Year Month sst rhum slp wspd vwnd cldc sst_anomaly Year 1.000000 -0.010203 0.145686 -0.098235 0.048908 0.646453 -0.035567 0.241832 0.379038 Month -0.010203 1.000000 0.762722 0.392842 -0.434216 -0.385048 0.151316 0.380311 -0.017207 sst 0.145686 0.762722 1.000000 0.635370 -0.632031 -0.416486 0.432498 0.559898 0.312656 rhum -0.098235 0.392842 0.635370 1.000000 -0.768182 -0.380635 0.803748 0.305508 0.014696 slp 0.048908 -0.434216 -0.632031 -0.768182 1.000000 0.317397 -0.812037 -0.548567 0.129542 wspd 0.646453 -0.385048 -0.416486 -0.380635 0.317397 1.000000 -0.253309 0.007404 0.040588 vwnd -0.035567 0.151316 0.432498 0.803748 -0.812037 -0.253309 1.000000 0.236100 -0.002599 cldc 0.241832 0.380311 0.559898 0.305508 -0.548567 0.007404 0.236100 1.000000 0.019122 sst_anomaly 0.379038 -0.017207 0.312656 0.014696 0.129542 0.040588 -0.002599 0.019122 1.000000 Alguma correla\u00e7\u00f5es fortes interessantes: Temperatura do mar (sst) com ano (Year) Temperatura do mar (sst) e umidade (rhum) Velocidade de vento (wspd) e ano (Year) Press\u00e3o (slp) e Umidade (rhum) An\u00e1lises mais aprofundadas dessas vari\u00e1veis veremos no notebook de an\u00e1lise do PDI. month_sst = atlantic_mdr . groupby ( 'Month' )[ 'sst' ] . mean () month_rhum = atlantic_mdr . groupby ( 'Month' )[ 'rhum' ] . mean () # month_slp = atlantic_mdr.groupby('Month')['slp'].mean() # month_cldc = atlantic_mdr.groupby('Month')['cldc'].mean() m = np . arange ( 1 , 13 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 9 , 5 )) ax . plot ( month_sst , lw = 3.5 , color = 'blue' , label = 'sst' ) ax . set_xticks ( m ); ax . set_xlabel ( \"M\u00eas\" , fontsize = 14 ) ax . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - SST\" , fontsize = 14 ) ax2 = ax . twinx () ax2 . plot ( month_rhum , lw = 3.5 , color = 'orange' , label = 'rhum' ) ax2 . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - Umidade\" , fontsize = 14 ) ax . legend ( loc = 'best' , fontsize = 14 ); ax2 . legend ( loc = 'upper left' , fontsize = 14 ); plt . savefig ( \"../figs/mensal_sst_rhum.jpg\" ) Veja acima que os picos de temperatura e umidade, coincidem razoavelmente com a temporada de furac\u00f5es. month_slp = atlantic_mdr . groupby ( 'Month' )[ 'slp' ] . mean () month_cldc = atlantic_mdr . groupby ( 'Month' )[ 'cldc' ] . mean () m = np . arange ( 1 , 13 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 9 , 5 )) ax . plot ( month_slp , lw = 3.5 , color = 'blue' , label = 'slp' ) ax . set_xticks ( m ); ax . set_xlabel ( \"M\u00eas\" , fontsize = 14 ) ax . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - Press\u00e3o\" , fontsize = 14 ) ax2 = ax . twinx () ax2 . plot ( month_cldc , lw = 3.5 , color = 'orange' , label = 'cldc' ) ax2 . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - Nebulosidade\" , fontsize = 14 ) ax . legend ( loc = 'best' , fontsize = 14 ); ax2 . legend ( loc = 'lower left' , fontsize = 14 ); plt . savefig ( \"../figs/mensal_slp_cldc.jpg\" ) O pico de nebulosidade das nuvens tamb\u00e9m coincide de forma razo\u00e1vel com a temporada de furac\u00f5es. Assim como os valores mais baixos de press\u00e3o. Essas caracter\u00edsticas est\u00e3o relacionadas com a forma\u00e7\u00e3o do evento de tempestade forte ou furac\u00e3o. Dataset HURDAT2 (Hurricane) - An\u00e1lise e Limpeza Passemos agora a analisar os dados de tempestades e furac\u00f5es. # O foco principal do trabalho se dar\u00e1 nos dados do atl\u00e2ntico data_atl = pd . read_csv ( '../Datasets/atlantic.csv' , parse_dates = [ 'Date' ]) data_atl . dtypes ID object Name object Date datetime64 [ ns ] Time int64 Event object Status object Latitude object Longitude object Maximum Wind int64 Minimum Pressure int64 Low Wind NE int64 Low Wind SE int64 Low Wind SW int64 Low Wind NW int64 Moderate Wind NE int64 Moderate Wind SE int64 Moderate Wind SW int64 Moderate Wind NW int64 High Wind NE int64 High Wind SE int64 High Wind SW int64 High Wind NW int64 dtype : object # formatando dados de data data_atl [ 'Year' ] = pd . DatetimeIndex ( data_atl [ 'Date' ]) . year data_atl [ 'Month' ] = pd . DatetimeIndex ( data_atl [ 'Date' ]) . month data_atl [ 'Day' ] = pd . DatetimeIndex ( data_atl [ 'Date' ]) . day print ( data_atl . columns ) data_atl . head () Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Low Wind NE', 'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE', 'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW', 'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW', 'Year', 'Month', 'Day'], dtype='object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind SE Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW Year Month Day 0 AL011851 UNNAMED 1851-06-25 0 HU 28.0N 94.8W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 1 AL011851 UNNAMED 1851-06-25 600 HU 28.0N 95.4W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 2 AL011851 UNNAMED 1851-06-25 1200 HU 28.0N 96.0W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 3 AL011851 UNNAMED 1851-06-25 1800 HU 28.1N 96.5W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 4 AL011851 UNNAMED 1851-06-25 2100 L HU 28.2N 96.8W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 5 rows \u00d7 25 columns # Registro de Furac\u00f5es \u00e9 maior em determinada \u00e9poca do ano print ( data_atl . groupby ([ 'Month' ])[ 'ID' ] . count ()) Month 1 132 2 13 3 14 4 81 5 655 6 2349 7 3262 8 10857 9 18926 10 9802 11 2548 12 466 Name: ID, dtype: int64 fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) fig . suptitle ( 'N\u00famero de fura\u00e7\u00f5es registrados por m\u00eas do ano' , fontsize = 28 , y = 1.06 ) ax . bar ( data_atl . groupby ([ 'Month' ])[ 'Month' ] . mean (), data_atl . groupby ([ 'Month' ])[ 'ID' ] . count (), ls = '--' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de registros (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$M\u00eas$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) # ax.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( \"../figs/furacoes_mes.jpg\" ) Per\u00edodo de furac\u00f5es se concentra entre os meses de julho e novembro. Isso deve estar relacionado com o per\u00edodo anterior (ver\u00e3o do hemisf\u00e9rio norte, onde acontece o maior n\u00famero de casos) O aquecimento das \u00e1guas est\u00e1 intimamente ligado com a forma\u00e7\u00e3o das massas de ar que ocasionam os furac\u00f5es Isso nos d\u00e1 uma pista da forma de correla\u00e7\u00e3o temporal que devemos buscar para predizer os eventos ## Formata\u00e7\u00e3o dos dados para plot pressao x vento data_atl_ext = data_atl . copy () data_atl_mwmp = data_atl . copy () data_atl_mw = data_atl . copy () ind_nan_ext = [] ind_nan_mwmp = [] ind_nan_mw = [] for l in range ( len ( data_atl )): if ( data_atl_mw [ 'Maximum Wind' ][ l ] < 0 ): ind_nan_mw . append ( l ) ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( data_atl_mwmp [ 'Minimum Pressure' ][ l ] < 0 ): ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( min ( data_atl_ext [ 'Low Wind NE' ][ l ], data_atl_ext [ 'Low Wind SE' ][ l ], data_atl_ext [ 'Low Wind SW' ][ l ], data_atl_ext [ 'Low Wind NW' ][ l ], data_atl_ext [ 'Moderate Wind NE' ][ l ], data_atl_ext [ 'Moderate Wind SE' ][ l ], data_atl_ext [ 'Moderate Wind SW' ][ l ], data_atl_ext [ 'Moderate Wind NW' ][ l ], data_atl_ext [ 'High Wind NE' ][ l ], data_atl_ext [ 'High Wind SE' ][ l ], data_atl_ext [ 'High Wind SW' ][ l ], data_atl_ext [ 'High Wind NW' ][ l ]) < 0 ): ind_nan_ext . append ( l ) data_atl_ext = data_atl_ext . drop ( ind_nan_ext , 0 ) data_atl_mwmp = data_atl_mwmp . drop ( ind_nan_mwmp , 0 ) data_atl_mw = data_atl_mw . drop ( ind_nan_mw , 0 ) print ( len ( data_atl_ext )) print ( len ( data_atl_mwmp )) print ( len ( data_atl_mw )) --------------------------------------------------------------------------- NameError Traceback ( most recent call last ) < ipython - input - 3 - 754270 ff204f > in < module > 1 ## Formata\u00e7\u00e3o dos dados para plot pressao x vento ----> 2 data_atl_ext = data_atl.copy() 3 data_atl_mwmp = data_atl . copy () 4 data_atl_mw = data_atl . copy () 5 NameError : name 'data_atl' is not defined fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . scatter ( data_atl_mwmp [ 'Minimum Pressure' ], data_atl_mwmp [ 'Maximum Wind' ], alpha = 0.2 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Press\u00e3o M\u00ednima$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) # ax.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/pressaoMin_Velo_max.jpg' ) Abaixo formatamos as Latitudes e Longitudes para remover os terminadores W, E, N e S. Como indicador de hemisf\u00e9rio, usamos sinal negativo para sul e oeste e positovo para norte e leste. Precisaremos que esses dados sejam numericos para aplica\u00e7\u00f5es futuras e n\u00e3o textuais. data_atl [[ 'Latitude' , 'Longitude' ]] . dtypes Latitude object Longitude object dtype: object data_atl . Latitude = data_atl . Latitude . apply ( lambda x : - float ( x . rstrip ( \"S\" )) if x . endswith ( \"S\" ) else float ( x . rstrip ( \"N\" ))) data_atl . Longitude = data_atl . Longitude . apply ( lambda x : - float ( x . rstrip ( \"W\" )) if x . endswith ( \"W\" ) else float ( x . rstrip ( \"E\" ))) data_atl [[ 'Latitude' , 'Longitude' ]] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Latitude Longitude count 49105.000000 49105.000000 mean 27.044904 -65.682533 std 10.077880 19.687240 min 7.200000 -359.100000 25% 19.100000 -81.000000 50% 26.400000 -68.000000 75% 33.100000 -52.500000 max 81.000000 63.000000 X_train = data_atl . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) ax . plot ( data_atl . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) # ax.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/numero_furacoes1851-2015.jpg' ) R^2_train = 0.40918060978409 Par\u00e2metro_const = -3940.349139351199 Par\u00e2metro_Year = 2.192423797184304 Vemos acima que o n\u00famero de registro de fura\u00e7\u00f5es tem crescido desde 1850, mas isso se deve \u00e0 maior capacidade de detec\u00e7\u00e3o com o passar dos anos. X_train = data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) ax . plot ( data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/maxwind_1851-2015.jpg' ) R^2_train = 0.49772308255924613 Par\u00e2metro_const = 345.3529584491673 Par\u00e2metro_Year = -0.15025433346159078 Vemos tamb\u00e9m que a velocidade m\u00e1xima de vento sustentada pelos furac\u00f5es reduziu, mas isso tamb\u00e9m se deve \u00e0 maior capacidade de registro de eventos de pequeno porte, que acabam pesando a m\u00e9dia para baixo. Assim, para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte; Consideramos apenas tempestades cuja dura\u00e7\u00e3o em dias \u00e9 maior que 2, e cuja classifica\u00e7\u00e3o na escala Saffir-Simpson seja no m\u00ednimo Tempestade Tropical. Para essa classifica\u00e7\u00e3o, a velocidade m\u00e1xima sustentada de vento deve ultrapassar 63km/h o que equivale a 34 knots (milhas n\u00e1uticas). #Filtro de Dura\u00e7\u00e3o data_atl_fdur = data_atl_mw . copy () duration = data_atl_mw . groupby ([ 'ID' ])[ 'Date' ] . max () - data_atl_mw . groupby ([ 'ID' ])[ 'Date' ] . min () duration . name = 'Duration' #print(duration) data_atl_fdur = pd . merge ( data_atl_fdur , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_fdur [ 'Duration' ] = pd . to_numeric ( data_atl_fdur [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_fdur = data_atl_fdur [ data_atl_fdur [ 'Duration' ] > 2 ] print ( len ( data_atl_fdur )) 46350 #Filtro de Max_Windspeed data_atl_fwind = data_atl_fdur . copy () data_atl_fwind = data_atl_fwind [ data_atl_fwind [ 'Maximum Wind' ] > 34 ] print ( len ( data_atl_fwind )) 35696 Vejamos os novos plots com os dados filtrados: # Com o novo filtro, o vi\u00e9s do aumento no n\u00famero de furac\u00f5es ao longo dos anos reduziu, mas ainda h\u00e1 um aumento # Isso mostra que essa tend\u00eancia pode ser algo n\u00e3o viesada, e que gera preocupa\u00e7\u00e3o pelo futuro X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/filtered_numero_furacoes1851-2015.jpg' ) R^2_train = 0.15573477903937427 Par\u00e2metro_const = -1661.4327723310091 Par\u00e2metro_Year = 0.9714289530628057 X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/filtered_maxwind_1851-2015.jpg' ) R^2_train = 0.11158706746149893 Par\u00e2metro_const = 167.44362825887208 Par\u00e2metro_Year = -0.05470982174808241 Com o novo filtro, o vi\u00e9s da redu\u00e7\u00e3o da velocidade m\u00e1xima sustentada de vento reduziu, quase para o n\u00edvel constante Isso pode significar que os filtros est\u00e3o relativamente bem adequados para retirada do vi\u00e9s inicial dos dados. Geraremos ent\u00e3o um novo DataFrame com algums filtros importantes: Velocidade M\u00e1xima Sustentada > 34 milhas n\u00e1uticas Dura\u00e7\u00e3o > 2 dias Furac\u00f5es a partir de 1950 (quando a capacidade de medi\u00e7\u00e3o come\u00e7a a evoluir O c\u00f3digo abaixo aplica esses filtros. data_atl_mw2 = data_atl . copy () data_atl_mw2_filtrado3 = data_atl_mw2 . copy () Lat_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Latitude' ] . first () Lat_min . name = 'Lat_min' data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lat_min , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ abs ( data_atl_mw2_filtrado3 [ 'Lat_min' ] - 12.5 ) > 0 ] Lon_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Longitude' ] . min () Lon_min . name = 'Lon_min' data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lon_min , how = 'inner' , left_on = 'ID' , right_index = True ) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Lon_min' ] > - 180 ] Wind_max = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Maximum Wind' ] . max () Wind_max . name = 'Wind_max' #print(Wind_max) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Wind_max , how = 'inner' , on = 'ID' ) #left_on='ID', right_index=True) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Wind_max' ] > 34 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Year' ] > 1950 ] duration = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date' ] . max () - data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date' ] . min () duration . name = 'Duration' #print(duration) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_mw2_filtrado3 [ 'Duration' ] = pd . to_numeric ( data_atl_mw2_filtrado3 [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Duration' ] > 2 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 . drop ([ 'Lat_min' , 'Lon_min' , 'Wind_max' ], 1 ) #data_atl_mw2_filtrado3.head() print ( len ( data_atl_mw2_filtrado3 )) print ( len ( data_atl . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl )) print ( len ( data_atl_mw2 )) data_atl_mw2_filtrado3 . head () 22386 1814 685 49105 49105 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW Year Month Day Duration 21948 AL011951 UNNAMED 1951-01-02 1200 EX 30.5 -58.0 50 -999 ... -999 -999 -999 -999 -999 -999 1951 1 2 10 21949 AL011951 UNNAMED 1951-01-02 1800 EX 29.9 -56.8 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 2 10 21950 AL011951 UNNAMED 1951-01-03 0 EX 29.0 -55.7 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 3 10 21951 AL011951 UNNAMED 1951-01-03 600 EX 27.5 -54.8 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 3 10 21952 AL011951 UNNAMED 1951-01-03 1200 EX 26.5 -54.5 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 3 10 5 rows \u00d7 26 columns Uma limpeza final ser\u00e1 feita nas entradas de texto: df = data_atl_mw2_filtrado3 . copy () np . unique ( df . Name )[ 0 : 5 ], np . unique ( df . Event ), np . unique ( df . Status ) (array([' AMY', ' ANA', ' BOB', ' DOG', ' DON'], dtype=object), array([' ', ' C', ' G', ' I', ' L', ' P', ' R', ' S', ' T', ' W'], dtype=object), array([' DB', ' EX', ' HU', ' LO', ' SD', ' SS', ' TD', ' TS', ' WV'], dtype=object)) Veja que as entradas est\u00e3o espa\u00e7adas, abaixo corrigimos isso: df [ 'Name' ] = df [ 'Name' ] . apply ( lambda x : x . strip ()) df [ 'Event' ] = df [ 'Event' ] . apply ( lambda x : x . strip ()) df [ 'Status' ] = df [ 'Status' ] . apply ( lambda x : x . strip ()) np . unique ( df . Name )[ 0 : 5 ], np . unique ( df . Event ), np . unique ( df . Status ) (array(['ABBY', 'ABLE', 'AGNES', 'ALBERTO', 'ALEX'], dtype=object), array(['', 'C', 'G', 'I', 'L', 'P', 'R', 'S', 'T', 'W'], dtype=object), array(['DB', 'EX', 'HU', 'LO', 'SD', 'SS', 'TD', 'TS', 'WV'], dtype=object)) #Salvando em csv data_atl_mw2_filtrado3 = df . copy () data_atl_mw2_filtrado3 . to_csv ( '../Datasets/data_atl_mw2_filtrado3.csv' , encoding = 'utf-8' , index = False ) Uni\u00e3o dos dados via k-NN ponderado Abaixo vamos gerar um novo dataframe que pegar\u00e1 o filtrado3.csv gerado acima e buscar\u00e1 os dados clim\u00e1ticos nos datasets da ICOADS diretamente dos arquivos .nc. Faremos essa busca via coordenadas e para lidar com dados faltantes implementaremos um k-NN ponderado pelo inverso das dist\u00e2ncias entre as coordenadas originais e o vizinho considerado no algoritmo. Leitura dos dados e limpezas adicionais df = pd . read_csv ( '../Datasets/data_atl_mw2_filtrado3.csv' , parse_dates = [ 'Date' ]) df . dtypes ID object Name object Date datetime64 [ ns ] Time int64 Event object Status object Latitude float64 Longitude float64 Maximum Wind int64 Minimum Pressure int64 Low Wind NE int64 Low Wind SE int64 Low Wind SW int64 Low Wind NW int64 Moderate Wind NE int64 Moderate Wind SE int64 Moderate Wind SW int64 Moderate Wind NW int64 High Wind NE int64 High Wind SE int64 High Wind SW int64 High Wind NW int64 Year int64 Month int64 Day int64 Duration int64 dtype : object pd . DataFrame ( zip ( df . columns , [ np . sum ( df [ x ] == - 999 ) / len ( df ) for x in df . columns ]) , columns = [ 'Variable' , 'Missing Ratio' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Variable Missing Ratio 0 ID 0.000000 1 Name 0.000000 2 Date 0.000000 3 Time 0.000000 4 Event 0.000000 5 Status 0.000000 6 Latitude 0.000000 7 Longitude 0.000000 8 Maximum Wind 0.000000 9 Minimum Pressure 0.256723 10 Low Wind NE 0.750201 11 Low Wind SE 0.750201 12 Low Wind SW 0.750201 13 Low Wind NW 0.750201 14 Moderate Wind NE 0.750201 15 Moderate Wind SE 0.750201 16 Moderate Wind SW 0.750201 17 Moderate Wind NW 0.750201 18 High Wind NE 0.750201 19 High Wind SE 0.750201 20 High Wind SW 0.750201 21 High Wind NW 0.750201 22 Year 0.000000 23 Month 0.000000 24 Day 0.000000 25 Duration 0.000000 Removeremos as colunas abaixo, pois n\u00e3o utilizaremos nas an\u00e1lises e s\u00e3o muitos esparsas df = df . drop ([ 'Low Wind NE' , 'Low Wind SE' , 'Low Wind SW' , 'Low Wind NW' , 'Moderate Wind NE' , 'Moderate Wind SE' , 'Moderate Wind SW' , 'Moderate Wind NW' , 'High Wind NE' , 'High Wind SE' , 'High Wind SW' , 'High Wind NW' ], axis = 1 ) Na c\u00e9lula abaixo, lemos os dados clim\u00e1ticos e os salvamos en vari\u00e1veis para uso futuro. sst_mean = nc . Dataset ( 'sst.mean.nc' , 'r' ) rhum_mean = nc . Dataset ( 'rhum.mean.nc' , 'r' ) wspd_mean = nc . Dataset ( 'wspd.mean.nc' , 'r' ) slp_mean = nc . Dataset ( 'slp.mean.nc' , 'r' ) # vwnd_mean = nc.Dataset('Datasets/vwnd.mean.nc','r') # Resolvemos n\u00e3o considerar o vwnd por ser ru\u00eddo cldc_mean = nc . Dataset ( \"cldc.mean.nc\" , 'r' ) lats = sst_mean . variables [ 'lat' ][:] lons = sst_mean . variables [ 'lon' ][:] time = sst_mean . variables [ 'time' ][:] lons = [ x - 180 for x in lons ] lats = [ x for x in lats ] sst = sst_mean . variables [ 'sst' ][:,:,:] rhum = rhum_mean . variables [ 'rhum' ][:,:,:] wspd = wspd_mean . variables [ 'wspd' ][:,:,:] slp = slp_mean . variables [ 'slp' ][:,:,:] cldc = cldc_mean . variables [ 'cldc' ][:,:,:] sst_mean . close () wspd_mean . close () rhum_mean . close () slp_mean . close () cldc_mean . close () Formula da dist\u00e2ncia Dados duas coordenadas (\\varphi_1,\\lambda_1) e ( \\varphi_2,\\lambda_2) em radianos, a F\u00f3rmula Haversine \u00e9 capaz de calcular a dist\u00e2ncia real entre esses dois pontos no mapa: Onde r \u00e9 o raio da Terra. Usando r aproximadamente igual a 6371 km o valor de d ser\u00e1 a dist\u00e2ncia em km dos dois pontos dados em coordenadas geogr\u00e1ficas. Abaixo, seguimos uma implementa\u00e7\u00e3o equivalente obtida no Stack Overflow . def distance ( lat1 , lon1 , lat2 , lon2 ): # approximate radius of earth in km R = 6371.0 lat1 = radians ( lat1 ) lon1 = radians ( lon1 ) lat2 = radians ( lat2 ) lon2 = radians ( lon2 ) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) if R * c == 0 : return 0.000000001 #Handling Zero Division problems else : return R * c k-NN ponderado por inverso da dist\u00e2ncia Abaixo segue a implementa\u00e7\u00e3o de algumas fun\u00e7\u00f5es auxiliares para realizar esse preenchimento de dados almejado. o que queremos \u00e9 dos dados que filtramos do HURDAT, para cada linha, usando as coordenadas geogr\u00e1ficas, gerar novas colunas contendo os dados do ICOADS para aquela data espef\u00edcica e aquela coordenada espec\u00edfica. Por conta de dados faltantes no ICOADS, nem sempre existir\u00e1 registros para todas as coordenadas. Assim, buscamos os k vizinhos mais pr\u00f3ximos a calculamos a m\u00e9dia entre eles, por\u00e9m a m\u00e9dia \u00e9 ponderada pelo inverso da dist\u00e2ncia entre o vizinho e a coordenada original (usando a F\u00f3rmula Haversive). Sendo assim, pontos mais distantes ter\u00e3o menor peso na m\u00e9dia, enquanto pontos mais pr\u00f3ximos ser\u00e3o considerados mais importantes. Por simplicidade, usamos k=15 . Funcionamento das fun\u00e7\u00f5es: get_coords: recebe uma coordenada e a lista de coordenadas dispon\u00edveis no ICOADS. Retorna uma coordenada que existe na lista passada. Precisamos dessa fun\u00e7\u00e3o pois os dados de coordenadas do ICOADS s\u00e3o intervalados 2 a 2. get_neighboors: recebe uma data, latitude, longitude, uma array de dados do ICOADS e a quantidade de vizinhos a buscar. Retorna uma lista de k vizinhos e uma lista de dist\u00e2ncias em kilometros. get_data: usa as fun\u00e7\u00f5es acima para gerar a m\u00e9dia ponderada pelo inverso da dist\u00e2ncia get_data_df: aplica as fun\u00e7\u00f5es acima para gerar uma nova coluna nos dados HURDAT2 da forma como almejado inicialmente. def get_coord ( coord , l ): if int ( coord ) in l : return int ( coord ) elif int ( coord ) + 1 in l : return int ( coord ) + 1 elif int ( coord ) - 1 in l : return int ( coord ) - 1 elif int ( coord ) + 2 in l : return int ( coord ) + 2 elif int ( coord ) - 2 in l : return int ( coord ) - 2 def get_neighboors ( t , lat , lon , marray , k ): nb = [] dist = [] lat_data = get_coord ( lat , lats ) #nearest lat in our lats list lon_data = get_coord ( lon , lons ) lat_i = lats . index ( lat_data ) lon_i = lons . index ( lon_data ) if marray [ t , lat_i , lon_i ]: nb . append ( marray [ t , lat_i , lon_i ]) dist . append ( distance ( lat , lon , lat_data , lon_data )) j = 1 while len ( nb ) < k : lower_i = ( lat_i - j ) #%90 upper_i = ( lat_i + j ) #%90 right_i = ( lon_i + j ) #%180 left_i = ( lon_i - j ) #%180 # if right_i>=len() left_values = marray [ t , lower_i : upper_i , left_i ] upper_values = marray [ t , upper_i , left_i : right_i ] right_values = marray [ t , upper_i : lower_i : - 1 , right_i ] lower_values = marray [ t , lower_i , right_i : left_i : - 1 ] [ nb . append ( x ) for x in left_values if x ] [ dist . append ( distance ( lat , lon , lats [ i ], lons [ left_i ])) for i in range ( len ( left_values )) if left_values [ i ]] if len ( nb ) >= k : break [ nb . append ( x ) for x in upper_values if x ] [ dist . append ( distance ( lat , lon , lats [ upper_i ], lons [ i ])) for i in range ( len ( upper_values )) if upper_values [ i ]] if len ( nb ) >= k : break [ nb . append ( x ) for x in right_values if x ] [ dist . append ( distance ( lat , lon , lats [ i ], lons [ right_i ])) for i in range ( len ( right_values )) if right_values [ i ]] if len ( nb ) >= k : break [ nb . append ( x ) for x in lower_values if x ] [ dist . append ( distance ( lat , lon , lats [ lower_i ], lons [ i ])) for i in range ( len ( lower_values )) if lower_values [ i ]] if len ( nb ) >= k : break j += 1 if prob != []: print ( prob ) return nb , dist period = pd . date_range ( start = \"1800-01-01\" , end = \"2020-07-01\" , freq = \"MS\" ) . to_pydatetime () . tolist () def get_data ( datetime , lat , lon , dataset , k ): #k is the number of neighboors year = datetime . year month = datetime . month time_index = period . index ( dt . datetime ( year , month , 1 , 0 , 0 )) nb , dist = get_neighboors ( time_index , lat , lon , dataset , k ) inv_dist = [ 1 / x for x in dist ] return sum ( nb [ i ] * inv_dist [ i ] for i in range ( len ( nb ))) / sum ( inv_dist ) def get_data_df ( df , dataset , label , k = 15 ): output = pd . DataFrame ( np . zeros ([ len ( df ), 1 ]), columns = [ label ]) for i in range ( len ( df )): output . loc [ i , label ] = get_data ( df . Date [ i ], df . Latitude [ i ], df . Longitude [ i ], dataset , k ) return output Abaixo executamos tudo que foi implementado acima para gerar o novo dataframe. Salvamos em um novo csv que usaremos nas pr\u00f3ximas an\u00e1lises. %% time # Esta c\u00e9lula demora um pouco a ser executada; Todos essas dados j\u00e1 est\u00e3o salvos em data_atl_merged2.csv df . loc [:, 'sst' ] = get_data_df ( df , sst , 'sst' ) print ( \"OK -- sst\" ) df . loc [:, 'rhum' ] = get_data_df ( df , rhum , 'rhum' ) print ( \"OK -- rhum\" ) df . loc [:, 'wspd' ] = get_data_df ( df , rhum , 'wspd' ) print ( \"OK -- wspd\" ) df . loc [:, 'slp' ] = get_data_df ( df , slp , 'slp' ) print ( \"OK -- slp\" ) df . loc [:, 'cldc' ] = get_data_df ( df , cldc , 'cldc' ) print ( \"OK -- cldc \\n \" ) df . columns OK -- sst OK -- rhum OK -- wspd OK -- slp OK -- cldc CPU times : user 2 min 34 s , sys : 868 ms , total : 2 min 35 s Wall time : 2 min 33 s Index ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Year' , 'Month' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ], dtype = 'object' ) df . to_csv ( '../Datasets/data_atl_merged2.csv' , index = 0 ) df = pd . read_csv ( '../Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) df . dtypes ID object Name object Date datetime64 [ ns ] Time int64 Event object Status object Latitude float64 Longitude float64 Maximum Wind int64 Minimum Pressure int64 Date_c object Year int64 Month int64 Day int64 Latitude_c float64 Longitude_c float64 Duration int64 sst float64 rhum float64 wspd float64 slp float64 cldc float64 dtype : object Outras Visualiza\u00e7\u00f5es import warnings warnings . filterwarnings ( 'ignore' ) dfplot = df [[ 'ID' , 'Year' , 'Maximum Wind' ]] def category ( mw ): if mw >= 137 : return \"Categoria 5\" elif mw >= 113 : return \"Categoria 4\" elif mw >= 96 : return \"Categoria 3\" elif mw >= 83 : return \"Categoria 2\" elif mw >= 64 : return \"Categoria 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" plt . title ( \"N\u00ba Tempestades por Categoria M\u00e1xima\" , fontsize = 15 ) cat_id = dfplot . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) dfplot . loc [:, 'Categoria' ] = dfplot . ID . apply ( lambda x : cat_id [ x ]) dfplot . groupby ( 'Categoria' )[ 'ID' ] . count () . plot . bar (); # plt.savefig('figs/hur_by_category.jpg') #Major Hurricanes by Year plt . title ( \"N\u00ba Furac\u00f5es Graves por ano (Cat>=3)\" , fontsize = 15 ) major_df = dfplot [ dfplot . Categoria . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] major_df . groupby ( 'Year' )[ 'ID' ] . count () . plot () plt . xlabel ( \"Ano\" ); # plt.savefig(\"figs/major_hurricanes_year.jpg\") Nos plots abaixo usaremos a biblioteca troPYcal. Internamente ela tem o dataset HURDAT2 atualizado e possui fun\u00e7\u00f5es de visualiza\u00e7\u00e3o prontas, simples de serem usadas. Refer\u00eancia: https://tropycal.github.io/tropycal/examples/index.html hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) storm = hurdat_atl . get_storm (( 'michael' , 2018 )) # storm . plot ( return_ax = True ) plt . savefig ( '../figs/troPycal_michal18.jpg' ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (4.0 seconds) storm . plot_tors ( plotPPH = True ) plt . savefig ( '../figs/troPYcal_michaelPPH.jpg' ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (10.42 seconds) <Figure size 432x288 with 0 Axes> storm . plot_nhc_forecast ( forecast = 2 , return_ax = True ) plt . savefig ( '../figs/troPYcal_michael_fore2.jpg' ) storm . plot_nhc_forecast ( forecast = 12 , return_ax = True ) plt . savefig ( '../figs/troPYcal_michael_fore12.jpg' ) ibtracs = tracks . TrackDataset ( basin = 'all' , source = 'ibtracs' , ibtracs_mode = 'jtwc_neumann' , catarina = True ) storm = ibtracs . get_storm (( 'catarina' , 2004 )) storm . plot ( return_ax = True ) plt . savefig ( '../figs/troPYcal_catarina.jpg' ) --> Starting to read in ibtracs data --> Completed reading in ibtracs data (114.01 seconds) tor_data = tornado . TornadoDataset () tor_ax , domain , leg_tor = tor_data . plot_tors ( dt . datetime ( 2011 , 4 , 27 ), plotPPH = True , return_ax = True ) tor_ax plt . savefig ( '../figs/troPYcal_dailyPPH.jpg' ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (9.15 seconds) hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) # # hurdat_atl . assign_storm_tornadoes ( dist_thresh = 750 ) # # storm = hurdat_atl . get_storm (( 'ivan' , 2004 )) # # storm . plot_tors ( plotPPH = True , return_ax = True ) plt . savefig ( '../figs/troPYcal_ivan04.jpg' ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (8.13 seconds) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (9.86 seconds) --> Starting to assign tornadoes to storms --> Completed assigning tornadoes to storm (403.30 seconds) hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) #Retrieve Hurricane Michael from 2018 storm = hurdat_atl . get_storm (( 'michael' , 2018 )) #Retrieve the 2017 Atlantic hurricane season season = hurdat_atl . get_season ( 2017 ) #Printing the Storm object lists relevant data: print ( storm ) print ( hurdat_atl . search_name ( 'Michael' )) # --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (4.39 seconds) < tropycal . tracks . Storm > Storm Summary : Maximum Wind : 140 knots Minimum Pressure : 919 hPa Start Date : 0600 UTC 07 October 2018 End Date : 1800 UTC 11 October 2018 Variables : date ( datetime ) [ 2018 - 10 - 06 18 : 00 : 00 .... 2018 - 10 - 15 18 : 00 : 00 ] extra_obs ( int64 ) [ 0 .... 0 ] special ( str ) [ .... ] type ( str ) [ LO .... EX ] lat ( float64 ) [ 17 . 8 .... 41 . 2 ] lon ( float64 ) [ - 86 . 6 .... - 10 . 0 ] vmax ( int64 ) [ 25 .... 35 ] mslp ( int64 ) [ 1006 .... 1001 ] wmo_basin ( str ) [ north_atlantic .... north_atlantic ] More Information : id : AL142018 operational_id : AL142018 name : MICHAEL year : 2018 season : 2018 basin : north_atlantic source_info : NHC Hurricane Database source : hurdat ace : 12 . 5 realtime : False [ 2000 , 2012 , 2018 ] hurdat_atl . ace_climo ( plot_year = 2018 , compare_years = 2017 ) plt . savefig ( '../figs/troPYcal_ACE.jpg' ) <Figure size 432x288 with 0 Axes> hurdat_atl . ace_climo ( rolling_sum = 30 , plot_year = 2018 , compare_years = 2017 ) plt . savefig ( '../figs/troPYcal_ACE_rolling.jpg' ) <Figure size 432x288 with 0 Axes> hurdat_atl . wind_pres_relationship ( storm = ( 'sandy' , 2012 )) plt . savefig ( '../figs/troPYcal_wind_press.jpg' ) <Figure size 432x288 with 0 Axes> # ibtracs = tracks.TrackDataset(basin='all',source='ibtracs',ibtracs_mode='jtwc_neumann',catarina=True) ibtracs . gridded_stats ( request = \"maximum wind\" , return_ax = True ) # plt . savefig ( '../figs/troPYcal_maxwind.jpg' ) --> Getting filtered storm tracks --> Grouping by lat/lon/storm --> Generating plot ibtracs . gridded_stats ( request = \"number of storms\" , thresh = { 'dv_min' : 30 }, prop = { 'cmap' : 'plasma_r' }) plt . savefig ( '../figs/troPYcal_num_storms.jpg' ) --> Getting filtered storm tracks --> Grouping by lat/lon/storm --> Generating plot","title":"Data Cleaning + EDA"},{"location":"Data_Cleaning_and_EDA/#exploracao-limpeza-dos-dados-e-visualizacoes","text":"Trabalharemos com tr\u00eas datasets. O primeiro \u00e9 um subconjunto do ICOADS (International Comprehensive Ocean-Atmosphere Data Set) fornecido pela NOAA (National Oceanic and Atmospheric Administration) que possui v\u00e1rios dados clim\u00e1ticos de cobertura mensal, desde 1800 at\u00e9 o presente, e com abrang\u00eancia mar\u00edtima; Possui dados como temperatura do mar, umidade, press\u00e3o, cobertura de nuvens, velocidade de vento, etc. Na c\u00e9lula abaixo, h\u00e1 um c\u00f3digo que baixa o subconjunto que usaremos diretamente da fonte oficial. N\u00e3o colocamos estes arquivos no GitHub por conta do limite de tamanho imposto sobre os uploads/commits. # !sh ../download_data.sh O segundo grande conjunto de dados \u00e9 o HURDAT2 cuja fonte oficial \u00e9 a NHC (National Hurricane Center), divis\u00e3o da NOAA respons\u00e1vel pelos fura\u00e7\u00f5es e tempestades. Os dados do Kaggle fornecem dados de tempestades e furac\u00f5es desde o s\u00e9culo XIX at\u00e9 2015 no pac\u00edfico e atl\u00e2ntico, mas iremos focar nossa an\u00e1lise no dadaset do atl\u00e2ntico. O terceiro, \u00e9 um dado com o \u00edndice PDI, mas ele j\u00e1 vem da fonte em boas condi\u00e7\u00f5es e n\u00e3o necessita de limpeza. Veja mais no notebook PowerDissipationIndex.ipynb . ## Imports % matplotlib inline import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as c from mpl_toolkits.basemap import Basemap , shiftgrid import pandas as pd import netCDF4 as nc from math import sin , cos , sqrt , atan2 , radians import scipy import geopy import xarray import networkx import requests import cartopy import tropycal import tropycal.tracks as tracks import tropycal.tornado as tornado import datetime as dt import statsmodels.api as sm from statsmodels.api import OLS","title":"Explora\u00e7\u00e3o, Limpeza dos dados e Visualiza\u00e7\u00f5es"},{"location":"Data_Cleaning_and_EDA/#dataset-icoads","text":"Vamos analisar os dados baixados acima. O formato .nc \u00e9 leg\u00edvel ao python atrav\u00e9s da biblioteca netCDF4 que est\u00e1 nos pr\u00e9-requisitos. Al\u00e9m disso, ser\u00e3o necess\u00e1rias instalar os pacotes abaixo; Descomente a c\u00e9lula, execute-a e d\u00ea um \"Restart Runtime\" no Jupyter. Certifique-se que instalou todos os pr\u00e9-requisitos do arquivo \"requirements.txt\". # !apt-get install libgeos-3.5.0 # !apt-get install libgeos-dev # !pip install https://github.com/matplotlib/basemap/archive/master.zip","title":"Dataset ICOADS"},{"location":"Data_Cleaning_and_EDA/#analise-de-dados-faltantes","text":"sst_mean = nc . Dataset ( '../Datasets/sst.mean.nc' , 'r' ) rhum_mean = nc . Dataset ( '../Datasets/rhum.mean.nc' , 'r' ) wspd_mean = nc . Dataset ( '../Datasets/wspd.mean.nc' , 'r' ) slp_mean = nc . Dataset ( '../Datasets/slp.mean.nc' , 'r' ) vwnd_mean = nc . Dataset ( '../Datasets/vwnd.mean.nc' , 'r' ) cldc_mean = nc . Dataset ( \"../Datasets/cldc.mean.nc\" , 'r' ) lats = sst_mean . variables [ 'lat' ][:] lons = sst_mean . variables [ 'lon' ][:] time = sst_mean . variables [ 'time' ][:] sst = sst_mean . variables [ 'sst' ][:,:,:] rhum = rhum_mean . variables [ 'rhum' ][:,:,:] wspd = wspd_mean . variables [ 'wspd' ][:,:,:] slp = slp_mean . variables [ 'slp' ][:,:,:] vwnd = vwnd_mean . variables [ 'vwnd' ][:,:,:] cldc = cldc_mean . variables [ 'cldc' ][:,:,:] sst_mean . close () wspd_mean . close () rhum_mean . close () slp_mean . close () vwnd_mean . close () cldc_mean . close () period = pd . date_range ( start = \"1800-01-01\" , end = \"2020-07-01\" , freq = \"MS\" ) . to_pydatetime () . tolist () def get_missing ( data : list , labels : list ) -> dict : missing = {} lenght = data [ 0 ] . shape [ 0 ] for j , item in enumerate ( data ): missing [ labels [ j ]] = [] for i in range ( lenght ): missing [ labels [ j ]] . append ( 100 * np . sum ( item [ i ] . mask ) / item [ i ] . data . size ) return missing missing = get_missing ([ sst , wspd , rhum , slp , vwnd , cldc ],[ 'sst' , 'wspd' , 'rhum' , 'slp' , 'vwnd' , 'cldc' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Dados Faltantes - Global\" , fontsize = 15 ) ax . set_xlabel ( \"Ano\" , fontsize = 12 ) ax . set_ylabel ( \" % d e dados faltantes\" , fontsize = 12 ) for key , value in missing . items (): ax . plot ( period , missing [ key ], label = key ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( '../figs/missing-global.png' ) plt . show () print ( type ( sst )) <class 'numpy.ma.core.MaskedArray'> Legenda: sst: Sea Surface Temperature (Temperatura na superf\u00edcie do mar) wspd: Scalar Wind Speed (Velocidade de vento escalar) rhum: Relative Humidity (Umidade relativa) slp: Sea Level Pressure (Press\u00e3o no n\u00edvel do mar) vwnd: V-wind component (Componente V-wind) cldc: Cloudiness (Nebulosidade das nuvens) Como os continentes representam aproximadamente 29,1\\% da suferf\u00edcie terrestre e nossos dados s\u00f3 preenchem os oceanos, os continentes s\u00e3o preenchidos como dados inexistentes. Ent\u00e3o naturalmente nossa cota inferior de dados faltantes \u00e9 essa porcentagem. Note que os dados lidos vem no formato de \"numpy masked array\" que tem um atributo \"mask\" que \u00e9 indicadora de dado faltante, isso nos ajudar\u00e1 a lidar com esses dados. Como vemos no plot acima temos v\u00e1rias d\u00e9cadas com n\u00edveis de dados faltantes acima de 90\\% mas vamos analisar focadamente e atl\u00e2ntico norte, que \u00e9 nossa regi\u00e3o de estudos. sst_at = sst [:, 34 : 40 , 51 : 82 ] #10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W wspd_at = wspd [:, 34 : 40 , 51 : 82 ] rhum_at = rhum [:, 34 : 40 , 51 : 82 ] slp_at = slp [:, 34 : 40 , 51 : 82 ] vwnd_at = vwnd [:, 34 : 40 , 51 : 82 ] cldc_at = cldc [:, 34 : 40 , 51 : 82 ] # sst_pac = sst[:,14:45,0:41] #0\u00b0-60\u00b0N, 100\u00b0W-180\u00b0W # wspd_pac = wspd[:,14:45,0:41] # rhum_pac = rhum[:,14:45,0:41] missing_at = get_missing ([ sst_at , wspd_at , rhum_at , slp_at , vwnd_at , cldc_at ],[ 'sst_at' , 'wspd_at' , 'rhum_at' , 'slp_at' , 'vwnd_at' , 'cldc_at' ]) # missing_pac = get_missing([sst_pac,wspd_pac,rhum_pac],['sst_pac','wspd_pac','rhum_pac']) # fig,(ax,ax1) = plt.subplots(2,1,figsize=(10,8)) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Dados Faltantes - Atl\u00e2ntico Norte MDR*\" , fontsize = 15 ) ax . set_xlabel ( \"Ano\" , fontsize = 12 ) ax . set_ylabel ( \" % d e dados faltantes\" , fontsize = 12 ) for key , value in missing_at . items (): ax . plot ( period , missing_at [ key ], label = key [ 0 : - 3 ]) plt . axvline ( x = period [ 1860 ], label = \"Jan/1955\" , color = 'black' , lw = 3 , ls = '--' ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( '../figs/missing_mdr.png' ) *MDR \u00e9 a abrevia\u00e7\u00e3o de Main Development Region ou regi\u00e3o central de desenvolvimento dos furac\u00f5es no Atl\u00e2ntico Norte e se refere \u00e0 faixa 10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W. Vemos que a partir de 1950-1960, os dados come\u00e7am a ficar mais completos na regi\u00e3o de estudos. Ent\u00e3o, para entender a rela\u00e7\u00e3o as vari\u00e1veis, iremos trabalhar a partir desta data. Entretanto, nada nos impede de usar os dados mais antigos, j\u00e1 que as medi\u00e7\u00f5es n\u00e3o variam muito quando est\u00e3o perto, se quisermos trabalhar com tend\u00eancias de longo prazo podemos cortar os dados a partir de 1920, trabalhar com a m\u00e9dia das regi\u00f5es estudadas, mesmo que com ~70\\% de dados faltantes. Isso pois temos a array indicadora, que pode ajudar em modelos, e tamb\u00e9m essa porcentagem \u00e9 um pouco mais baixa devido \u00e0s faixas continentais considaradas no corte de coordenadas. Abaixo temos um exemplo de como os dados de temperatura est\u00e3o distribu\u00eddos em Janeiro de 1955","title":"An\u00e1lise de dados faltantes"},{"location":"Data_Cleaning_and_EDA/#visualizacao","text":"#Transforms longitude ranges from [0,360] para [-180,180] --> useful for plot sst [:], lonsn = shiftgrid ( 180 , sst [:], lons , start = False ) wspd [:], lonsn = shiftgrid ( 180 , wspd [:], lons , start = False ) # shum[:],lonsn = shiftgrid(180,shum[:],lons,start=False) rhum [:], lonsn = shiftgrid ( 180 , rhum [:], lons , start = False ) lons = lonsn #Reference: https://annefou.github.io/metos_python/04-plotting/ time_index = 1860 fig = plt . figure ( figsize = [ 12 , 15 ]) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . set_title ( \"sst: {} \" . format ( period [ time_index ] . date ()), fontsize = 16 ) map = Basemap ( projection = 'cyl' , llcrnrlat =- 90 , urcrnrlat = 90 , \\ llcrnrlon =- 180 , urcrnrlon = 180 , resolution = 'c' , ax = ax ) map . drawcoastlines () map . fillcontinents ( color = '#ffe2ab' ) map . drawparallels ( np . arange ( - 90. , 120. , 30. ), labels = [ 1 , 0 , 0 , 0 ]) map . drawmeridians ( np . arange ( - 180. , 180. , 60. ), labels = [ 0 , 0 , 0 , 1 ]) llons , llats = np . meshgrid ( lons , lats ) x , y = map ( llons , llats ) cmap = c . ListedColormap ([ '#35978f' , '#ffffcc' , '#ffeda0' , '#fed976' , '#feb24c' , '#fd8d3c' , '#fc4e2a' , '#e31a1c' , '#bd0026' , '#800026' ]) bounds = list ( np . arange ( - 5 , 37 , 1 )) # bounds=list(np.arange(10,100,5)) norm = c . BoundaryNorm ( bounds , ncolors = cmap . N ) cs = map . contourf ( x , y , sst [ time_index ], cmap = cmap , norm = norm , levels = bounds ) fig . colorbar ( cs , cmap = cmap , norm = norm , boundaries = bounds , ticks = bounds , ax = ax , orientation = 'horizontal' ); plt . savefig ( '../figs/sst_1955.png' )","title":"Visualiza\u00e7\u00e3o"},{"location":"Data_Cleaning_and_EDA/#agregacao-dos-dados-mdr","text":"Criaremos a seguir uma dataframe com as m\u00e9dias espaciais da regi\u00e3o MDR, para an\u00e1lise futura com o PDI. Usamos essa m\u00e9dia baseados na premissa razo\u00e1vel de que nesse corte espacial da MDR do atl\u00e2ntico os valores n\u00e3o variam muito dentro de um m\u00eas. Fazemos essa m\u00e9dias para an\u00e1lises de mais longo prazo como podem ver no notebook do PDI . print ( period [ 1860 ]) # -- Jan/1955 def get_mean ( data ): size = data . shape [ 0 ] new = np . array ([]) for i in range ( size ): new = np . append ( new , np . mean ( data [ i ,:,:])) return new #Come\u00e7aremos do \u00edndice 1788, representando Janeiro de 1949, para corresponder com os dados de PDI. data_at = pd . DataFrame ( get_mean ( sst_at [ 1788 :,:,:]), columns = [ \"sst\" ]) period_df = pd . DataFrame ( period [ 1788 :], columns = [ \"Date\" ]) period_df [ 'Year' ] = period_df . Date . map ( lambda x : x . year ) period_df [ 'Month' ] = period_df . Date . map ( lambda x : x . month ) data_at [ 'rhum' ] = pd . DataFrame ( get_mean ( rhum_at [ 1788 :,:,:]), columns = [ \"rhum\" ]) data_at [ 'slp' ] = pd . DataFrame ( get_mean ( slp_at [ 1788 :,:,:]), columns = [ \"slp\" ]) data_at [ 'wspd' ] = pd . DataFrame ( get_mean ( wspd_at [ 1788 :,:,:]), columns = [ \"wspd\" ]) data_at [ 'vwnd' ] = pd . DataFrame ( get_mean ( vwnd_at [ 1788 :,:,:]), columns = [ \"vwnd\" ]) data_at [ 'cldc' ] = pd . DataFrame ( get_mean ( cldc_at [ 1788 :,:,:]), columns = [ \"cldc\" ]) atlantic_mdr = pd . concat ([ period_df , data_at ], axis = 1 ) #C\u00f3digo que calcula desvios da temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica #Apenas para visualiza\u00e7\u00e3o cum_sum = {} for i in range ( 1 , 13 ): cum_sum [ i ] = 0 k = 0 #year count for i in range ( 0 , atlantic_mdr . shape [ 0 ] - 12 ): month = atlantic_mdr . iloc [ i ,:] . Month if month % 12 == 1 : k += 1 cum_sum [ month ] += atlantic_mdr . iloc [ i , 3 ] atlantic_mdr . loc [ atlantic_mdr . index [ i ], 'sst_anomaly' ] = atlantic_mdr . iloc [ i , 3 ] - cum_sum [ month ] / k atlantic_mdr . drop ( 'sst_anomaly' , axis = 1 ) . to_csv ( '../Datasets/atlantic_mdr.csv' , index = False ) atlantic_mdr . iloc [ 12 : 24 ,:] 1955-01-01 00:00:00 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Year Month sst rhum slp wspd vwnd cldc sst_anomaly 12 1950-01-01 1950 1 24.782794 75.675756 1012.699219 7.800527 -2.709708 4.139401 -0.138282 13 1950-02-01 1950 2 24.251776 78.633936 1012.883875 6.583213 -2.401613 3.965875 -0.129778 14 1950-03-01 1950 3 24.395219 78.334148 1013.134516 6.621354 -2.062759 4.087092 -0.020090 15 1950-04-01 1950 4 24.900423 77.248673 1011.096354 5.958877 -0.454326 3.146995 -0.010952 16 1950-05-01 1950 5 25.355377 79.638362 1010.577563 6.486837 0.516981 4.291411 -0.061999 17 1950-06-01 1950 6 26.110960 81.022388 1009.393293 6.946026 2.076482 4.650387 0.087756 18 1950-07-01 1950 7 26.516357 80.354432 1007.978285 6.342278 2.375750 5.184904 -0.070533 19 1950-08-01 1950 8 27.312561 81.914887 1007.856950 5.279884 1.956808 5.109233 -0.081636 20 1950-09-01 1950 9 27.811223 80.212305 1007.869104 4.947030 0.525868 5.305648 -0.057075 21 1950-10-01 1950 10 27.702060 80.523185 1010.697798 4.656318 -1.037250 4.423469 -0.026328 22 1950-11-01 1950 11 27.303333 81.614127 1009.745018 5.103683 -2.024746 3.765682 0.058410 23 1950-12-01 1950 12 26.351468 77.828336 1012.071718 5.696333 -4.750430 4.100716 0.117639 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 5 )) ax . plot ( np . arange ( 1949 , 2021 , 1 ), atlantic_mdr . groupby ([ 'Year' ]) . agg ({ 'sst_anomaly' : np . mean })[ 'sst_anomaly' ]) ax . set_title ( \"Desvios na temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica\" , fontsize = 14 ) ax . set_xlabel ( \"Ano\" , fontsize = 12 ) ax . set_ylabel ( \"Desvio da temperatura\" , fontsize = 12 ); Podemos ver que ap\u00f3s 1970-1980 inicia-se uma tend\u00eancia crescente de aumento de temperatura do mar em rela\u00e7\u00e3o \u00e0 m\u00e9dia hist\u00f3rica. corr = atlantic_mdr . corr () corr . style . background_gradient ( cmap = 'coolwarm' ) #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col0 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col1 { background-color: #9dbdff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col2 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col3 { background-color: #b9d0f9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col4 { background-color: #d6dce4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col5 { background-color: #f4987a; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col6 { background-color: #c9d7f0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col7 { background-color: #dfdbd9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow0_col8 { background-color: #bcd2f7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col0 { background-color: #536edd; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col1 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col2 { background-color: #e36b54; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col3 { background-color: #f7ba9f; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col4 { background-color: #7ea1fa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col5 { background-color: #4055c8; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col6 { background-color: #e5d8d1; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col7 { background-color: #f2cbb7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow1_col8 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col0 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col1 { background-color: #e7745b; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col2 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col3 { background-color: #ee8669; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col4 { background-color: #5977e3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col5 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col6 { background-color: #f7b194; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col7 { background-color: #f7a688; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow2_col8 { background-color: #a7c5fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col0 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col1 { background-color: #eed0c0; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col2 { background-color: #f18f71; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col3 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col4 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col5 { background-color: #4257c9; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col6 { background-color: #d85646; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col7 { background-color: #ead5c9; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow3_col8 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col0 { background-color: #6485ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col1 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col2 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col3 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col4 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col5 { background-color: #e1dad6; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col6 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col7 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow4_col8 { background-color: #6788ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col0 { background-color: #f7b497; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col1 { background-color: #445acc; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col2 { background-color: #6384eb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col3 { background-color: #82a6fb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col4 { background-color: #f4c5ad; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col5 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col6 { background-color: #a1c0ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col7 { background-color: #b2ccfb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow5_col8 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col0 { background-color: #4b64d5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col1 { background-color: #c3d5f4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col2 { background-color: #f6bda2; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col3 { background-color: #d95847; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col4 { background-color: #3b4cc0; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col5 { background-color: #5e7de7; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col6 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col7 { background-color: #dedcdb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow6_col8 { background-color: #3e51c5; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col0 { background-color: #a2c1ff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col1 { background-color: #edd2c3; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col2 { background-color: #f6a283; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col3 { background-color: #f2c9b4; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col4 { background-color: #688aef; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col5 { background-color: #9ebeff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col6 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col7 { background-color: #b40426; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow7_col8 { background-color: #455cce; color: #f1f1f1; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col0 { background-color: #cbd8ee; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col1 { background-color: #9bbcff; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col2 { background-color: #efcfbf; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col3 { background-color: #cdd9ec; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col4 { background-color: #e2dad5; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col5 { background-color: #a6c4fe; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col6 { background-color: #cedaeb; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col7 { background-color: #b5cdfa; color: #000000; } #T_f64d8f40_eb28_11ea_88a4_cdafc426433brow8_col8 { background-color: #b40426; color: #f1f1f1; } Year Month sst rhum slp wspd vwnd cldc sst_anomaly Year 1.000000 -0.010203 0.145686 -0.098235 0.048908 0.646453 -0.035567 0.241832 0.379038 Month -0.010203 1.000000 0.762722 0.392842 -0.434216 -0.385048 0.151316 0.380311 -0.017207 sst 0.145686 0.762722 1.000000 0.635370 -0.632031 -0.416486 0.432498 0.559898 0.312656 rhum -0.098235 0.392842 0.635370 1.000000 -0.768182 -0.380635 0.803748 0.305508 0.014696 slp 0.048908 -0.434216 -0.632031 -0.768182 1.000000 0.317397 -0.812037 -0.548567 0.129542 wspd 0.646453 -0.385048 -0.416486 -0.380635 0.317397 1.000000 -0.253309 0.007404 0.040588 vwnd -0.035567 0.151316 0.432498 0.803748 -0.812037 -0.253309 1.000000 0.236100 -0.002599 cldc 0.241832 0.380311 0.559898 0.305508 -0.548567 0.007404 0.236100 1.000000 0.019122 sst_anomaly 0.379038 -0.017207 0.312656 0.014696 0.129542 0.040588 -0.002599 0.019122 1.000000 Alguma correla\u00e7\u00f5es fortes interessantes: Temperatura do mar (sst) com ano (Year) Temperatura do mar (sst) e umidade (rhum) Velocidade de vento (wspd) e ano (Year) Press\u00e3o (slp) e Umidade (rhum) An\u00e1lises mais aprofundadas dessas vari\u00e1veis veremos no notebook de an\u00e1lise do PDI. month_sst = atlantic_mdr . groupby ( 'Month' )[ 'sst' ] . mean () month_rhum = atlantic_mdr . groupby ( 'Month' )[ 'rhum' ] . mean () # month_slp = atlantic_mdr.groupby('Month')['slp'].mean() # month_cldc = atlantic_mdr.groupby('Month')['cldc'].mean() m = np . arange ( 1 , 13 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 9 , 5 )) ax . plot ( month_sst , lw = 3.5 , color = 'blue' , label = 'sst' ) ax . set_xticks ( m ); ax . set_xlabel ( \"M\u00eas\" , fontsize = 14 ) ax . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - SST\" , fontsize = 14 ) ax2 = ax . twinx () ax2 . plot ( month_rhum , lw = 3.5 , color = 'orange' , label = 'rhum' ) ax2 . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - Umidade\" , fontsize = 14 ) ax . legend ( loc = 'best' , fontsize = 14 ); ax2 . legend ( loc = 'upper left' , fontsize = 14 ); plt . savefig ( \"../figs/mensal_sst_rhum.jpg\" ) Veja acima que os picos de temperatura e umidade, coincidem razoavelmente com a temporada de furac\u00f5es. month_slp = atlantic_mdr . groupby ( 'Month' )[ 'slp' ] . mean () month_cldc = atlantic_mdr . groupby ( 'Month' )[ 'cldc' ] . mean () m = np . arange ( 1 , 13 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 9 , 5 )) ax . plot ( month_slp , lw = 3.5 , color = 'blue' , label = 'slp' ) ax . set_xticks ( m ); ax . set_xlabel ( \"M\u00eas\" , fontsize = 14 ) ax . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - Press\u00e3o\" , fontsize = 14 ) ax2 = ax . twinx () ax2 . plot ( month_cldc , lw = 3.5 , color = 'orange' , label = 'cldc' ) ax2 . set_ylabel ( \"M\u00e9dia Hist\u00f3rica - Nebulosidade\" , fontsize = 14 ) ax . legend ( loc = 'best' , fontsize = 14 ); ax2 . legend ( loc = 'lower left' , fontsize = 14 ); plt . savefig ( \"../figs/mensal_slp_cldc.jpg\" ) O pico de nebulosidade das nuvens tamb\u00e9m coincide de forma razo\u00e1vel com a temporada de furac\u00f5es. Assim como os valores mais baixos de press\u00e3o. Essas caracter\u00edsticas est\u00e3o relacionadas com a forma\u00e7\u00e3o do evento de tempestade forte ou furac\u00e3o.","title":"Agrega\u00e7\u00e3o dos Dados MDR"},{"location":"Data_Cleaning_and_EDA/#dataset-hurdat2-hurricane-analise-e-limpeza","text":"Passemos agora a analisar os dados de tempestades e furac\u00f5es. # O foco principal do trabalho se dar\u00e1 nos dados do atl\u00e2ntico data_atl = pd . read_csv ( '../Datasets/atlantic.csv' , parse_dates = [ 'Date' ]) data_atl . dtypes ID object Name object Date datetime64 [ ns ] Time int64 Event object Status object Latitude object Longitude object Maximum Wind int64 Minimum Pressure int64 Low Wind NE int64 Low Wind SE int64 Low Wind SW int64 Low Wind NW int64 Moderate Wind NE int64 Moderate Wind SE int64 Moderate Wind SW int64 Moderate Wind NW int64 High Wind NE int64 High Wind SE int64 High Wind SW int64 High Wind NW int64 dtype : object # formatando dados de data data_atl [ 'Year' ] = pd . DatetimeIndex ( data_atl [ 'Date' ]) . year data_atl [ 'Month' ] = pd . DatetimeIndex ( data_atl [ 'Date' ]) . month data_atl [ 'Day' ] = pd . DatetimeIndex ( data_atl [ 'Date' ]) . day print ( data_atl . columns ) data_atl . head () Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Low Wind NE', 'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE', 'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW', 'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW', 'Year', 'Month', 'Day'], dtype='object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind SE Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW Year Month Day 0 AL011851 UNNAMED 1851-06-25 0 HU 28.0N 94.8W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 1 AL011851 UNNAMED 1851-06-25 600 HU 28.0N 95.4W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 2 AL011851 UNNAMED 1851-06-25 1200 HU 28.0N 96.0W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 3 AL011851 UNNAMED 1851-06-25 1800 HU 28.1N 96.5W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 4 AL011851 UNNAMED 1851-06-25 2100 L HU 28.2N 96.8W 80 -999 ... -999 -999 -999 -999 -999 -999 -999 1851 6 25 5 rows \u00d7 25 columns # Registro de Furac\u00f5es \u00e9 maior em determinada \u00e9poca do ano print ( data_atl . groupby ([ 'Month' ])[ 'ID' ] . count ()) Month 1 132 2 13 3 14 4 81 5 655 6 2349 7 3262 8 10857 9 18926 10 9802 11 2548 12 466 Name: ID, dtype: int64 fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) fig . suptitle ( 'N\u00famero de fura\u00e7\u00f5es registrados por m\u00eas do ano' , fontsize = 28 , y = 1.06 ) ax . bar ( data_atl . groupby ([ 'Month' ])[ 'Month' ] . mean (), data_atl . groupby ([ 'Month' ])[ 'ID' ] . count (), ls = '--' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de registros (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$M\u00eas$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) # ax.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( \"../figs/furacoes_mes.jpg\" ) Per\u00edodo de furac\u00f5es se concentra entre os meses de julho e novembro. Isso deve estar relacionado com o per\u00edodo anterior (ver\u00e3o do hemisf\u00e9rio norte, onde acontece o maior n\u00famero de casos) O aquecimento das \u00e1guas est\u00e1 intimamente ligado com a forma\u00e7\u00e3o das massas de ar que ocasionam os furac\u00f5es Isso nos d\u00e1 uma pista da forma de correla\u00e7\u00e3o temporal que devemos buscar para predizer os eventos ## Formata\u00e7\u00e3o dos dados para plot pressao x vento data_atl_ext = data_atl . copy () data_atl_mwmp = data_atl . copy () data_atl_mw = data_atl . copy () ind_nan_ext = [] ind_nan_mwmp = [] ind_nan_mw = [] for l in range ( len ( data_atl )): if ( data_atl_mw [ 'Maximum Wind' ][ l ] < 0 ): ind_nan_mw . append ( l ) ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( data_atl_mwmp [ 'Minimum Pressure' ][ l ] < 0 ): ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( min ( data_atl_ext [ 'Low Wind NE' ][ l ], data_atl_ext [ 'Low Wind SE' ][ l ], data_atl_ext [ 'Low Wind SW' ][ l ], data_atl_ext [ 'Low Wind NW' ][ l ], data_atl_ext [ 'Moderate Wind NE' ][ l ], data_atl_ext [ 'Moderate Wind SE' ][ l ], data_atl_ext [ 'Moderate Wind SW' ][ l ], data_atl_ext [ 'Moderate Wind NW' ][ l ], data_atl_ext [ 'High Wind NE' ][ l ], data_atl_ext [ 'High Wind SE' ][ l ], data_atl_ext [ 'High Wind SW' ][ l ], data_atl_ext [ 'High Wind NW' ][ l ]) < 0 ): ind_nan_ext . append ( l ) data_atl_ext = data_atl_ext . drop ( ind_nan_ext , 0 ) data_atl_mwmp = data_atl_mwmp . drop ( ind_nan_mwmp , 0 ) data_atl_mw = data_atl_mw . drop ( ind_nan_mw , 0 ) print ( len ( data_atl_ext )) print ( len ( data_atl_mwmp )) print ( len ( data_atl_mw )) --------------------------------------------------------------------------- NameError Traceback ( most recent call last ) < ipython - input - 3 - 754270 ff204f > in < module > 1 ## Formata\u00e7\u00e3o dos dados para plot pressao x vento ----> 2 data_atl_ext = data_atl.copy() 3 data_atl_mwmp = data_atl . copy () 4 data_atl_mw = data_atl . copy () 5 NameError : name 'data_atl' is not defined fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . scatter ( data_atl_mwmp [ 'Minimum Pressure' ], data_atl_mwmp [ 'Maximum Wind' ], alpha = 0.2 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Press\u00e3o M\u00ednima$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) # ax.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/pressaoMin_Velo_max.jpg' ) Abaixo formatamos as Latitudes e Longitudes para remover os terminadores W, E, N e S. Como indicador de hemisf\u00e9rio, usamos sinal negativo para sul e oeste e positovo para norte e leste. Precisaremos que esses dados sejam numericos para aplica\u00e7\u00f5es futuras e n\u00e3o textuais. data_atl [[ 'Latitude' , 'Longitude' ]] . dtypes Latitude object Longitude object dtype: object data_atl . Latitude = data_atl . Latitude . apply ( lambda x : - float ( x . rstrip ( \"S\" )) if x . endswith ( \"S\" ) else float ( x . rstrip ( \"N\" ))) data_atl . Longitude = data_atl . Longitude . apply ( lambda x : - float ( x . rstrip ( \"W\" )) if x . endswith ( \"W\" ) else float ( x . rstrip ( \"E\" ))) data_atl [[ 'Latitude' , 'Longitude' ]] . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Latitude Longitude count 49105.000000 49105.000000 mean 27.044904 -65.682533 std 10.077880 19.687240 min 7.200000 -359.100000 25% 19.100000 -81.000000 50% 26.400000 -68.000000 75% 33.100000 -52.500000 max 81.000000 63.000000 X_train = data_atl . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) ax . plot ( data_atl . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) # ax.legend(loc='best', fontsize=12); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/numero_furacoes1851-2015.jpg' ) R^2_train = 0.40918060978409 Par\u00e2metro_const = -3940.349139351199 Par\u00e2metro_Year = 2.192423797184304 Vemos acima que o n\u00famero de registro de fura\u00e7\u00f5es tem crescido desde 1850, mas isso se deve \u00e0 maior capacidade de detec\u00e7\u00e3o com o passar dos anos. X_train = data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) ax . plot ( data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/maxwind_1851-2015.jpg' ) R^2_train = 0.49772308255924613 Par\u00e2metro_const = 345.3529584491673 Par\u00e2metro_Year = -0.15025433346159078 Vemos tamb\u00e9m que a velocidade m\u00e1xima de vento sustentada pelos furac\u00f5es reduziu, mas isso tamb\u00e9m se deve \u00e0 maior capacidade de registro de eventos de pequeno porte, que acabam pesando a m\u00e9dia para baixo. Assim, para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte; Consideramos apenas tempestades cuja dura\u00e7\u00e3o em dias \u00e9 maior que 2, e cuja classifica\u00e7\u00e3o na escala Saffir-Simpson seja no m\u00ednimo Tempestade Tropical. Para essa classifica\u00e7\u00e3o, a velocidade m\u00e1xima sustentada de vento deve ultrapassar 63km/h o que equivale a 34 knots (milhas n\u00e1uticas). #Filtro de Dura\u00e7\u00e3o data_atl_fdur = data_atl_mw . copy () duration = data_atl_mw . groupby ([ 'ID' ])[ 'Date' ] . max () - data_atl_mw . groupby ([ 'ID' ])[ 'Date' ] . min () duration . name = 'Duration' #print(duration) data_atl_fdur = pd . merge ( data_atl_fdur , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_fdur [ 'Duration' ] = pd . to_numeric ( data_atl_fdur [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_fdur = data_atl_fdur [ data_atl_fdur [ 'Duration' ] > 2 ] print ( len ( data_atl_fdur )) 46350 #Filtro de Max_Windspeed data_atl_fwind = data_atl_fdur . copy () data_atl_fwind = data_atl_fwind [ data_atl_fwind [ 'Maximum Wind' ] > 34 ] print ( len ( data_atl_fwind )) 35696 Vejamos os novos plots com os dados filtrados: # Com o novo filtro, o vi\u00e9s do aumento no n\u00famero de furac\u00f5es ao longo dos anos reduziu, mas ainda h\u00e1 um aumento # Isso mostra que essa tend\u00eancia pode ser algo n\u00e3o viesada, e que gera preocupa\u00e7\u00e3o pelo futuro X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/filtered_numero_furacoes1851-2015.jpg' ) R^2_train = 0.15573477903937427 Par\u00e2metro_const = -1661.4327723310091 Par\u00e2metro_Year = 0.9714289530628057 X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) plt . savefig ( '../figs/filtered_maxwind_1851-2015.jpg' ) R^2_train = 0.11158706746149893 Par\u00e2metro_const = 167.44362825887208 Par\u00e2metro_Year = -0.05470982174808241 Com o novo filtro, o vi\u00e9s da redu\u00e7\u00e3o da velocidade m\u00e1xima sustentada de vento reduziu, quase para o n\u00edvel constante Isso pode significar que os filtros est\u00e3o relativamente bem adequados para retirada do vi\u00e9s inicial dos dados. Geraremos ent\u00e3o um novo DataFrame com algums filtros importantes: Velocidade M\u00e1xima Sustentada > 34 milhas n\u00e1uticas Dura\u00e7\u00e3o > 2 dias Furac\u00f5es a partir de 1950 (quando a capacidade de medi\u00e7\u00e3o come\u00e7a a evoluir O c\u00f3digo abaixo aplica esses filtros. data_atl_mw2 = data_atl . copy () data_atl_mw2_filtrado3 = data_atl_mw2 . copy () Lat_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Latitude' ] . first () Lat_min . name = 'Lat_min' data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lat_min , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ abs ( data_atl_mw2_filtrado3 [ 'Lat_min' ] - 12.5 ) > 0 ] Lon_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Longitude' ] . min () Lon_min . name = 'Lon_min' data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lon_min , how = 'inner' , left_on = 'ID' , right_index = True ) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Lon_min' ] > - 180 ] Wind_max = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Maximum Wind' ] . max () Wind_max . name = 'Wind_max' #print(Wind_max) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Wind_max , how = 'inner' , on = 'ID' ) #left_on='ID', right_index=True) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Wind_max' ] > 34 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Year' ] > 1950 ] duration = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date' ] . max () - data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date' ] . min () duration . name = 'Duration' #print(duration) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_mw2_filtrado3 [ 'Duration' ] = pd . to_numeric ( data_atl_mw2_filtrado3 [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Duration' ] > 2 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 . drop ([ 'Lat_min' , 'Lon_min' , 'Wind_max' ], 1 ) #data_atl_mw2_filtrado3.head() print ( len ( data_atl_mw2_filtrado3 )) print ( len ( data_atl . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl )) print ( len ( data_atl_mw2 )) data_atl_mw2_filtrado3 . head () 22386 1814 685 49105 49105 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW Year Month Day Duration 21948 AL011951 UNNAMED 1951-01-02 1200 EX 30.5 -58.0 50 -999 ... -999 -999 -999 -999 -999 -999 1951 1 2 10 21949 AL011951 UNNAMED 1951-01-02 1800 EX 29.9 -56.8 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 2 10 21950 AL011951 UNNAMED 1951-01-03 0 EX 29.0 -55.7 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 3 10 21951 AL011951 UNNAMED 1951-01-03 600 EX 27.5 -54.8 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 3 10 21952 AL011951 UNNAMED 1951-01-03 1200 EX 26.5 -54.5 45 -999 ... -999 -999 -999 -999 -999 -999 1951 1 3 10 5 rows \u00d7 26 columns Uma limpeza final ser\u00e1 feita nas entradas de texto: df = data_atl_mw2_filtrado3 . copy () np . unique ( df . Name )[ 0 : 5 ], np . unique ( df . Event ), np . unique ( df . Status ) (array([' AMY', ' ANA', ' BOB', ' DOG', ' DON'], dtype=object), array([' ', ' C', ' G', ' I', ' L', ' P', ' R', ' S', ' T', ' W'], dtype=object), array([' DB', ' EX', ' HU', ' LO', ' SD', ' SS', ' TD', ' TS', ' WV'], dtype=object)) Veja que as entradas est\u00e3o espa\u00e7adas, abaixo corrigimos isso: df [ 'Name' ] = df [ 'Name' ] . apply ( lambda x : x . strip ()) df [ 'Event' ] = df [ 'Event' ] . apply ( lambda x : x . strip ()) df [ 'Status' ] = df [ 'Status' ] . apply ( lambda x : x . strip ()) np . unique ( df . Name )[ 0 : 5 ], np . unique ( df . Event ), np . unique ( df . Status ) (array(['ABBY', 'ABLE', 'AGNES', 'ALBERTO', 'ALEX'], dtype=object), array(['', 'C', 'G', 'I', 'L', 'P', 'R', 'S', 'T', 'W'], dtype=object), array(['DB', 'EX', 'HU', 'LO', 'SD', 'SS', 'TD', 'TS', 'WV'], dtype=object)) #Salvando em csv data_atl_mw2_filtrado3 = df . copy () data_atl_mw2_filtrado3 . to_csv ( '../Datasets/data_atl_mw2_filtrado3.csv' , encoding = 'utf-8' , index = False )","title":"Dataset HURDAT2 (Hurricane) - An\u00e1lise e Limpeza"},{"location":"Data_Cleaning_and_EDA/#uniao-dos-dados-via-k-nn-ponderado","text":"Abaixo vamos gerar um novo dataframe que pegar\u00e1 o filtrado3.csv gerado acima e buscar\u00e1 os dados clim\u00e1ticos nos datasets da ICOADS diretamente dos arquivos .nc. Faremos essa busca via coordenadas e para lidar com dados faltantes implementaremos um k-NN ponderado pelo inverso das dist\u00e2ncias entre as coordenadas originais e o vizinho considerado no algoritmo.","title":"Uni\u00e3o dos dados via k-NN ponderado"},{"location":"Data_Cleaning_and_EDA/#leitura-dos-dados-e-limpezas-adicionais","text":"df = pd . read_csv ( '../Datasets/data_atl_mw2_filtrado3.csv' , parse_dates = [ 'Date' ]) df . dtypes ID object Name object Date datetime64 [ ns ] Time int64 Event object Status object Latitude float64 Longitude float64 Maximum Wind int64 Minimum Pressure int64 Low Wind NE int64 Low Wind SE int64 Low Wind SW int64 Low Wind NW int64 Moderate Wind NE int64 Moderate Wind SE int64 Moderate Wind SW int64 Moderate Wind NW int64 High Wind NE int64 High Wind SE int64 High Wind SW int64 High Wind NW int64 Year int64 Month int64 Day int64 Duration int64 dtype : object pd . DataFrame ( zip ( df . columns , [ np . sum ( df [ x ] == - 999 ) / len ( df ) for x in df . columns ]) , columns = [ 'Variable' , 'Missing Ratio' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Variable Missing Ratio 0 ID 0.000000 1 Name 0.000000 2 Date 0.000000 3 Time 0.000000 4 Event 0.000000 5 Status 0.000000 6 Latitude 0.000000 7 Longitude 0.000000 8 Maximum Wind 0.000000 9 Minimum Pressure 0.256723 10 Low Wind NE 0.750201 11 Low Wind SE 0.750201 12 Low Wind SW 0.750201 13 Low Wind NW 0.750201 14 Moderate Wind NE 0.750201 15 Moderate Wind SE 0.750201 16 Moderate Wind SW 0.750201 17 Moderate Wind NW 0.750201 18 High Wind NE 0.750201 19 High Wind SE 0.750201 20 High Wind SW 0.750201 21 High Wind NW 0.750201 22 Year 0.000000 23 Month 0.000000 24 Day 0.000000 25 Duration 0.000000 Removeremos as colunas abaixo, pois n\u00e3o utilizaremos nas an\u00e1lises e s\u00e3o muitos esparsas df = df . drop ([ 'Low Wind NE' , 'Low Wind SE' , 'Low Wind SW' , 'Low Wind NW' , 'Moderate Wind NE' , 'Moderate Wind SE' , 'Moderate Wind SW' , 'Moderate Wind NW' , 'High Wind NE' , 'High Wind SE' , 'High Wind SW' , 'High Wind NW' ], axis = 1 ) Na c\u00e9lula abaixo, lemos os dados clim\u00e1ticos e os salvamos en vari\u00e1veis para uso futuro. sst_mean = nc . Dataset ( 'sst.mean.nc' , 'r' ) rhum_mean = nc . Dataset ( 'rhum.mean.nc' , 'r' ) wspd_mean = nc . Dataset ( 'wspd.mean.nc' , 'r' ) slp_mean = nc . Dataset ( 'slp.mean.nc' , 'r' ) # vwnd_mean = nc.Dataset('Datasets/vwnd.mean.nc','r') # Resolvemos n\u00e3o considerar o vwnd por ser ru\u00eddo cldc_mean = nc . Dataset ( \"cldc.mean.nc\" , 'r' ) lats = sst_mean . variables [ 'lat' ][:] lons = sst_mean . variables [ 'lon' ][:] time = sst_mean . variables [ 'time' ][:] lons = [ x - 180 for x in lons ] lats = [ x for x in lats ] sst = sst_mean . variables [ 'sst' ][:,:,:] rhum = rhum_mean . variables [ 'rhum' ][:,:,:] wspd = wspd_mean . variables [ 'wspd' ][:,:,:] slp = slp_mean . variables [ 'slp' ][:,:,:] cldc = cldc_mean . variables [ 'cldc' ][:,:,:] sst_mean . close () wspd_mean . close () rhum_mean . close () slp_mean . close () cldc_mean . close ()","title":"Leitura dos dados e limpezas adicionais"},{"location":"Data_Cleaning_and_EDA/#formula-da-distancia","text":"Dados duas coordenadas (\\varphi_1,\\lambda_1) e ( \\varphi_2,\\lambda_2) em radianos, a F\u00f3rmula Haversine \u00e9 capaz de calcular a dist\u00e2ncia real entre esses dois pontos no mapa: Onde r \u00e9 o raio da Terra. Usando r aproximadamente igual a 6371 km o valor de d ser\u00e1 a dist\u00e2ncia em km dos dois pontos dados em coordenadas geogr\u00e1ficas. Abaixo, seguimos uma implementa\u00e7\u00e3o equivalente obtida no Stack Overflow . def distance ( lat1 , lon1 , lat2 , lon2 ): # approximate radius of earth in km R = 6371.0 lat1 = radians ( lat1 ) lon1 = radians ( lon1 ) lat2 = radians ( lat2 ) lon2 = radians ( lon2 ) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) if R * c == 0 : return 0.000000001 #Handling Zero Division problems else : return R * c","title":"Formula da dist\u00e2ncia"},{"location":"Data_Cleaning_and_EDA/#k-nn-ponderado-por-inverso-da-distancia","text":"Abaixo segue a implementa\u00e7\u00e3o de algumas fun\u00e7\u00f5es auxiliares para realizar esse preenchimento de dados almejado. o que queremos \u00e9 dos dados que filtramos do HURDAT, para cada linha, usando as coordenadas geogr\u00e1ficas, gerar novas colunas contendo os dados do ICOADS para aquela data espef\u00edcica e aquela coordenada espec\u00edfica. Por conta de dados faltantes no ICOADS, nem sempre existir\u00e1 registros para todas as coordenadas. Assim, buscamos os k vizinhos mais pr\u00f3ximos a calculamos a m\u00e9dia entre eles, por\u00e9m a m\u00e9dia \u00e9 ponderada pelo inverso da dist\u00e2ncia entre o vizinho e a coordenada original (usando a F\u00f3rmula Haversive). Sendo assim, pontos mais distantes ter\u00e3o menor peso na m\u00e9dia, enquanto pontos mais pr\u00f3ximos ser\u00e3o considerados mais importantes. Por simplicidade, usamos k=15 . Funcionamento das fun\u00e7\u00f5es: get_coords: recebe uma coordenada e a lista de coordenadas dispon\u00edveis no ICOADS. Retorna uma coordenada que existe na lista passada. Precisamos dessa fun\u00e7\u00e3o pois os dados de coordenadas do ICOADS s\u00e3o intervalados 2 a 2. get_neighboors: recebe uma data, latitude, longitude, uma array de dados do ICOADS e a quantidade de vizinhos a buscar. Retorna uma lista de k vizinhos e uma lista de dist\u00e2ncias em kilometros. get_data: usa as fun\u00e7\u00f5es acima para gerar a m\u00e9dia ponderada pelo inverso da dist\u00e2ncia get_data_df: aplica as fun\u00e7\u00f5es acima para gerar uma nova coluna nos dados HURDAT2 da forma como almejado inicialmente. def get_coord ( coord , l ): if int ( coord ) in l : return int ( coord ) elif int ( coord ) + 1 in l : return int ( coord ) + 1 elif int ( coord ) - 1 in l : return int ( coord ) - 1 elif int ( coord ) + 2 in l : return int ( coord ) + 2 elif int ( coord ) - 2 in l : return int ( coord ) - 2 def get_neighboors ( t , lat , lon , marray , k ): nb = [] dist = [] lat_data = get_coord ( lat , lats ) #nearest lat in our lats list lon_data = get_coord ( lon , lons ) lat_i = lats . index ( lat_data ) lon_i = lons . index ( lon_data ) if marray [ t , lat_i , lon_i ]: nb . append ( marray [ t , lat_i , lon_i ]) dist . append ( distance ( lat , lon , lat_data , lon_data )) j = 1 while len ( nb ) < k : lower_i = ( lat_i - j ) #%90 upper_i = ( lat_i + j ) #%90 right_i = ( lon_i + j ) #%180 left_i = ( lon_i - j ) #%180 # if right_i>=len() left_values = marray [ t , lower_i : upper_i , left_i ] upper_values = marray [ t , upper_i , left_i : right_i ] right_values = marray [ t , upper_i : lower_i : - 1 , right_i ] lower_values = marray [ t , lower_i , right_i : left_i : - 1 ] [ nb . append ( x ) for x in left_values if x ] [ dist . append ( distance ( lat , lon , lats [ i ], lons [ left_i ])) for i in range ( len ( left_values )) if left_values [ i ]] if len ( nb ) >= k : break [ nb . append ( x ) for x in upper_values if x ] [ dist . append ( distance ( lat , lon , lats [ upper_i ], lons [ i ])) for i in range ( len ( upper_values )) if upper_values [ i ]] if len ( nb ) >= k : break [ nb . append ( x ) for x in right_values if x ] [ dist . append ( distance ( lat , lon , lats [ i ], lons [ right_i ])) for i in range ( len ( right_values )) if right_values [ i ]] if len ( nb ) >= k : break [ nb . append ( x ) for x in lower_values if x ] [ dist . append ( distance ( lat , lon , lats [ lower_i ], lons [ i ])) for i in range ( len ( lower_values )) if lower_values [ i ]] if len ( nb ) >= k : break j += 1 if prob != []: print ( prob ) return nb , dist period = pd . date_range ( start = \"1800-01-01\" , end = \"2020-07-01\" , freq = \"MS\" ) . to_pydatetime () . tolist () def get_data ( datetime , lat , lon , dataset , k ): #k is the number of neighboors year = datetime . year month = datetime . month time_index = period . index ( dt . datetime ( year , month , 1 , 0 , 0 )) nb , dist = get_neighboors ( time_index , lat , lon , dataset , k ) inv_dist = [ 1 / x for x in dist ] return sum ( nb [ i ] * inv_dist [ i ] for i in range ( len ( nb ))) / sum ( inv_dist ) def get_data_df ( df , dataset , label , k = 15 ): output = pd . DataFrame ( np . zeros ([ len ( df ), 1 ]), columns = [ label ]) for i in range ( len ( df )): output . loc [ i , label ] = get_data ( df . Date [ i ], df . Latitude [ i ], df . Longitude [ i ], dataset , k ) return output Abaixo executamos tudo que foi implementado acima para gerar o novo dataframe. Salvamos em um novo csv que usaremos nas pr\u00f3ximas an\u00e1lises. %% time # Esta c\u00e9lula demora um pouco a ser executada; Todos essas dados j\u00e1 est\u00e3o salvos em data_atl_merged2.csv df . loc [:, 'sst' ] = get_data_df ( df , sst , 'sst' ) print ( \"OK -- sst\" ) df . loc [:, 'rhum' ] = get_data_df ( df , rhum , 'rhum' ) print ( \"OK -- rhum\" ) df . loc [:, 'wspd' ] = get_data_df ( df , rhum , 'wspd' ) print ( \"OK -- wspd\" ) df . loc [:, 'slp' ] = get_data_df ( df , slp , 'slp' ) print ( \"OK -- slp\" ) df . loc [:, 'cldc' ] = get_data_df ( df , cldc , 'cldc' ) print ( \"OK -- cldc \\n \" ) df . columns OK -- sst OK -- rhum OK -- wspd OK -- slp OK -- cldc CPU times : user 2 min 34 s , sys : 868 ms , total : 2 min 35 s Wall time : 2 min 33 s Index ([ 'ID' , 'Name' , 'Date' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'Minimum Pressure' , 'Date_c' , 'Year' , 'Month' , 'Day' , 'Latitude_c' , 'Longitude_c' , 'Duration' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ], dtype = 'object' ) df . to_csv ( '../Datasets/data_atl_merged2.csv' , index = 0 ) df = pd . read_csv ( '../Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) df . dtypes ID object Name object Date datetime64 [ ns ] Time int64 Event object Status object Latitude float64 Longitude float64 Maximum Wind int64 Minimum Pressure int64 Date_c object Year int64 Month int64 Day int64 Latitude_c float64 Longitude_c float64 Duration int64 sst float64 rhum float64 wspd float64 slp float64 cldc float64 dtype : object","title":"k-NN ponderado por inverso da dist\u00e2ncia"},{"location":"Data_Cleaning_and_EDA/#outras-visualizacoes","text":"import warnings warnings . filterwarnings ( 'ignore' ) dfplot = df [[ 'ID' , 'Year' , 'Maximum Wind' ]] def category ( mw ): if mw >= 137 : return \"Categoria 5\" elif mw >= 113 : return \"Categoria 4\" elif mw >= 96 : return \"Categoria 3\" elif mw >= 83 : return \"Categoria 2\" elif mw >= 64 : return \"Categoria 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" plt . title ( \"N\u00ba Tempestades por Categoria M\u00e1xima\" , fontsize = 15 ) cat_id = dfplot . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) dfplot . loc [:, 'Categoria' ] = dfplot . ID . apply ( lambda x : cat_id [ x ]) dfplot . groupby ( 'Categoria' )[ 'ID' ] . count () . plot . bar (); # plt.savefig('figs/hur_by_category.jpg') #Major Hurricanes by Year plt . title ( \"N\u00ba Furac\u00f5es Graves por ano (Cat>=3)\" , fontsize = 15 ) major_df = dfplot [ dfplot . Categoria . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] major_df . groupby ( 'Year' )[ 'ID' ] . count () . plot () plt . xlabel ( \"Ano\" ); # plt.savefig(\"figs/major_hurricanes_year.jpg\") Nos plots abaixo usaremos a biblioteca troPYcal. Internamente ela tem o dataset HURDAT2 atualizado e possui fun\u00e7\u00f5es de visualiza\u00e7\u00e3o prontas, simples de serem usadas. Refer\u00eancia: https://tropycal.github.io/tropycal/examples/index.html hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) storm = hurdat_atl . get_storm (( 'michael' , 2018 )) # storm . plot ( return_ax = True ) plt . savefig ( '../figs/troPycal_michal18.jpg' ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (4.0 seconds) storm . plot_tors ( plotPPH = True ) plt . savefig ( '../figs/troPYcal_michaelPPH.jpg' ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (10.42 seconds) <Figure size 432x288 with 0 Axes> storm . plot_nhc_forecast ( forecast = 2 , return_ax = True ) plt . savefig ( '../figs/troPYcal_michael_fore2.jpg' ) storm . plot_nhc_forecast ( forecast = 12 , return_ax = True ) plt . savefig ( '../figs/troPYcal_michael_fore12.jpg' ) ibtracs = tracks . TrackDataset ( basin = 'all' , source = 'ibtracs' , ibtracs_mode = 'jtwc_neumann' , catarina = True ) storm = ibtracs . get_storm (( 'catarina' , 2004 )) storm . plot ( return_ax = True ) plt . savefig ( '../figs/troPYcal_catarina.jpg' ) --> Starting to read in ibtracs data --> Completed reading in ibtracs data (114.01 seconds) tor_data = tornado . TornadoDataset () tor_ax , domain , leg_tor = tor_data . plot_tors ( dt . datetime ( 2011 , 4 , 27 ), plotPPH = True , return_ax = True ) tor_ax plt . savefig ( '../figs/troPYcal_dailyPPH.jpg' ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (9.15 seconds) hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) # # hurdat_atl . assign_storm_tornadoes ( dist_thresh = 750 ) # # storm = hurdat_atl . get_storm (( 'ivan' , 2004 )) # # storm . plot_tors ( plotPPH = True , return_ax = True ) plt . savefig ( '../figs/troPYcal_ivan04.jpg' ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (8.13 seconds) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (9.86 seconds) --> Starting to assign tornadoes to storms --> Completed assigning tornadoes to storm (403.30 seconds) hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) #Retrieve Hurricane Michael from 2018 storm = hurdat_atl . get_storm (( 'michael' , 2018 )) #Retrieve the 2017 Atlantic hurricane season season = hurdat_atl . get_season ( 2017 ) #Printing the Storm object lists relevant data: print ( storm ) print ( hurdat_atl . search_name ( 'Michael' )) # --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (4.39 seconds) < tropycal . tracks . Storm > Storm Summary : Maximum Wind : 140 knots Minimum Pressure : 919 hPa Start Date : 0600 UTC 07 October 2018 End Date : 1800 UTC 11 October 2018 Variables : date ( datetime ) [ 2018 - 10 - 06 18 : 00 : 00 .... 2018 - 10 - 15 18 : 00 : 00 ] extra_obs ( int64 ) [ 0 .... 0 ] special ( str ) [ .... ] type ( str ) [ LO .... EX ] lat ( float64 ) [ 17 . 8 .... 41 . 2 ] lon ( float64 ) [ - 86 . 6 .... - 10 . 0 ] vmax ( int64 ) [ 25 .... 35 ] mslp ( int64 ) [ 1006 .... 1001 ] wmo_basin ( str ) [ north_atlantic .... north_atlantic ] More Information : id : AL142018 operational_id : AL142018 name : MICHAEL year : 2018 season : 2018 basin : north_atlantic source_info : NHC Hurricane Database source : hurdat ace : 12 . 5 realtime : False [ 2000 , 2012 , 2018 ] hurdat_atl . ace_climo ( plot_year = 2018 , compare_years = 2017 ) plt . savefig ( '../figs/troPYcal_ACE.jpg' ) <Figure size 432x288 with 0 Axes> hurdat_atl . ace_climo ( rolling_sum = 30 , plot_year = 2018 , compare_years = 2017 ) plt . savefig ( '../figs/troPYcal_ACE_rolling.jpg' ) <Figure size 432x288 with 0 Axes> hurdat_atl . wind_pres_relationship ( storm = ( 'sandy' , 2012 )) plt . savefig ( '../figs/troPYcal_wind_press.jpg' ) <Figure size 432x288 with 0 Axes> # ibtracs = tracks.TrackDataset(basin='all',source='ibtracs',ibtracs_mode='jtwc_neumann',catarina=True) ibtracs . gridded_stats ( request = \"maximum wind\" , return_ax = True ) # plt . savefig ( '../figs/troPYcal_maxwind.jpg' ) --> Getting filtered storm tracks --> Grouping by lat/lon/storm --> Generating plot ibtracs . gridded_stats ( request = \"number of storms\" , thresh = { 'dv_min' : 30 }, prop = { 'cmap' : 'plasma_r' }) plt . savefig ( '../figs/troPYcal_num_storms.jpg' ) --> Getting filtered storm tracks --> Grouping by lat/lon/storm --> Generating plot","title":"Outras Visualiza\u00e7\u00f5es"},{"location":"NN-TrackPrediction/","text":"Redes Neurais para Previs\u00e3o de Trajet\u00f3ria Notebook refer\u00eancia: NN-TrackPrediction.ipynb Al\u00e9m de todas as an\u00e1lises feitas, implementamos tamb\u00e9m uma rede neural que usa informa\u00e7\u00f5es de um furac\u00e3o de algumas horas atr\u00e1s e do presente, e projeta a sua posi\u00e7\u00e3o futura em coordenadas geogr\u00e1ficas. Funcionamento da Predi\u00e7\u00e3o O input da rede \u00e9 uma matriz do tipo: \\left[\\begin{matrix} x_1^{(t-2)} & x_2^{(t-2)} & ...& x_n^{(t-2)}\\\\ x_1^{(t-1)} & x_2^{(t-1)} & ...& x_n^{(t-1)}\\\\ x_1^{(t)} & x_2^{(t)} & ...& x_n^{(t)}\\end{matrix}\\right] na qual: x_1,~x_2, ~..., ~x_n representam os n preditores usados. Testamos dois conjuntos de modelos, o primeiro com 4 preditores (Tempo, Latitude, Longitude e Velocidade de vento) e o segundo com 8 (anteriores mais Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade). Fizemos uma compara\u00e7\u00e3o da fun\u00e7\u00e3o de perda entre eles para eleger o melhor conjunto de entrada, e usar 8 preditores apresentou melhores resultados. x_i^{(k)} representa o preditor i no registro de tempo k . Cada registro dos nossos dados est\u00e1 espa\u00e7ado por 6 horas do pr\u00f3ximo e do anterior. Sendo assim, pela matriz acima, usamos um conjunto de 3 registros sequenciais, que representam 18 horas. Nossa sa\u00edda \u00e9 da forma: O vetor Y geral \u00e9 composto por: [lat^{(t+1)},~ lon^{(t+1)}], que representa a latitude e longitude no registro t+1 ou seja, + horas depois do \u00faltimo ponto de treinamento. Treino, Valida\u00e7\u00e3o e Teste Para formatar os dados de treino, teste e valida\u00e7\u00e3o, tivemos que fazer um tratamento diferenciado e manual, para que o treinamento e previs\u00e3o ocorresse tempestade por tempestade e n\u00e3o misturasse dados. Usamos uma divis\u00e3o de 70% para treino, 20% para valida\u00e7\u00e3o e 10% para teste. Para mais detalhes do processo veja o notebook NN-TrackPrediction.ipynb . Anatomia das redes e compara\u00e7\u00e3o dos modelos Fizemos v\u00e1rios modelos com anatomias diferentes apesar de parecidas, elegemos o melhor atrav\u00e9s do MSE (Erros M\u00e9dio Quadr\u00e1tico) nos dados de teste e valida\u00e7\u00e3o. A anatomia desse modelo campe\u00e3o \u00e9 a seguinte: 8 vari\u00e1veis de input (Tempo, Latitude, Longitude e Velocidade de vento, Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade) para cada um dos registros passados. Como usamos 3 registros temos ent\u00e3o 24 neur\u00f4nios de entrada. 1 \u00fanica camada interna com 9 neur\u00f4nios e fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoidal. 2 neur\u00f4nios para a camada de output, um para latitude e outro para longitude, com ativa\u00e7\u00e3o linear. Camada de dropout de 15% para evitar overfitting. C\u00f3digo model_892 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) Performance Tivemos um R2-score de aproximadamente 0.988 nos dados de teste, um resultado excelente! Abaixo h\u00e1 um c\u00f3digo oculto que plota a rela\u00e7\u00e3o bem pr\u00f3xima de linear entre os dados previstos e reais. C\u00f3digo from sklearn.metrics import r2_score ypred = model_892 . predict ( xtest2 ) lat_r2 = r2_score ( ytest2 [:, 0 ], ypred [:, 0 ]) lon_r2 = r2_score ( ytest2 [:, 1 ], ypred [:, 1 ]) tot_r2 = r2_score ( ytest2 , ypred ) print ( f \"R2 Latitude Teste - { lat_r2 } \" ) print ( f \"R2 Longitude Teste - { lon_r2 } \" ) print ( f \"R2 Total Teste - { tot_r2 } \" ) fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 18 , 6 )) ax . set_title ( \"Latitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax . set_xlabel ( \"Real\" , fontsize = 13 ) ax . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax . scatter ( ytest2 [:, 0 ], ypred [:, 0 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lat_r2 , 3 ) } \" ) ax . legend ( loc = 'best' , fontsize = 13 ) ax1 . set_title ( \"Longitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax1 . set_xlabel ( \"Real\" , fontsize = 13 ) ax1 . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax1 . scatter ( ytest2 [:, 1 ], ypred [:, 1 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lon_r2 , 3 ) } \" ) ax1 . legend ( loc = 'best' , fontsize = 13 ); plt . savefig ( '../figs/lat_lon_teste.jpg' ) R2 Latitude Teste - 0.9890722375652974 R2 Longitude Teste - 0.9879249013965508 R2 Total Teste - 0.9884985694809241 Previs\u00f5es Testamos a previs\u00e3o para algumas tempestades espec\u00edficas do conjunto de testes e tivemos bons resultados. A fun\u00e7\u00e3o predict_storm recebe um ID de tempestade, e plota as predi\u00e7\u00f5es al\u00e9m de printar os R2 scores's de Latitude e Longitude nessa ordem. predict_storm ( 'AL092011' ) 0.912396866066715 0.8587266243812284 predict_storm ( 'AL112015' ) 0.9650855905600251 0.9805961966628389 Para mais detalhes do funcionamento da fun\u00e7\u00e3o predict visite o notebook NN-TrackPrediction.ipynb .","title":"Prevendo trajet\u00f3rias"},{"location":"NN-TrackPrediction/#redes-neurais-para-previsao-de-trajetoria","text":"Notebook refer\u00eancia: NN-TrackPrediction.ipynb Al\u00e9m de todas as an\u00e1lises feitas, implementamos tamb\u00e9m uma rede neural que usa informa\u00e7\u00f5es de um furac\u00e3o de algumas horas atr\u00e1s e do presente, e projeta a sua posi\u00e7\u00e3o futura em coordenadas geogr\u00e1ficas.","title":"Redes Neurais para Previs\u00e3o de Trajet\u00f3ria"},{"location":"NN-TrackPrediction/#funcionamento-da-predicao","text":"O input da rede \u00e9 uma matriz do tipo: \\left[\\begin{matrix} x_1^{(t-2)} & x_2^{(t-2)} & ...& x_n^{(t-2)}\\\\ x_1^{(t-1)} & x_2^{(t-1)} & ...& x_n^{(t-1)}\\\\ x_1^{(t)} & x_2^{(t)} & ...& x_n^{(t)}\\end{matrix}\\right] na qual: x_1,~x_2, ~..., ~x_n representam os n preditores usados. Testamos dois conjuntos de modelos, o primeiro com 4 preditores (Tempo, Latitude, Longitude e Velocidade de vento) e o segundo com 8 (anteriores mais Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade). Fizemos uma compara\u00e7\u00e3o da fun\u00e7\u00e3o de perda entre eles para eleger o melhor conjunto de entrada, e usar 8 preditores apresentou melhores resultados. x_i^{(k)} representa o preditor i no registro de tempo k . Cada registro dos nossos dados est\u00e1 espa\u00e7ado por 6 horas do pr\u00f3ximo e do anterior. Sendo assim, pela matriz acima, usamos um conjunto de 3 registros sequenciais, que representam 18 horas. Nossa sa\u00edda \u00e9 da forma: O vetor Y geral \u00e9 composto por: [lat^{(t+1)},~ lon^{(t+1)}], que representa a latitude e longitude no registro t+1 ou seja, + horas depois do \u00faltimo ponto de treinamento.","title":"Funcionamento da Predi\u00e7\u00e3o"},{"location":"NN-TrackPrediction/#treino-validacao-e-teste","text":"Para formatar os dados de treino, teste e valida\u00e7\u00e3o, tivemos que fazer um tratamento diferenciado e manual, para que o treinamento e previs\u00e3o ocorresse tempestade por tempestade e n\u00e3o misturasse dados. Usamos uma divis\u00e3o de 70% para treino, 20% para valida\u00e7\u00e3o e 10% para teste. Para mais detalhes do processo veja o notebook NN-TrackPrediction.ipynb .","title":"Treino, Valida\u00e7\u00e3o e Teste"},{"location":"NN-TrackPrediction/#anatomia-das-redes-e-comparacao-dos-modelos","text":"Fizemos v\u00e1rios modelos com anatomias diferentes apesar de parecidas, elegemos o melhor atrav\u00e9s do MSE (Erros M\u00e9dio Quadr\u00e1tico) nos dados de teste e valida\u00e7\u00e3o. A anatomia desse modelo campe\u00e3o \u00e9 a seguinte: 8 vari\u00e1veis de input (Tempo, Latitude, Longitude e Velocidade de vento, Temperatura do mar, Umidade, Press\u00e3o e Nebulosidade) para cada um dos registros passados. Como usamos 3 registros temos ent\u00e3o 24 neur\u00f4nios de entrada. 1 \u00fanica camada interna com 9 neur\u00f4nios e fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoidal. 2 neur\u00f4nios para a camada de output, um para latitude e outro para longitude, com ativa\u00e7\u00e3o linear. Camada de dropout de 15% para evitar overfitting. C\u00f3digo model_892 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ])","title":"Anatomia das redes e compara\u00e7\u00e3o dos modelos"},{"location":"NN-TrackPrediction/#performance","text":"Tivemos um R2-score de aproximadamente 0.988 nos dados de teste, um resultado excelente! Abaixo h\u00e1 um c\u00f3digo oculto que plota a rela\u00e7\u00e3o bem pr\u00f3xima de linear entre os dados previstos e reais. C\u00f3digo from sklearn.metrics import r2_score ypred = model_892 . predict ( xtest2 ) lat_r2 = r2_score ( ytest2 [:, 0 ], ypred [:, 0 ]) lon_r2 = r2_score ( ytest2 [:, 1 ], ypred [:, 1 ]) tot_r2 = r2_score ( ytest2 , ypred ) print ( f \"R2 Latitude Teste - { lat_r2 } \" ) print ( f \"R2 Longitude Teste - { lon_r2 } \" ) print ( f \"R2 Total Teste - { tot_r2 } \" ) fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 18 , 6 )) ax . set_title ( \"Latitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax . set_xlabel ( \"Real\" , fontsize = 13 ) ax . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax . scatter ( ytest2 [:, 0 ], ypred [:, 0 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lat_r2 , 3 ) } \" ) ax . legend ( loc = 'best' , fontsize = 13 ) ax1 . set_title ( \"Longitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax1 . set_xlabel ( \"Real\" , fontsize = 13 ) ax1 . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax1 . scatter ( ytest2 [:, 1 ], ypred [:, 1 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lon_r2 , 3 ) } \" ) ax1 . legend ( loc = 'best' , fontsize = 13 ); plt . savefig ( '../figs/lat_lon_teste.jpg' ) R2 Latitude Teste - 0.9890722375652974 R2 Longitude Teste - 0.9879249013965508 R2 Total Teste - 0.9884985694809241","title":"Performance"},{"location":"NN-TrackPrediction/#previsoes","text":"Testamos a previs\u00e3o para algumas tempestades espec\u00edficas do conjunto de testes e tivemos bons resultados. A fun\u00e7\u00e3o predict_storm recebe um ID de tempestade, e plota as predi\u00e7\u00f5es al\u00e9m de printar os R2 scores's de Latitude e Longitude nessa ordem. predict_storm ( 'AL092011' ) 0.912396866066715 0.8587266243812284 predict_storm ( 'AL112015' ) 0.9650855905600251 0.9805961966628389 Para mais detalhes do funcionamento da fun\u00e7\u00e3o predict visite o notebook NN-TrackPrediction.ipynb .","title":"Previs\u00f5es"},{"location":"PowerDissipationIndex/","text":"Power Dissipation Index (PDI) Analysis Definitions PDI is an index that represents the destructive power of a storm combining together, intensity, duration, and frequency. References: Emanuel, 2005 and Emanuel, 2007 In the references, Kerry Emanuel defines the index as: PDI\\equiv\\int_0^{\\tau}V^3_{max}dt~, where V_{max} is the maximum sustained wind speed, and \\tau is the lifetime of the storm event. The PDI Dataset We're gonna use the PDI calculated by National Oceanic & Atmospheric Administration (NOAA) which data is avaible at Our World in Data . It covers the North Atlantic, Caribbean and Gulf of Mexico storms. The data has been smoothed through a five-year weighted average plotted at the center, in order to remove interannual variability. We're gonna o the same smooth with our climate dataset of Atlantic MDR. import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score import scipy.stats as stats from math import * from statsmodels.graphics.tsaplots import plot_acf , plot_pacf from statsmodels.tsa.holtwinters import ExponentialSmoothing , HoltWintersResults from statsmodels.tsa.stattools import adfuller from sklearn.model_selection import TimeSeriesSplit import datetime as dt sns . set () % matplotlib inline raw_pdi = pd . read_csv ( '../Datasets/cyclone-power-dissipation-index.csv' ) raw_pdi . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entity Code Year Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) 0 North Atlantic NaN 1951 2.7846 1 North Atlantic NaN 1952 2.3445 2 North Atlantic NaN 1953 2.2639 3 North Atlantic NaN 1954 2.4730 4 North Atlantic NaN 1955 2.4041 raw_pdi . dtypes Entity object Code float64 Year int64 Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) float64 dtype: object PDI = raw_pdi [[ 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' ]] . rename ( columns = { 'Year' : 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' : 'PDI' }) PDI = PDI . set_index ( 'Year' ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 9 )) axs [ 0 , 0 ] . set_title ( \"Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 0 ] . set_ylabel ( \"PDI\" , fontsize = 16 ) axs [ 0 , 0 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 0 ] . plot ( PDI . index , PDI . PDI , lw = 4 ); axs [ 0 , 1 ] . set_title ( \"Logarithm of Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 1 ] . set_ylabel ( \"log(PDI)\" , fontsize = 16 ) axs [ 0 , 1 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 1 ] . plot ( PDI . index , np . log ( PDI . PDI ), lw = 4 ) sns . distplot ( PDI . PDI , ax = axs [ 1 , 0 ]) sns . distplot ( np . log ( PDI . PDI ), ax = axs [ 1 , 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ef52a30> Atlantic MDR Climate Data Let's do the same smoothing operation in our \"atlantic_mdr\" dataset. We're gonna apply a 1\u20133\u20134\u20133\u20131 weighted average Emanuel, 2007 on the data, to get out of interannual variability. atlantic_mdr = pd . read_csv ( '../Datasets/atlantic_mdr.csv' ) def smooth ( col ): n = len ( col ) new_col = np . zeros ([ n , 1 ]) w = np . array ([[ 1 , 3 , 4 , 3 , 1 ]]) for i in range ( 2 , n - 2 ): new_col [ i ] = w . dot ( np . array ( col [ i - 2 : i + 3 ]) . reshape ( - 1 , 1 )) / 12 return new_col . ravel () # atlantic_mdr = atlantic_mdr[(atlantic_mdr.Month>=8) & (atlantic_mdr.Month<=10)] mdr_annual = atlantic_mdr . groupby ( 'Year' ) . agg ({ 'sst' : np . mean , 'rhum' : np . mean , 'wspd' : np . mean , 'slp' : np . mean , 'vwnd' : np . mean , 'cldc' : np . mean }) for col in mdr_annual . columns : mdr_annual . loc [:, col ] = smooth ( mdr_annual [ col ]) mdr_annual = mdr_annual . loc [ 1951 : 2013 ,:] # mdr_annual mdr_annual [ 'PDI' ] = np . array ( PDI . PDI ) . reshape ( - 1 , 1 ) # mdr_annual Analysing Correlation # corr = mdr_annual.corr() # corr.style.background_gradient(cmap='coolwarm') # corr = df.corr() # corr.style.background_gradient(cmap='coolwarm') f , axs = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( mdr_annual . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 0 ]); df = mdr_annual . copy () df [ 'PDI' ] = np . log ( df [ 'PDI' ]) df = df . rename ( columns = { \"PDI\" : \"logPDI\" }) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( df . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 1 ]); plt . show () fig , ax = plt . subplots ( figsize = ( 10 , 6 )) plt . title ( \"North Atlantic MDR SST vs PDI\" , fontsize = 18 ) plt . xlabel ( \"Year\" , fontsize = 14 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) ax2 = ax . twinx () ax . plot ( mdr_annual . sst , lw = 4 , label = \"SST\" ); ax . set_ylabel ( \"Sea Surface Temperature\" , fontsize = 14 ) ax2 . plot ( mdr_annual . PDI , lw = 4 , label = \"PDI\" , color = 'orange' ); ax2 . set_ylabel ( \"Power Dissipation Index\" , fontsize = 14 ) fig . legend ( loc = \"upper left\" , fontsize = 14 , bbox_to_anchor = ( 0 , 1 ), bbox_transform = ax . transAxes ); # sns.scatterplot(x=np.log(mdr_annual['sst']), y=np.log(mdr_annual['PDI'])) # sns.scatterplot(x=X.sst, y=mdr_annual['PDI']) Simple Linear Model Quanto \u00e0 trajet\u00f3ria do PDI, \u00e9 observ\u00e1vel uma leve tend\u00eancia positiva. # Aparentemente o PDI cresceu: X = mdr_annual . drop ( columns = \"PDI\" ) X = sm . add_constant ( X ) Y = np . array ( mdr_annual [ 'PDI' ]) . reshape ( - 1 , 1 ) X [ 'Year' ] = np . array ( X . index ) model = sm . OLS ( Y , sm . add_constant ( X [ 'Year' ])) . fit () plt . plot ( PDI ) plt . plot ( model . fittedvalues ) plt . title ( \"Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta\" ) Text(0.5, 1.0, 'Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta') Usando as covari\u00e1veis do dataset, podemos fazer um modelo onde achamos rela\u00e7\u00e3o com a press\u00e3o ao nivel do mar (slp), a humidade (rhum) e a \"cloudin. # X['sst'] = np.log(X['sst']) ( xtrain , xtest , ytrain , ytest ) = train_test_split ( X . drop ( columns = [ \"sst\" , \"wspd\" , \"vwnd\" , \"Year\" ]), np . log ( Y )) model = sm . OLS ( ytrain , xtrain ) . fit () ypred = model . predict ( xtest ) print ( \"R2 nos dados separados para teste:\" , r2_score ( ytest , ypred )) print ( model . summary ()) R2 nos dados separados para teste : 0 . 2268374488496503 OLS Regression Results ============================================================================== Dep . Variable : y R - squared : 0 . 781 Model : OLS Adj . R - squared : 0 . 766 Method : Least Squares F - statistic : 51 . 26 Date : Mon , 31 Aug 2020 Prob ( F - statistic ): 2 . 98 e - 14 Time : 10 : 22 : 52 Log - Likelihood : 14 . 019 No . Observations : 47 AIC : - 20 . 04 Df Residuals : 43 BIC : - 12 . 64 Df Model : 3 Covariance Type : nonrobust ============================================================================== coef std err t P >| t | [ 0 . 025 0 . 975 ] ------------------------------------------------------------------------------ const - 484 . 2535 146 . 318 - 3 . 310 0 . 002 - 779 . 332 - 189 . 175 rhum 0 . 7547 0 . 079 9 . 497 0 . 000 0 . 594 0 . 915 slp 0 . 4106 0 . 143 2 . 871 0 . 006 0 . 122 0 . 699 cldc 1 . 9827 0 . 229 8 . 641 0 . 000 1 . 520 2 . 445 ============================================================================== Omnibus : 4 . 476 Durbin - Watson : 2 . 046 Prob ( Omnibus ): 0 . 107 Jarque - Bera ( JB ): 3 . 413 Skew : - 0 . 468 Prob ( JB ): 0 . 182 Kurtosis : 3 . 932 Cond . No . 5 . 42 e + 06 ============================================================================== Warnings : [ 1 ] Standard Errors assume that the covariance matrix of the errors is correctly specified . [ 2 ] The condition number is large , 5 . 42 e + 06 . This might indicate that there are strong multicollinearity or other numerical problems . Pode parecer um pouco contraintuitivo a princ\u00edpio n\u00e3o achar rela\u00e7\u00e3o significativa da temperatura do mar e da intensidade (na verdade, a destrutibilidade, o PDI) dos furac\u00f5es. Uma explica\u00e7\u00e3o plausivel para isso \u00e9 que os dados de temperatura conseguem ser explicados pelas outras vari\u00e1veis: model = sm . OLS ( X [ 'sst' ], X . drop ( columns = [ 'vwnd' , 'wspd' , 'Year' , 'sst' ])) . fit () print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: sst R-squared: 0.749 Model: OLS Adj. R-squared: 0.736 Method: Least Squares F-statistic: 58.61 Date: Mon, 31 Aug 2020 Prob (F-statistic): 1.08e-17 Time: 10:22:52 Log-Likelihood: 53.106 No. Observations: 63 AIC: -98.21 Df Residuals: 59 BIC: -89.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -177.9485 73.852 -2.410 0.019 -325.726 -30.171 rhum 0.2643 0.040 6.528 0.000 0.183 0.345 slp 0.1764 0.072 2.446 0.017 0.032 0.321 cldc 1.3740 0.113 12.204 0.000 1.149 1.599 ============================================================================== Omnibus: 0.296 Durbin-Watson: 0.473 Prob(Omnibus): 0.862 Jarque-Bera (JB): 0.479 Skew: -0.023 Prob(JB): 0.787 Kurtosis: 2.575 Cond. No. 5.52e+06 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 5.52e+06. This might indicate that there are strong multicollinearity or other numerical problems. Time series Analysis tsPDI = pd . DataFrame ( Y , index = [ dt . datetime ( x , 1 , 1 ) for x in mdr_annual . index ], columns = [ \"PDI\" ]) tsPDI . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PDI 1951-01-01 2.7846 1952-01-01 2.3445 1953-01-01 2.2639 1954-01-01 2.4730 1955-01-01 2.4041 Vamos tratar agora a trajet\u00f3ria do PDI como uma s\u00e9rie temporal. Inicialmente, vamos ajustar um modelo autorregressivo de ordem 2: mod = sm . tsa . ARIMA ( tsPDI , order = ( 2 , 0 , 0 )) modfit = mod . fit () # modfit.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit.resid, ax = axs[0, 0]) # plot_pacf(modfit.resid, ax = axs[0, 1]) # sm.qqplot(modfit.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit.resid, ax = axs[1, 1]) # plt.show()# modfit.plot_diagnostics() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia das previs\u00f5es com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Modelando o log da s\u00e9rie: tsLogPDI = tsPDI . apply ( np . log ) #sm.tsa.stattools.adfuller(tsLogPDI['PDI']) ainda n\u00e3o \u00e9 estacionaria a 5% de significancia mod2 = sm . tsa . ARIMA ( tsLogPDI , order = ( 2 , 0 , 0 )) modfit2 = mod2 . fit () # modfit2.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit2.resid, ax = axs[0, 0]) # plot_pacf(modfit2.resid, ax = axs[0, 1]) # sm.qqplot(modfit2.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit2.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: sims = np . zeros (( 1000 , step )) for i in range ( 1000 ): sims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( sims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 30 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 15 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () # plt.plot(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) # sm.tsa.stattools.adfuller(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) tsTr = np . log ( np . diff ( np . log ( tsPDI [ 'PDI' ])) + 1 ) # terceiro modelo: log(delta(log(PDI))+1) como ARMA(2,1) mod3 = sm . tsa . ARIMA ( tsTr , order = ( 2 , 0 , 1 )) modfit3 = mod3 . fit () # modfit3.summary() # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit3.resid, ax = axs[0, 0]) # plot_pacf(modfit3.resid, ax = axs[0, 1]) # sm.qqplot(modfit3.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit3.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsTr ) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = range ( len ( tsTr ), len ( tsTr ) + step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsTr ) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia da transforma\u00e7\u00e3o predita com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es da transforma\u00e7\u00e3o' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = last + np . cumsum ( dsims [ i ]) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 50 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do log(PDI)' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = np . exp ( last + np . cumsum ( dsims [ i ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Count Medir destrutibilidade do furac\u00e3o n\u00e3o \u00e9 coisa trivial; n\u00e3o basta olhar o PDI visto que os nossos dados de PDI podem estar errados por medi\u00e7\u00f5es inconsistentes de velocidade at\u00e9 os anos 90, onde come\u00e7amos a ter um comportamento an\u00f4malo do ENSO; al\u00e9m da din\u00e2mica sazonal de correntes termosal\u00ednicas gerarem sazonalidade nas temperaturas entre dec\u00e1das (Atlantic Multidecadal Oscilation, AMO). Outras alternativas comumente consideradas s\u00e3o a de contagem de furac\u00f5es no ano e de tamanho de cada furac\u00e3o; cada uma delas com seus problemas particulares. Abaixo, fazemos um modelo simples de GLM para contagem usando como preditores as covari data = pd . read_csv ( '../Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data = data [[ 'ID' , 'Name' , 'Date' , 'Year' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ]] def category ( mw ): if mw >= 137 : return \"Category 5\" elif mw >= 113 : return \"Category 4\" elif mw >= 96 : return \"Category 3\" elif mw >= 83 : return \"Category 2\" elif mw >= 64 : return \"Category 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" cat_id = data . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) data . loc [:, 'Category' ] = data . ID . apply ( lambda x : cat_id [ x ]) # data.groupby('Category')['ID'].count().plot.bar(); major_df = data [ data . Category . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] major_df . groupby ( 'Year' )[ 'ID' ] . count () . plot () #sm.tsa.stattools.adfuller(count) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ec16a60> count = major_df . groupby ( \"Year\" )[ \"ID\" ] . count () X = pd . pivot_table ( major_df , index = \"Year\" , values = [ \"sst\" , \"rhum\" , \"wspd\" , \"slp\" , \"cldc\" ]) X [ 'Y' ] = X . index . values countModel = sm . GLM ( count , sm . add_constant ( X ), family = sm . families . NegativeBinomial ()) countFit = countModel . fit () print ( countFit . summary ()) #descomente para ver o sumario do ajuste plt . plot ( count ) plt . plot ( countFit . predict ( sm . add_constant ( X ))) plt . show () Generalized Linear Model Regression Results ============================================================================== Dep. Variable: ID No. Observations: 60 Model: GLM Df Residuals: 54 Model Family: NegativeBinomial Df Model: 5 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -347.61 Date: Mon, 31 Aug 2020 Deviance: 26.626 Time: 10:22:58 Pearson chi2: 26.3 No. Iterations: 12 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -146.2316 147.049 -0.994 0.320 -434.443 141.980 cldc 0.1977 0.589 0.336 0.737 -0.956 1.351 rhum 0.0182 0.067 0.271 0.786 -0.113 0.150 slp 0.1338 0.142 0.940 0.347 -0.145 0.413 sst 0.0952 0.132 0.723 0.470 -0.163 0.353 wspd 0.0182 0.067 0.271 0.786 -0.113 0.150 Y 0.0048 0.008 0.602 0.547 -0.011 0.020 ==============================================================================","title":"Destrutividade no longo prazo"},{"location":"PowerDissipationIndex/#power-dissipation-index-pdi-analysis","text":"","title":"Power Dissipation Index (PDI) Analysis"},{"location":"PowerDissipationIndex/#definitions","text":"PDI is an index that represents the destructive power of a storm combining together, intensity, duration, and frequency. References: Emanuel, 2005 and Emanuel, 2007 In the references, Kerry Emanuel defines the index as: PDI\\equiv\\int_0^{\\tau}V^3_{max}dt~, where V_{max} is the maximum sustained wind speed, and \\tau is the lifetime of the storm event.","title":"Definitions"},{"location":"PowerDissipationIndex/#the-pdi-dataset","text":"We're gonna use the PDI calculated by National Oceanic & Atmospheric Administration (NOAA) which data is avaible at Our World in Data . It covers the North Atlantic, Caribbean and Gulf of Mexico storms. The data has been smoothed through a five-year weighted average plotted at the center, in order to remove interannual variability. We're gonna o the same smooth with our climate dataset of Atlantic MDR. import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score import scipy.stats as stats from math import * from statsmodels.graphics.tsaplots import plot_acf , plot_pacf from statsmodels.tsa.holtwinters import ExponentialSmoothing , HoltWintersResults from statsmodels.tsa.stattools import adfuller from sklearn.model_selection import TimeSeriesSplit import datetime as dt sns . set () % matplotlib inline raw_pdi = pd . read_csv ( '../Datasets/cyclone-power-dissipation-index.csv' ) raw_pdi . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entity Code Year Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) 0 North Atlantic NaN 1951 2.7846 1 North Atlantic NaN 1952 2.3445 2 North Atlantic NaN 1953 2.2639 3 North Atlantic NaN 1954 2.4730 4 North Atlantic NaN 1955 2.4041 raw_pdi . dtypes Entity object Code float64 Year int64 Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) float64 dtype: object PDI = raw_pdi [[ 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' ]] . rename ( columns = { 'Year' : 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' : 'PDI' }) PDI = PDI . set_index ( 'Year' ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 9 )) axs [ 0 , 0 ] . set_title ( \"Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 0 ] . set_ylabel ( \"PDI\" , fontsize = 16 ) axs [ 0 , 0 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 0 ] . plot ( PDI . index , PDI . PDI , lw = 4 ); axs [ 0 , 1 ] . set_title ( \"Logarithm of Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 1 ] . set_ylabel ( \"log(PDI)\" , fontsize = 16 ) axs [ 0 , 1 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 1 ] . plot ( PDI . index , np . log ( PDI . PDI ), lw = 4 ) sns . distplot ( PDI . PDI , ax = axs [ 1 , 0 ]) sns . distplot ( np . log ( PDI . PDI ), ax = axs [ 1 , 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ef52a30>","title":"The PDI Dataset"},{"location":"PowerDissipationIndex/#atlantic-mdr-climate-data","text":"Let's do the same smoothing operation in our \"atlantic_mdr\" dataset. We're gonna apply a 1\u20133\u20134\u20133\u20131 weighted average Emanuel, 2007 on the data, to get out of interannual variability. atlantic_mdr = pd . read_csv ( '../Datasets/atlantic_mdr.csv' ) def smooth ( col ): n = len ( col ) new_col = np . zeros ([ n , 1 ]) w = np . array ([[ 1 , 3 , 4 , 3 , 1 ]]) for i in range ( 2 , n - 2 ): new_col [ i ] = w . dot ( np . array ( col [ i - 2 : i + 3 ]) . reshape ( - 1 , 1 )) / 12 return new_col . ravel () # atlantic_mdr = atlantic_mdr[(atlantic_mdr.Month>=8) & (atlantic_mdr.Month<=10)] mdr_annual = atlantic_mdr . groupby ( 'Year' ) . agg ({ 'sst' : np . mean , 'rhum' : np . mean , 'wspd' : np . mean , 'slp' : np . mean , 'vwnd' : np . mean , 'cldc' : np . mean }) for col in mdr_annual . columns : mdr_annual . loc [:, col ] = smooth ( mdr_annual [ col ]) mdr_annual = mdr_annual . loc [ 1951 : 2013 ,:] # mdr_annual mdr_annual [ 'PDI' ] = np . array ( PDI . PDI ) . reshape ( - 1 , 1 ) # mdr_annual","title":"Atlantic MDR Climate Data"},{"location":"PowerDissipationIndex/#analysing-correlation","text":"# corr = mdr_annual.corr() # corr.style.background_gradient(cmap='coolwarm') # corr = df.corr() # corr.style.background_gradient(cmap='coolwarm') f , axs = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( mdr_annual . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 0 ]); df = mdr_annual . copy () df [ 'PDI' ] = np . log ( df [ 'PDI' ]) df = df . rename ( columns = { \"PDI\" : \"logPDI\" }) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( df . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 1 ]); plt . show () fig , ax = plt . subplots ( figsize = ( 10 , 6 )) plt . title ( \"North Atlantic MDR SST vs PDI\" , fontsize = 18 ) plt . xlabel ( \"Year\" , fontsize = 14 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) ax2 = ax . twinx () ax . plot ( mdr_annual . sst , lw = 4 , label = \"SST\" ); ax . set_ylabel ( \"Sea Surface Temperature\" , fontsize = 14 ) ax2 . plot ( mdr_annual . PDI , lw = 4 , label = \"PDI\" , color = 'orange' ); ax2 . set_ylabel ( \"Power Dissipation Index\" , fontsize = 14 ) fig . legend ( loc = \"upper left\" , fontsize = 14 , bbox_to_anchor = ( 0 , 1 ), bbox_transform = ax . transAxes ); # sns.scatterplot(x=np.log(mdr_annual['sst']), y=np.log(mdr_annual['PDI'])) # sns.scatterplot(x=X.sst, y=mdr_annual['PDI'])","title":"Analysing Correlation"},{"location":"PowerDissipationIndex/#simple-linear-model","text":"Quanto \u00e0 trajet\u00f3ria do PDI, \u00e9 observ\u00e1vel uma leve tend\u00eancia positiva. # Aparentemente o PDI cresceu: X = mdr_annual . drop ( columns = \"PDI\" ) X = sm . add_constant ( X ) Y = np . array ( mdr_annual [ 'PDI' ]) . reshape ( - 1 , 1 ) X [ 'Year' ] = np . array ( X . index ) model = sm . OLS ( Y , sm . add_constant ( X [ 'Year' ])) . fit () plt . plot ( PDI ) plt . plot ( model . fittedvalues ) plt . title ( \"Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta\" ) Text(0.5, 1.0, 'Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta') Usando as covari\u00e1veis do dataset, podemos fazer um modelo onde achamos rela\u00e7\u00e3o com a press\u00e3o ao nivel do mar (slp), a humidade (rhum) e a \"cloudin. # X['sst'] = np.log(X['sst']) ( xtrain , xtest , ytrain , ytest ) = train_test_split ( X . drop ( columns = [ \"sst\" , \"wspd\" , \"vwnd\" , \"Year\" ]), np . log ( Y )) model = sm . OLS ( ytrain , xtrain ) . fit () ypred = model . predict ( xtest ) print ( \"R2 nos dados separados para teste:\" , r2_score ( ytest , ypred )) print ( model . summary ()) R2 nos dados separados para teste : 0 . 2268374488496503 OLS Regression Results ============================================================================== Dep . Variable : y R - squared : 0 . 781 Model : OLS Adj . R - squared : 0 . 766 Method : Least Squares F - statistic : 51 . 26 Date : Mon , 31 Aug 2020 Prob ( F - statistic ): 2 . 98 e - 14 Time : 10 : 22 : 52 Log - Likelihood : 14 . 019 No . Observations : 47 AIC : - 20 . 04 Df Residuals : 43 BIC : - 12 . 64 Df Model : 3 Covariance Type : nonrobust ============================================================================== coef std err t P >| t | [ 0 . 025 0 . 975 ] ------------------------------------------------------------------------------ const - 484 . 2535 146 . 318 - 3 . 310 0 . 002 - 779 . 332 - 189 . 175 rhum 0 . 7547 0 . 079 9 . 497 0 . 000 0 . 594 0 . 915 slp 0 . 4106 0 . 143 2 . 871 0 . 006 0 . 122 0 . 699 cldc 1 . 9827 0 . 229 8 . 641 0 . 000 1 . 520 2 . 445 ============================================================================== Omnibus : 4 . 476 Durbin - Watson : 2 . 046 Prob ( Omnibus ): 0 . 107 Jarque - Bera ( JB ): 3 . 413 Skew : - 0 . 468 Prob ( JB ): 0 . 182 Kurtosis : 3 . 932 Cond . No . 5 . 42 e + 06 ============================================================================== Warnings : [ 1 ] Standard Errors assume that the covariance matrix of the errors is correctly specified . [ 2 ] The condition number is large , 5 . 42 e + 06 . This might indicate that there are strong multicollinearity or other numerical problems . Pode parecer um pouco contraintuitivo a princ\u00edpio n\u00e3o achar rela\u00e7\u00e3o significativa da temperatura do mar e da intensidade (na verdade, a destrutibilidade, o PDI) dos furac\u00f5es. Uma explica\u00e7\u00e3o plausivel para isso \u00e9 que os dados de temperatura conseguem ser explicados pelas outras vari\u00e1veis: model = sm . OLS ( X [ 'sst' ], X . drop ( columns = [ 'vwnd' , 'wspd' , 'Year' , 'sst' ])) . fit () print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: sst R-squared: 0.749 Model: OLS Adj. R-squared: 0.736 Method: Least Squares F-statistic: 58.61 Date: Mon, 31 Aug 2020 Prob (F-statistic): 1.08e-17 Time: 10:22:52 Log-Likelihood: 53.106 No. Observations: 63 AIC: -98.21 Df Residuals: 59 BIC: -89.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -177.9485 73.852 -2.410 0.019 -325.726 -30.171 rhum 0.2643 0.040 6.528 0.000 0.183 0.345 slp 0.1764 0.072 2.446 0.017 0.032 0.321 cldc 1.3740 0.113 12.204 0.000 1.149 1.599 ============================================================================== Omnibus: 0.296 Durbin-Watson: 0.473 Prob(Omnibus): 0.862 Jarque-Bera (JB): 0.479 Skew: -0.023 Prob(JB): 0.787 Kurtosis: 2.575 Cond. No. 5.52e+06 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 5.52e+06. This might indicate that there are strong multicollinearity or other numerical problems.","title":"Simple Linear Model"},{"location":"PowerDissipationIndex/#time-series-analysis","text":"tsPDI = pd . DataFrame ( Y , index = [ dt . datetime ( x , 1 , 1 ) for x in mdr_annual . index ], columns = [ \"PDI\" ]) tsPDI . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PDI 1951-01-01 2.7846 1952-01-01 2.3445 1953-01-01 2.2639 1954-01-01 2.4730 1955-01-01 2.4041 Vamos tratar agora a trajet\u00f3ria do PDI como uma s\u00e9rie temporal. Inicialmente, vamos ajustar um modelo autorregressivo de ordem 2: mod = sm . tsa . ARIMA ( tsPDI , order = ( 2 , 0 , 0 )) modfit = mod . fit () # modfit.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit.resid, ax = axs[0, 0]) # plot_pacf(modfit.resid, ax = axs[0, 1]) # sm.qqplot(modfit.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit.resid, ax = axs[1, 1]) # plt.show()# modfit.plot_diagnostics() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia das previs\u00f5es com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Modelando o log da s\u00e9rie: tsLogPDI = tsPDI . apply ( np . log ) #sm.tsa.stattools.adfuller(tsLogPDI['PDI']) ainda n\u00e3o \u00e9 estacionaria a 5% de significancia mod2 = sm . tsa . ARIMA ( tsLogPDI , order = ( 2 , 0 , 0 )) modfit2 = mod2 . fit () # modfit2.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit2.resid, ax = axs[0, 0]) # plot_pacf(modfit2.resid, ax = axs[0, 1]) # sm.qqplot(modfit2.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit2.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: sims = np . zeros (( 1000 , step )) for i in range ( 1000 ): sims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( sims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 30 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 15 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () # plt.plot(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) # sm.tsa.stattools.adfuller(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) tsTr = np . log ( np . diff ( np . log ( tsPDI [ 'PDI' ])) + 1 ) # terceiro modelo: log(delta(log(PDI))+1) como ARMA(2,1) mod3 = sm . tsa . ARIMA ( tsTr , order = ( 2 , 0 , 1 )) modfit3 = mod3 . fit () # modfit3.summary() # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit3.resid, ax = axs[0, 0]) # plot_pacf(modfit3.resid, ax = axs[0, 1]) # sm.qqplot(modfit3.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit3.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsTr ) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = range ( len ( tsTr ), len ( tsTr ) + step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsTr ) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia da transforma\u00e7\u00e3o predita com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es da transforma\u00e7\u00e3o' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = last + np . cumsum ( dsims [ i ]) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 50 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do log(PDI)' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = np . exp ( last + np . cumsum ( dsims [ i ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show ()","title":"Time series Analysis"},{"location":"PowerDissipationIndex/#count","text":"Medir destrutibilidade do furac\u00e3o n\u00e3o \u00e9 coisa trivial; n\u00e3o basta olhar o PDI visto que os nossos dados de PDI podem estar errados por medi\u00e7\u00f5es inconsistentes de velocidade at\u00e9 os anos 90, onde come\u00e7amos a ter um comportamento an\u00f4malo do ENSO; al\u00e9m da din\u00e2mica sazonal de correntes termosal\u00ednicas gerarem sazonalidade nas temperaturas entre dec\u00e1das (Atlantic Multidecadal Oscilation, AMO). Outras alternativas comumente consideradas s\u00e3o a de contagem de furac\u00f5es no ano e de tamanho de cada furac\u00e3o; cada uma delas com seus problemas particulares. Abaixo, fazemos um modelo simples de GLM para contagem usando como preditores as covari data = pd . read_csv ( '../Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data = data [[ 'ID' , 'Name' , 'Date' , 'Year' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ]] def category ( mw ): if mw >= 137 : return \"Category 5\" elif mw >= 113 : return \"Category 4\" elif mw >= 96 : return \"Category 3\" elif mw >= 83 : return \"Category 2\" elif mw >= 64 : return \"Category 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" cat_id = data . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) data . loc [:, 'Category' ] = data . ID . apply ( lambda x : cat_id [ x ]) # data.groupby('Category')['ID'].count().plot.bar(); major_df = data [ data . Category . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] major_df . groupby ( 'Year' )[ 'ID' ] . count () . plot () #sm.tsa.stattools.adfuller(count) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ec16a60> count = major_df . groupby ( \"Year\" )[ \"ID\" ] . count () X = pd . pivot_table ( major_df , index = \"Year\" , values = [ \"sst\" , \"rhum\" , \"wspd\" , \"slp\" , \"cldc\" ]) X [ 'Y' ] = X . index . values countModel = sm . GLM ( count , sm . add_constant ( X ), family = sm . families . NegativeBinomial ()) countFit = countModel . fit () print ( countFit . summary ()) #descomente para ver o sumario do ajuste plt . plot ( count ) plt . plot ( countFit . predict ( sm . add_constant ( X ))) plt . show () Generalized Linear Model Regression Results ============================================================================== Dep. Variable: ID No. Observations: 60 Model: GLM Df Residuals: 54 Model Family: NegativeBinomial Df Model: 5 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -347.61 Date: Mon, 31 Aug 2020 Deviance: 26.626 Time: 10:22:58 Pearson chi2: 26.3 No. Iterations: 12 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -146.2316 147.049 -0.994 0.320 -434.443 141.980 cldc 0.1977 0.589 0.336 0.737 -0.956 1.351 rhum 0.0182 0.067 0.271 0.786 -0.113 0.150 slp 0.1338 0.142 0.940 0.347 -0.145 0.413 sst 0.0952 0.132 0.723 0.470 -0.163 0.353 wspd 0.0182 0.067 0.271 0.786 -0.113 0.150 Y 0.0048 0.008 0.602 0.547 -0.011 0.020 ==============================================================================","title":"Count"}]}