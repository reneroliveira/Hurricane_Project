{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Oi (\u00e9 para mudar) No momento de confec\u00e7\u00e3o desse trabalho, o furac\u00e3o Laura estava atingindo Lousiana, o estado americano, causando a morte de pelo menos 8 pessoas e um preju\u00edzo de aproximadamente 9 bilh\u00f5es de dol\u00e1res. Olhando esse caso e considerando que casos como esse acontecem todo ano, tempestades tropicais s\u00e3o, variando com a intensidade, um problema para a humanidade tanto do ponto de vista econ\u00f4mico quanto do humanit\u00e1rio. Colocado esse problema, nossa proposta \u00e9 analisar o comportamento de curto prazo de furac\u00f5es e achar dire\u00e7\u00f5es que nos levem a entender como podemos minimizar esses constantes danos de furac\u00f5es.","title":"Home"},{"location":"#oi-e-para-mudar","text":"No momento de confec\u00e7\u00e3o desse trabalho, o furac\u00e3o Laura estava atingindo Lousiana, o estado americano, causando a morte de pelo menos 8 pessoas e um preju\u00edzo de aproximadamente 9 bilh\u00f5es de dol\u00e1res. Olhando esse caso e considerando que casos como esse acontecem todo ano, tempestades tropicais s\u00e3o, variando com a intensidade, um problema para a humanidade tanto do ponto de vista econ\u00f4mico quanto do humanit\u00e1rio. Colocado esse problema, nossa proposta \u00e9 analisar o comportamento de curto prazo de furac\u00f5es e achar dire\u00e7\u00f5es que nos levem a entender como podemos minimizar esses constantes danos de furac\u00f5es.","title":"Oi (\u00e9 para mudar)"},{"location":"EDA_weather_hurricane_cleaning_and_filtering/","text":"EDA Instala\u00e7\u00e3o e Importa\u00e7\u00e3o de pacotes ''' !apt-get install libgeos-3.5.0 !apt-get install libgeos-dev !pip install https://github.com/matplotlib/basemap/archive/master.zip !pip install pyproj==1.9.6 ''' '\\n!apt-get install libgeos-3.5.0\\n!apt-get install libgeos-dev\\n!pip install https://github.com/matplotlib/basemap/archive/master.zip\\n!pip install pyproj==1.9.6\\n' ''' !pip install netcdf4 ''' '\\n!pip install netcdf4\\n' ''' Dependencies for Tropycal Package: https://pypi.org/project/tropycal/ matplotlib >= 2.2.2 numpy >= 1.14.3 scipy >= 1.1.0 pandas >= 0.23.0 geopy >= 1.18.1 xarray >= 0.10.7 networkx >= 2.0.0 requests >= 2.22.0 To fully leverage tropycal's plotting capabilities, it is strongly recommended to have cartopy >= 0.17.0 installed. !pip install numpy !pip install pandas !pip install matplotlib !pip install scipy #!pip uninstall geopy !pip install geopy !pip install xarray !pip install networkx !pip install requests !pip install proj !pip install proj-data !pip install geos #!pip uninstall cartopy !apt-get -qq install python-cartopy python3-cartopy !pip install cartopy !pip install tropycal #!pip freeze ''' \"\\nDependencies for Tropycal Package: https://pypi.org/project/tropycal/\\nmatplotlib >= 2.2.2\\nnumpy >= 1.14.3\\nscipy >= 1.1.0\\npandas >= 0.23.0\\ngeopy >= 1.18.1\\nxarray >= 0.10.7\\nnetworkx >= 2.0.0\\nrequests >= 2.22.0\\nTo fully leverage tropycal's plotting capabilities, it is strongly recommended to have cartopy >= 0.17.0 installed.\\n\\n\\n!pip install numpy\\n!pip install pandas\\n!pip install matplotlib\\n!pip install scipy\\n#!pip uninstall geopy\\n!pip install geopy\\n!pip install xarray\\n!pip install networkx\\n!pip install requests\\n!pip install proj\\n!pip install proj-data\\n!pip install geos\\n#!pip uninstall cartopy\\n!apt-get -qq install python-cartopy python3-cartopy\\n!pip install cartopy\\n!pip install tropycal\\n#!pip freeze\\n\" % matplotlib inline import numpy as np import matplotlib import matplotlib.pyplot as plt import matplotlib.colors as c #from mpl_toolkits.basemap import Basemap,shiftgrid import pandas as pd import netCDF4 as nc from itertools import chain from netCDF4 import Dataset from netCDF4 import date2index from datetime import datetime import scipy import geopy import xarray import networkx import requests import cartopy import tropycal import tropycal.tracks as tracks import tropycal.tornado as tornado import datetime as dt import statsmodels.api as sm from statsmodels.api import OLS ''' from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.preprocessing import PolynomialFeatures from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold import math from scipy.special import gamma import seaborn as sns sns.set() alpha = 0.5 from sklearn import preprocessing ''' ' \\n from sklearn.decomposition import PCA \\n from sklearn.linear_model import LogisticRegression \\n from sklearn.linear_model import LogisticRegressionCV \\n from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \\n from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \\n from sklearn.preprocessing import PolynomialFeatures \\n from sklearn.neighbors import KNeighborsClassifier \\n from sklearn.model_selection import cross_val_score \\n from sklearn.metrics import accuracy_score \\n from sklearn.model_selection import KFold \\n\\n import math \\n from scipy.special import gamma \\n\\n import seaborn as sns \\n sns.set() \\n\\n alpha = 0.5 \\n\\n from sklearn import preprocessing \\n ' ''' #import csv from pandas.plotting import scatter_matrix from sklearn.ensemble import RandomForestRegressor from sklearn.neural_network import MLPRegressor from sklearn.model_selection import train_test_split ''' ' \\n #import csv \\n from pandas.plotting import scatter_matrix \\n\\n from sklearn.ensemble import RandomForestRegressor \\n\\n from sklearn.neural_network import MLPRegressor \\n from sklearn.model_selection import train_test_split \\n ' Extra\u00e7\u00e3o, Transforma\u00e7\u00e3o, Carregamento e Limpeza dos Dados # Refer\u00eancia para os dados: https://www.kaggle.com/noaa/hurricane-database # O foco principal do trabalho se dar\u00e1 nos dados do atl\u00e2ntico data_atl = pd . read_csv ( 'data/atlantic.csv' ) data_pac = pd . read_csv ( 'data/pacific.csv' ) # formatando dados de data data_atl [ 'Date_c' ] = pd . to_datetime ( data_atl [ 'Date' ] . astype ( str ), format = '%Y%m %d ' ) data_atl [ 'Year' ] = pd . DatetimeIndex ( data_atl [ 'Date_c' ]) . year data_atl [ 'Month' ] = pd . DatetimeIndex ( data_atl [ 'Date_c' ]) . month data_atl [ 'Day' ] = pd . DatetimeIndex ( data_atl [ 'Date_c' ]) . day print ( data_atl . columns ) data_atl . head () Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Low Wind NE', 'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE', 'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW', 'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW', 'Date_c', 'Year', 'Month', 'Day'], dtype='object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW Date_c Year Month Day 0 AL011851 UNNAMED 18510625 0 HU 28.0N 94.8W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 1 AL011851 UNNAMED 18510625 600 HU 28.0N 95.4W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 2 AL011851 UNNAMED 18510625 1200 HU 28.0N 96.0W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 3 AL011851 UNNAMED 18510625 1800 HU 28.1N 96.5W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 4 AL011851 UNNAMED 18510625 2100 L HU 28.2N 96.8W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 5 rows \u00d7 26 columns # Registro de Furac\u00f5es \u00e9 maior em determinada \u00e9poca do ano print ( data_atl . groupby ([ 'Month' ])[ 'ID' ] . count ()) Month 1 132 2 13 3 14 4 81 5 655 6 2349 7 3262 8 10857 9 18926 10 9802 11 2548 12 466 Name: ID, dtype: int64 # Per\u00edodo de furac\u00f5es se concentra entre os meses de julho e novembro. # Isso deve estar relacionado com o per\u00edodo anterior (ver\u00e3o do hemisf\u00e9rio norte, onde acontece o maior n\u00famero de casos) # O aquecimento das \u00e1guas est\u00e1 intimamente ligado com a forma\u00e7\u00e3o das massas de ar que ocasionam os furac\u00f5es # Isso nos d\u00e1 uma pista da forma de correla\u00e7\u00e3o temporal que devemos buscar para predizer os eventos fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) fig . suptitle ( 'N\u00famero de fura\u00e7\u00f5es registrados por m\u00eas do ano' , fontsize = 28 , y = 1.06 ) ax . plot ( data_atl . groupby ([ 'Month' ])[ 'Month' ] . mean (), data_atl . groupby ([ 'Month' ])[ 'ID' ] . count (), ls = '--' , label = r '$Furac\u00f5es$ $=$ $0$' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de registros (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$M\u00eas$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) data_atl_ext = data_atl . copy () data_atl_mwmp = data_atl . copy () data_atl_mw = data_atl . copy () ind_nan_ext = [] ind_nan_mwmp = [] ind_nan_mw = [] for l in range ( len ( data_atl )): if ( data_atl_mw [ 'Maximum Wind' ][ l ] < 0 ): ind_nan_mw . append ( l ) ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( data_atl_mwmp [ 'Minimum Pressure' ][ l ] < 0 ): ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( min ( data_atl_ext [ 'Low Wind NE' ][ l ], data_atl_ext [ 'Low Wind SE' ][ l ], data_atl_ext [ 'Low Wind SW' ][ l ], data_atl_ext [ 'Low Wind NW' ][ l ], data_atl_ext [ 'Moderate Wind NE' ][ l ], data_atl_ext [ 'Moderate Wind SE' ][ l ], data_atl_ext [ 'Moderate Wind SW' ][ l ], data_atl_ext [ 'Moderate Wind NW' ][ l ], data_atl_ext [ 'High Wind NE' ][ l ], data_atl_ext [ 'High Wind SE' ][ l ], data_atl_ext [ 'High Wind SW' ][ l ], data_atl_ext [ 'High Wind NW' ][ l ]) < 0 ): ind_nan_ext . append ( l ) data_atl_ext = data_atl_ext . drop ( ind_nan_ext , 0 ) data_atl_mwmp = data_atl_mwmp . drop ( ind_nan_mwmp , 0 ) data_atl_mw = data_atl_mw . drop ( ind_nan_mw , 0 ) print ( len ( data_atl_ext )) print ( len ( data_atl_mwmp )) print ( len ( data_atl_mw )) 5921 18436 48767 fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . scatter ( data_atl_mwmp [ 'Minimum Pressure' ], data_atl_mwmp [ 'Maximum Wind' ], alpha = 0.2 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Press\u00e3o M\u00ednima$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) No handles with labels found to put in legend. def format_lat ( latitude ): new_lat = [] for lat in latitude : if lat [ - 1 ] == \"N\" : new_lat . append ( float ( lat [ 0 : - 1 ])) else : new_lat . append ( - float ( lat [ 0 : - 1 ])) return new_lat def format_lon ( longitude ): new_lon = [] for lon in longitude : if lon [ - 1 ] == \"E\" : new_lon . append ( float ( lon [ 0 : - 1 ])) else : new_lon . append ( - float ( lon [ 0 : - 1 ])) return new_lon #format_lat(data_atl['Latitude']) data_atl [ 'Latitude_c' ] = format_lat ( data_atl [ 'Latitude' ]) data_atl [ 'Longitude_c' ] = format_lat ( data_atl [ 'Longitude' ]) # Todas as Latitudes s\u00e3o menores que 0, e as Longitudes maiores que 0 #print(data_atl[data_atl['Latitude_c'] < 0]) #print(data_atl[data_atl['Longitude_c'] > 0]) #N\u00famero de registro de fura\u00e7\u00f5es tem crescido, mas isso se deve \u00e0 maior capacidade de detec\u00e7\u00e3o com o passar dos anos X_train = data_atl . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) #plt.plot(data_atl.groupby(['Year'])['ID'].count()) No handles with labels found to put in legend . R ^ 2 _train = 0 . 40918060978408977 Par\u00e2metro_const = - 3940 . 3491393512004 Par\u00e2metro_Year = 2 . 192423797184305 #Velocidade m\u00e1xima de vento sustentada pelos furac\u00f5es reduziu, mas isso tamb\u00e9m se deve \u00e0 maior capacidade de registro #de eventos de pequeno porte X_train = data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) #plt.plot(data_atl_mw.groupby(['Year'])['Year'].mean(), data_atl_mw.groupby(['Year'])['Maximum Wind'].mean()) No handles with labels found to put in legend . R ^ 2 _train = 0 . 49772308255924613 Par\u00e2metro_const = 345 . 35295844916755 Par\u00e2metro_Year = - 0 . 15025433346159117 #Para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte data_atl_fdur = data_atl_mw . copy () duration = data_atl_mw . groupby ([ 'ID' ])[ 'Date_c' ] . max () - data_atl_mw . groupby ([ 'ID' ])[ 'Date_c' ] . min () duration . name = 'Duration' #print(duration) data_atl_fdur = pd . merge ( data_atl_fdur , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_fdur [ 'Duration' ] = pd . to_numeric ( data_atl_fdur [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_fdur = data_atl_fdur [ data_atl_fdur [ 'Duration' ] > 2 ] print ( len ( data_atl_fdur )) 46350 #Para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte (somente a partir de Tropical Storm) data_atl_fwind = data_atl_fdur . copy () data_atl_fwind = data_atl_fwind [ data_atl_fwind [ 'Maximum Wind' ] > 34 ] print ( len ( data_atl_fwind )) 35696 # Com o novo filtro, o vi\u00e9s do aumento no n\u00famero de furac\u00f5es ao longo dos anos reduziu, mas ainda h\u00e1 um aumento # Isso mostra que essa tend\u00eancia pode ser algo n\u00e3o viesada, e que gera preocupa\u00e7\u00e3o pelo futuro X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) No handles with labels found to put in legend . R ^ 2 _train = 0 . 15573477903937416 Par\u00e2metro_const = - 1661 . 4327723310093 Par\u00e2metro_Year = 0 . 9714289530628062 # Com o novo filtro, o vi\u00e9s da redu\u00e7\u00e3o da velocidade m\u00e1xima sustentada de vento reduziu, quase para o n\u00edvel constante # Isso pode significar que os filtros est\u00e3o relativamente bem adequados para retirada do vi\u00e9s inicial dos dados X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) #plt.plot(data_atl_mw.groupby(['Year'])['Year'].mean(), data_atl_mw.groupby(['Year'])['Maximum Wind'].mean()) No handles with labels found to put in legend . R ^ 2 _train = 0 . 11158706746149893 Par\u00e2metro_const = 167 . 44362825887168 Par\u00e2metro_Year = - 0 . 054709821748082546 data_atl_mw2 = data_atl_mw . copy () data_atl_mw2 [ 'Latitude_c' ] = format_lat ( data_atl_mw [ 'Latitude' ]) data_atl_mw2 [ 'Longitude_c' ] = format_lat ( data_atl_mw [ 'Longitude' ]) data_atl_mw2 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... High Wind NE High Wind SE High Wind SW High Wind NW Date_c Year Month Day Latitude_c Longitude_c 0 AL011851 UNNAMED 18510625 0 HU 28.0N 94.8W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.0 -94.8 1 AL011851 UNNAMED 18510625 600 HU 28.0N 95.4W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.0 -95.4 2 AL011851 UNNAMED 18510625 1200 HU 28.0N 96.0W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.0 -96.0 3 AL011851 UNNAMED 18510625 1800 HU 28.1N 96.5W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.1 -96.5 4 AL011851 UNNAMED 18510625 2100 L HU 28.2N 96.8W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.2 -96.8 5 rows \u00d7 28 columns #print(data_atl_mw2.groupby(['ID'], as_index=False)['ID', 'Latitude_c'].first()) data_atl_mw2_filtrado3 = data_atl_mw2 . copy () Lat_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Latitude_c' ] . first () Lat_min . name = 'Lat_min' #print(Lat_min) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lat_min , how = 'inner' , left_on = 'ID' , right_index = True ) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ abs ( data_atl_mw2_filtrado3 [ 'Lat_min' ] - 12.5 ) > 0 ] #data_atl_mw2_filtrado3.head() Lon_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Longitude_c' ] . min () Lon_min . name = 'Lon_min' #print(Lon_min) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lon_min , how = 'inner' , left_on = 'ID' , right_index = True ) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Lon_min' ] > - 180 ] Wind_max = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Maximum Wind' ] . max () Wind_max . name = 'Wind_max' #print(Wind_max) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Wind_max , how = 'inner' , on = 'ID' ) #left_on='ID', right_index=True) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Wind_max' ] > 34 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Year' ] > 1950 ] duration = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date_c' ] . max () - data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date_c' ] . min () duration . name = 'Duration' #print(duration) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_mw2_filtrado3 [ 'Duration' ] = pd . to_numeric ( data_atl_mw2_filtrado3 [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Duration' ] > 2 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 . drop ([ 'Lat_min' , 'Lon_min' , 'Wind_max' ], 1 ) #data_atl_mw2_filtrado3.head() print ( len ( data_atl_mw2_filtrado3 )) print ( len ( data_atl . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl )) print ( len ( data_atl_mw2 )) data_atl_mw2_filtrado3 . head () 22386 1814 685 49105 48767 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... High Wind SE High Wind SW High Wind NW Date_c Year Month Day Latitude_c Longitude_c Duration 21948 AL011951 UNNAMED 19510102 1200 EX 30.5N 58.0W 50 -999 ... -999 -999 -999 1951-01-02 1951 1 2 30.5 -58.0 10 21949 AL011951 UNNAMED 19510102 1800 EX 29.9N 56.8W 45 -999 ... -999 -999 -999 1951-01-02 1951 1 2 29.9 -56.8 10 21950 AL011951 UNNAMED 19510103 0 EX 29.0N 55.7W 45 -999 ... -999 -999 -999 1951-01-03 1951 1 3 29.0 -55.7 10 21951 AL011951 UNNAMED 19510103 600 EX 27.5N 54.8W 45 -999 ... -999 -999 -999 1951-01-03 1951 1 3 27.5 -54.8 10 21952 AL011951 UNNAMED 19510103 1200 EX 26.5N 54.5W 45 -999 ... -999 -999 -999 1951-01-03 1951 1 3 26.5 -54.5 10 5 rows \u00d7 29 columns data_atl_mw2_filtrado3 . to_csv ( 'data_atl_mw2_filtrado3.csv' , encoding = 'utf-8' , index = False ) Outras Visualiza\u00e7\u00f5es Refer\u00eancia: https://tropycal.github.io/tropycal/examples/index.html hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) storm = hurdat_atl . get_storm (( 'michael' , 2018 )) # storm . plot ( return_ax = True ) # storm . plot_tors ( plotPPH = True ) # storm . plot_nhc_forecast ( forecast = 2 , return_ax = True ) # storm . plot_nhc_forecast ( forecast = 12 , return_ax = True ) # ibtracs = tracks . TrackDataset ( basin = 'all' , source = 'ibtracs' , ibtracs_mode = 'jtwc_neumann' , catarina = True ) # storm = ibtracs . get_storm (( 'catarina' , 2004 )) storm . plot ( return_ax = True ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (12.27 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tracks \\ storm . py : 1239 : UserWarning : Reading in tornado data for this storm . If you seek to analyze tornado data for multiple storms , run \"TrackDataset.assign_storm_tornadoes()\" to avoid this warning in the future . warnings . warn ( warn_message ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (22.54 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Starting to read in ibtracs data --> Completed reading in ibtracs data (269.28 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' < cartopy . mpl . geoaxes . GeoAxesSubplot at 0 x19569dc8 > tor_data = tornado . TornadoDataset () tor_ax , domain , leg_tor = tor_data . plot_tors ( dt . datetime ( 2011 , 4 , 27 ), plotPPH = True , return_ax = True ) tor_ax hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) # hurdat_atl . assign_storm_tornadoes ( dist_thresh = 750 ) # storm = hurdat_atl . get_storm (( 'ivan' , 2004 )) # storm . plot_tors ( plotPPH = True , return_ax = True ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (28.47 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tornado \\ tools . py : 36 : SettingWithCopyWarning : A value is trying to be set on a copy of a slice from a DataFrame . Try using . loc [ row_indexer , col_indexer ] = value instead See the caveats in the documentation : https : // pandas . pydata . org / pandas - docs / stable / user_guide / indexing . html # returning - a - view - versus - a - copy dfTors [ 'SPC_time' ] = dfTors [ 'UTC_time' ] - timedelta ( hours = 12 ) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (8.9 seconds) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (17.77 seconds) --> Starting to assign tornadoes to storms --> Completed assigning tornadoes to storm (516.59 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' < cartopy . mpl . geoaxes . GeoAxesSubplot at 0 x1815d948 > hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) #Retrieve Hurricane Michael from 2018 storm = hurdat_atl . get_storm (( 'michael' , 2018 )) #Retrieve the 2017 Atlantic hurricane season season = hurdat_atl . get_season ( 2017 ) #Printing the Storm object lists relevant data: print ( storm ) print ( hurdat_atl . search_name ( 'Michael' )) # hurdat_atl . plot_storm (( 'michael' , 2018 ), return_ax = True ) # hurdat_atl . ace_climo ( plot_year = 2018 , compare_years = 2017 ) # hurdat_atl . ace_climo ( rolling_sum = 30 , plot_year = 2018 , compare_years = 2017 ) # hurdat_atl . wind_pres_relationship ( storm = ( 'sandy' , 2012 )) # ibtracs = tracks . TrackDataset ( basin = 'all' , source = 'ibtracs' , ibtracs_mode = 'jtwc_neumann' , catarina = True ) # ibtracs . gridded_stats ( request = \"maximum wind\" , return_ax = True ) # ibtracs . gridded_stats ( request = \"number of storms\" , thresh = { 'dv_min' : 30 }, prop = { 'cmap' : 'plasma_r' }) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (10.66 seconds) < tropycal . tracks . Storm > Storm Summary : Maximum Wind : 140 knots Minimum Pressure : 919 hPa Start Date : 0600 UTC 07 October 2018 End Date : 1800 UTC 11 October 2018 Variables : date ( datetime ) [ 2018 - 10 - 06 18 : 00 : 00 .... 2018 - 10 - 15 18 : 00 : 00 ] extra_obs ( int32 ) [ 0 .... 0 ] special ( str ) [ .... ] type ( str ) [ LO .... EX ] lat ( float64 ) [ 17 . 8 .... 41 . 2 ] lon ( float64 ) [ - 86 . 6 .... - 10 . 0 ] vmax ( int32 ) [ 25 .... 35 ] mslp ( int32 ) [ 1006 .... 1001 ] wmo_basin ( str ) [ north_atlantic .... north_atlantic ] More Information : id : AL142018 operational_id : AL142018 name : MICHAEL year : 2018 season : 2018 basin : north_atlantic source_info : NHC Hurricane Database source : hurdat ace : 12 . 5 realtime : False [ 2000 , 2012 , 2018 ] C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Starting to read in ibtracs data --> Completed reading in ibtracs data (518.89 seconds) --> Getting filtered storm tracks --> Grouping by lat/lon/storm C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tracks \\ tools . py : 101 : RuntimeWarning : All - NaN slice encountered return thresh , lambda x : np . nanmax ( x ) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tracks \\ tools . py : 101 : RuntimeWarning : All - NaN axis encountered return thresh , lambda x : np . nanmax ( x ) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Generating plot --> Getting filtered storm tracks --> Grouping by lat/lon/storm --> Generating plot","title":"EDA"},{"location":"EDA_weather_hurricane_cleaning_and_filtering/#eda","text":"","title":"EDA"},{"location":"EDA_weather_hurricane_cleaning_and_filtering/#instalacao-e-importacao-de-pacotes","text":"''' !apt-get install libgeos-3.5.0 !apt-get install libgeos-dev !pip install https://github.com/matplotlib/basemap/archive/master.zip !pip install pyproj==1.9.6 ''' '\\n!apt-get install libgeos-3.5.0\\n!apt-get install libgeos-dev\\n!pip install https://github.com/matplotlib/basemap/archive/master.zip\\n!pip install pyproj==1.9.6\\n' ''' !pip install netcdf4 ''' '\\n!pip install netcdf4\\n' ''' Dependencies for Tropycal Package: https://pypi.org/project/tropycal/ matplotlib >= 2.2.2 numpy >= 1.14.3 scipy >= 1.1.0 pandas >= 0.23.0 geopy >= 1.18.1 xarray >= 0.10.7 networkx >= 2.0.0 requests >= 2.22.0 To fully leverage tropycal's plotting capabilities, it is strongly recommended to have cartopy >= 0.17.0 installed. !pip install numpy !pip install pandas !pip install matplotlib !pip install scipy #!pip uninstall geopy !pip install geopy !pip install xarray !pip install networkx !pip install requests !pip install proj !pip install proj-data !pip install geos #!pip uninstall cartopy !apt-get -qq install python-cartopy python3-cartopy !pip install cartopy !pip install tropycal #!pip freeze ''' \"\\nDependencies for Tropycal Package: https://pypi.org/project/tropycal/\\nmatplotlib >= 2.2.2\\nnumpy >= 1.14.3\\nscipy >= 1.1.0\\npandas >= 0.23.0\\ngeopy >= 1.18.1\\nxarray >= 0.10.7\\nnetworkx >= 2.0.0\\nrequests >= 2.22.0\\nTo fully leverage tropycal's plotting capabilities, it is strongly recommended to have cartopy >= 0.17.0 installed.\\n\\n\\n!pip install numpy\\n!pip install pandas\\n!pip install matplotlib\\n!pip install scipy\\n#!pip uninstall geopy\\n!pip install geopy\\n!pip install xarray\\n!pip install networkx\\n!pip install requests\\n!pip install proj\\n!pip install proj-data\\n!pip install geos\\n#!pip uninstall cartopy\\n!apt-get -qq install python-cartopy python3-cartopy\\n!pip install cartopy\\n!pip install tropycal\\n#!pip freeze\\n\" % matplotlib inline import numpy as np import matplotlib import matplotlib.pyplot as plt import matplotlib.colors as c #from mpl_toolkits.basemap import Basemap,shiftgrid import pandas as pd import netCDF4 as nc from itertools import chain from netCDF4 import Dataset from netCDF4 import date2index from datetime import datetime import scipy import geopy import xarray import networkx import requests import cartopy import tropycal import tropycal.tracks as tracks import tropycal.tornado as tornado import datetime as dt import statsmodels.api as sm from statsmodels.api import OLS ''' from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.preprocessing import PolynomialFeatures from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold import math from scipy.special import gamma import seaborn as sns sns.set() alpha = 0.5 from sklearn import preprocessing ''' ' \\n from sklearn.decomposition import PCA \\n from sklearn.linear_model import LogisticRegression \\n from sklearn.linear_model import LogisticRegressionCV \\n from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \\n from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \\n from sklearn.preprocessing import PolynomialFeatures \\n from sklearn.neighbors import KNeighborsClassifier \\n from sklearn.model_selection import cross_val_score \\n from sklearn.metrics import accuracy_score \\n from sklearn.model_selection import KFold \\n\\n import math \\n from scipy.special import gamma \\n\\n import seaborn as sns \\n sns.set() \\n\\n alpha = 0.5 \\n\\n from sklearn import preprocessing \\n ' ''' #import csv from pandas.plotting import scatter_matrix from sklearn.ensemble import RandomForestRegressor from sklearn.neural_network import MLPRegressor from sklearn.model_selection import train_test_split ''' ' \\n #import csv \\n from pandas.plotting import scatter_matrix \\n\\n from sklearn.ensemble import RandomForestRegressor \\n\\n from sklearn.neural_network import MLPRegressor \\n from sklearn.model_selection import train_test_split \\n '","title":"Instala\u00e7\u00e3o e Importa\u00e7\u00e3o de pacotes"},{"location":"EDA_weather_hurricane_cleaning_and_filtering/#extracao-transformacao-carregamento-e-limpeza-dos-dados","text":"# Refer\u00eancia para os dados: https://www.kaggle.com/noaa/hurricane-database # O foco principal do trabalho se dar\u00e1 nos dados do atl\u00e2ntico data_atl = pd . read_csv ( 'data/atlantic.csv' ) data_pac = pd . read_csv ( 'data/pacific.csv' ) # formatando dados de data data_atl [ 'Date_c' ] = pd . to_datetime ( data_atl [ 'Date' ] . astype ( str ), format = '%Y%m %d ' ) data_atl [ 'Year' ] = pd . DatetimeIndex ( data_atl [ 'Date_c' ]) . year data_atl [ 'Month' ] = pd . DatetimeIndex ( data_atl [ 'Date_c' ]) . month data_atl [ 'Day' ] = pd . DatetimeIndex ( data_atl [ 'Date_c' ]) . day print ( data_atl . columns ) data_atl . head () Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Low Wind NE', 'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE', 'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW', 'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW', 'Date_c', 'Year', 'Month', 'Day'], dtype='object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW Date_c Year Month Day 0 AL011851 UNNAMED 18510625 0 HU 28.0N 94.8W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 1 AL011851 UNNAMED 18510625 600 HU 28.0N 95.4W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 2 AL011851 UNNAMED 18510625 1200 HU 28.0N 96.0W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 3 AL011851 UNNAMED 18510625 1800 HU 28.1N 96.5W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 4 AL011851 UNNAMED 18510625 2100 L HU 28.2N 96.8W 80 -999 ... -999 -999 -999 -999 -999 -999 1851-06-25 1851 6 25 5 rows \u00d7 26 columns # Registro de Furac\u00f5es \u00e9 maior em determinada \u00e9poca do ano print ( data_atl . groupby ([ 'Month' ])[ 'ID' ] . count ()) Month 1 132 2 13 3 14 4 81 5 655 6 2349 7 3262 8 10857 9 18926 10 9802 11 2548 12 466 Name: ID, dtype: int64 # Per\u00edodo de furac\u00f5es se concentra entre os meses de julho e novembro. # Isso deve estar relacionado com o per\u00edodo anterior (ver\u00e3o do hemisf\u00e9rio norte, onde acontece o maior n\u00famero de casos) # O aquecimento das \u00e1guas est\u00e1 intimamente ligado com a forma\u00e7\u00e3o das massas de ar que ocasionam os furac\u00f5es # Isso nos d\u00e1 uma pista da forma de correla\u00e7\u00e3o temporal que devemos buscar para predizer os eventos fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) fig . suptitle ( 'N\u00famero de fura\u00e7\u00f5es registrados por m\u00eas do ano' , fontsize = 28 , y = 1.06 ) ax . plot ( data_atl . groupby ([ 'Month' ])[ 'Month' ] . mean (), data_atl . groupby ([ 'Month' ])[ 'ID' ] . count (), ls = '--' , label = r '$Furac\u00f5es$ $=$ $0$' ) ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de registros (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$M\u00eas$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) data_atl_ext = data_atl . copy () data_atl_mwmp = data_atl . copy () data_atl_mw = data_atl . copy () ind_nan_ext = [] ind_nan_mwmp = [] ind_nan_mw = [] for l in range ( len ( data_atl )): if ( data_atl_mw [ 'Maximum Wind' ][ l ] < 0 ): ind_nan_mw . append ( l ) ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( data_atl_mwmp [ 'Minimum Pressure' ][ l ] < 0 ): ind_nan_mwmp . append ( l ) ind_nan_ext . append ( l ) elif ( min ( data_atl_ext [ 'Low Wind NE' ][ l ], data_atl_ext [ 'Low Wind SE' ][ l ], data_atl_ext [ 'Low Wind SW' ][ l ], data_atl_ext [ 'Low Wind NW' ][ l ], data_atl_ext [ 'Moderate Wind NE' ][ l ], data_atl_ext [ 'Moderate Wind SE' ][ l ], data_atl_ext [ 'Moderate Wind SW' ][ l ], data_atl_ext [ 'Moderate Wind NW' ][ l ], data_atl_ext [ 'High Wind NE' ][ l ], data_atl_ext [ 'High Wind SE' ][ l ], data_atl_ext [ 'High Wind SW' ][ l ], data_atl_ext [ 'High Wind NW' ][ l ]) < 0 ): ind_nan_ext . append ( l ) data_atl_ext = data_atl_ext . drop ( ind_nan_ext , 0 ) data_atl_mwmp = data_atl_mwmp . drop ( ind_nan_mwmp , 0 ) data_atl_mw = data_atl_mw . drop ( ind_nan_mw , 0 ) print ( len ( data_atl_ext )) print ( len ( data_atl_mwmp )) print ( len ( data_atl_mw )) 5921 18436 48767 fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . scatter ( data_atl_mwmp [ 'Minimum Pressure' ], data_atl_mwmp [ 'Maximum Wind' ], alpha = 0.2 , ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Press\u00e3o M\u00ednima$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) No handles with labels found to put in legend. def format_lat ( latitude ): new_lat = [] for lat in latitude : if lat [ - 1 ] == \"N\" : new_lat . append ( float ( lat [ 0 : - 1 ])) else : new_lat . append ( - float ( lat [ 0 : - 1 ])) return new_lat def format_lon ( longitude ): new_lon = [] for lon in longitude : if lon [ - 1 ] == \"E\" : new_lon . append ( float ( lon [ 0 : - 1 ])) else : new_lon . append ( - float ( lon [ 0 : - 1 ])) return new_lon #format_lat(data_atl['Latitude']) data_atl [ 'Latitude_c' ] = format_lat ( data_atl [ 'Latitude' ]) data_atl [ 'Longitude_c' ] = format_lat ( data_atl [ 'Longitude' ]) # Todas as Latitudes s\u00e3o menores que 0, e as Longitudes maiores que 0 #print(data_atl[data_atl['Latitude_c'] < 0]) #print(data_atl[data_atl['Longitude_c'] > 0]) #N\u00famero de registro de fura\u00e7\u00f5es tem crescido, mas isso se deve \u00e0 maior capacidade de detec\u00e7\u00e3o com o passar dos anos X_train = data_atl . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) #plt.plot(data_atl.groupby(['Year'])['ID'].count()) No handles with labels found to put in legend . R ^ 2 _train = 0 . 40918060978408977 Par\u00e2metro_const = - 3940 . 3491393512004 Par\u00e2metro_Year = 2 . 192423797184305 #Velocidade m\u00e1xima de vento sustentada pelos furac\u00f5es reduziu, mas isso tamb\u00e9m se deve \u00e0 maior capacidade de registro #de eventos de pequeno porte X_train = data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl_mw . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_mw . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) #plt.plot(data_atl_mw.groupby(['Year'])['Year'].mean(), data_atl_mw.groupby(['Year'])['Maximum Wind'].mean()) No handles with labels found to put in legend . R ^ 2 _train = 0 . 49772308255924613 Par\u00e2metro_const = 345 . 35295844916755 Par\u00e2metro_Year = - 0 . 15025433346159117 #Para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte data_atl_fdur = data_atl_mw . copy () duration = data_atl_mw . groupby ([ 'ID' ])[ 'Date_c' ] . max () - data_atl_mw . groupby ([ 'ID' ])[ 'Date_c' ] . min () duration . name = 'Duration' #print(duration) data_atl_fdur = pd . merge ( data_atl_fdur , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_fdur [ 'Duration' ] = pd . to_numeric ( data_atl_fdur [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_fdur = data_atl_fdur [ data_atl_fdur [ 'Duration' ] > 2 ] print ( len ( data_atl_fdur )) 46350 #Para n\u00e3o enviesar nossos dados, filtraremos os registros de pequeno porte (somente a partir de Tropical Storm) data_atl_fwind = data_atl_fdur . copy () data_atl_fwind = data_atl_fwind [ data_atl_fwind [ 'Maximum Wind' ] > 34 ] print ( len ( data_atl_fwind )) 35696 # Com o novo filtro, o vi\u00e9s do aumento no n\u00famero de furac\u00f5es ao longo dos anos reduziu, mas ainda h\u00e1 um aumento # Isso mostra que essa tend\u00eancia pode ser algo n\u00e3o viesada, e que gera preocupa\u00e7\u00e3o pelo futuro X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'ID' ] . count (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'N\u00famero de Furac\u00f5es Anuais (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Quantidade$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) No handles with labels found to put in legend . R ^ 2 _train = 0 . 15573477903937416 Par\u00e2metro_const = - 1661 . 4327723310093 Par\u00e2metro_Year = 0 . 9714289530628062 # Com o novo filtro, o vi\u00e9s da redu\u00e7\u00e3o da velocidade m\u00e1xima sustentada de vento reduziu, quase para o n\u00edvel constante # Isso pode significar que os filtros est\u00e3o relativamente bem adequados para retirada do vi\u00e9s inicial dos dados X_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean () y_train = data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean () X_train2 = sm . add_constant ( X_train ) #np.array(X_train).reshape(X_train.shape[0],1) OLS_obj = OLS ( y_train , X_train2 ) OLSModel = OLS_obj . fit () r2_train = OLSModel . rsquared #r2_test = 1 - ((OLSModel.predict(X_test2)-y_test)*(OLSModel.predict(X_test2)-y_test)).sum() / ((y_test.mean()-y_test)*(y_test.mean()-y_test)).sum() print ( f 'R^2_train = { r2_train } ' ) #print(f'R^2_test = {r2_test}') print ( f 'Par\u00e2metro_const = { OLSModel . params [ 0 ] } ' ) print ( f 'Par\u00e2metro_Year = { OLSModel . params [ 1 ] } ' ) w0 = OLSModel . params [ 0 ] + 1850 * OLSModel . params [ 1 ] w1 = OLSModel . params [ 0 ] + 2015 * OLSModel . params [ 1 ] fig , ax = plt . subplots ( 1 , 1 ) #, figsize=(16,10)) #fig.suptitle('Velocidade M\u00e1xima vs Press\u00e3o M\u00ednima (1851-2015)', fontsize=28, y=1.06) ax . plot ( data_atl_fwind . groupby ([ 'Year' ])[ 'Year' ] . mean (), data_atl_fwind . groupby ([ 'Year' ])[ 'Maximum Wind' ] . mean (), ls = '--' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . plot ([ 1850 , 2015 ], [ w0 , w1 ], ls = '-.' ) #, label=r'$Furac\u00f5es$ $=$ $0$') ax . tick_params ( labelsize = 24 ) ax . set_title ( f 'Velocidade M\u00e1xima vs Ano (1851-2015)' , fontsize = 24 ) ax . set_xlabel ( r '$Ano$' , fontsize = 16 ) ax . set_ylabel ( r '$Velocidade M\u00e1xima$' , fontsize = 16 ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . set_figheight ( 5 ) fig . set_figwidth ( 20 ) fig . tight_layout ( pad = 2.0 ) #plt.plot(data_atl_mw.groupby(['Year'])['Year'].mean(), data_atl_mw.groupby(['Year'])['Maximum Wind'].mean()) No handles with labels found to put in legend . R ^ 2 _train = 0 . 11158706746149893 Par\u00e2metro_const = 167 . 44362825887168 Par\u00e2metro_Year = - 0 . 054709821748082546 data_atl_mw2 = data_atl_mw . copy () data_atl_mw2 [ 'Latitude_c' ] = format_lat ( data_atl_mw [ 'Latitude' ]) data_atl_mw2 [ 'Longitude_c' ] = format_lat ( data_atl_mw [ 'Longitude' ]) data_atl_mw2 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... High Wind NE High Wind SE High Wind SW High Wind NW Date_c Year Month Day Latitude_c Longitude_c 0 AL011851 UNNAMED 18510625 0 HU 28.0N 94.8W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.0 -94.8 1 AL011851 UNNAMED 18510625 600 HU 28.0N 95.4W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.0 -95.4 2 AL011851 UNNAMED 18510625 1200 HU 28.0N 96.0W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.0 -96.0 3 AL011851 UNNAMED 18510625 1800 HU 28.1N 96.5W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.1 -96.5 4 AL011851 UNNAMED 18510625 2100 L HU 28.2N 96.8W 80 -999 ... -999 -999 -999 -999 1851-06-25 1851 6 25 28.2 -96.8 5 rows \u00d7 28 columns #print(data_atl_mw2.groupby(['ID'], as_index=False)['ID', 'Latitude_c'].first()) data_atl_mw2_filtrado3 = data_atl_mw2 . copy () Lat_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Latitude_c' ] . first () Lat_min . name = 'Lat_min' #print(Lat_min) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lat_min , how = 'inner' , left_on = 'ID' , right_index = True ) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ abs ( data_atl_mw2_filtrado3 [ 'Lat_min' ] - 12.5 ) > 0 ] #data_atl_mw2_filtrado3.head() Lon_min = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Longitude_c' ] . min () Lon_min . name = 'Lon_min' #print(Lon_min) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Lon_min , how = 'inner' , left_on = 'ID' , right_index = True ) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Lon_min' ] > - 180 ] Wind_max = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Maximum Wind' ] . max () Wind_max . name = 'Wind_max' #print(Wind_max) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , Wind_max , how = 'inner' , on = 'ID' ) #left_on='ID', right_index=True) #print(data_atl_mw2_filtrado3) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Wind_max' ] > 34 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Year' ] > 1950 ] duration = data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date_c' ] . max () - data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'Date_c' ] . min () duration . name = 'Duration' #print(duration) data_atl_mw2_filtrado3 = pd . merge ( data_atl_mw2_filtrado3 , duration , how = 'inner' , left_on = 'ID' , right_index = True ) data_atl_mw2_filtrado3 [ 'Duration' ] = pd . to_numeric ( data_atl_mw2_filtrado3 [ 'Duration' ] . dt . days , downcast = 'integer' ) data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 [ data_atl_mw2_filtrado3 [ 'Duration' ] > 2 ] data_atl_mw2_filtrado3 = data_atl_mw2_filtrado3 . drop ([ 'Lat_min' , 'Lon_min' , 'Wind_max' ], 1 ) #data_atl_mw2_filtrado3.head() print ( len ( data_atl_mw2_filtrado3 )) print ( len ( data_atl . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl_mw2_filtrado3 . groupby ([ 'ID' ])[ 'ID' ] . count ())) print ( len ( data_atl )) print ( len ( data_atl_mw2 )) data_atl_mw2_filtrado3 . head () 22386 1814 685 49105 48767 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... High Wind SE High Wind SW High Wind NW Date_c Year Month Day Latitude_c Longitude_c Duration 21948 AL011951 UNNAMED 19510102 1200 EX 30.5N 58.0W 50 -999 ... -999 -999 -999 1951-01-02 1951 1 2 30.5 -58.0 10 21949 AL011951 UNNAMED 19510102 1800 EX 29.9N 56.8W 45 -999 ... -999 -999 -999 1951-01-02 1951 1 2 29.9 -56.8 10 21950 AL011951 UNNAMED 19510103 0 EX 29.0N 55.7W 45 -999 ... -999 -999 -999 1951-01-03 1951 1 3 29.0 -55.7 10 21951 AL011951 UNNAMED 19510103 600 EX 27.5N 54.8W 45 -999 ... -999 -999 -999 1951-01-03 1951 1 3 27.5 -54.8 10 21952 AL011951 UNNAMED 19510103 1200 EX 26.5N 54.5W 45 -999 ... -999 -999 -999 1951-01-03 1951 1 3 26.5 -54.5 10 5 rows \u00d7 29 columns data_atl_mw2_filtrado3 . to_csv ( 'data_atl_mw2_filtrado3.csv' , encoding = 'utf-8' , index = False )","title":"Extra\u00e7\u00e3o, Transforma\u00e7\u00e3o, Carregamento e Limpeza dos Dados"},{"location":"EDA_weather_hurricane_cleaning_and_filtering/#outras-visualizacoes","text":"Refer\u00eancia: https://tropycal.github.io/tropycal/examples/index.html hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) storm = hurdat_atl . get_storm (( 'michael' , 2018 )) # storm . plot ( return_ax = True ) # storm . plot_tors ( plotPPH = True ) # storm . plot_nhc_forecast ( forecast = 2 , return_ax = True ) # storm . plot_nhc_forecast ( forecast = 12 , return_ax = True ) # ibtracs = tracks . TrackDataset ( basin = 'all' , source = 'ibtracs' , ibtracs_mode = 'jtwc_neumann' , catarina = True ) # storm = ibtracs . get_storm (( 'catarina' , 2004 )) storm . plot ( return_ax = True ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (12.27 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tracks \\ storm . py : 1239 : UserWarning : Reading in tornado data for this storm . If you seek to analyze tornado data for multiple storms , run \"TrackDataset.assign_storm_tornadoes()\" to avoid this warning in the future . warnings . warn ( warn_message ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (22.54 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Starting to read in ibtracs data --> Completed reading in ibtracs data (269.28 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' < cartopy . mpl . geoaxes . GeoAxesSubplot at 0 x19569dc8 > tor_data = tornado . TornadoDataset () tor_ax , domain , leg_tor = tor_data . plot_tors ( dt . datetime ( 2011 , 4 , 27 ), plotPPH = True , return_ax = True ) tor_ax hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) # hurdat_atl . assign_storm_tornadoes ( dist_thresh = 750 ) # storm = hurdat_atl . get_storm (( 'ivan' , 2004 )) # storm . plot_tors ( plotPPH = True , return_ax = True ) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (28.47 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tornado \\ tools . py : 36 : SettingWithCopyWarning : A value is trying to be set on a copy of a slice from a DataFrame . Try using . loc [ row_indexer , col_indexer ] = value instead See the caveats in the documentation : https : // pandas . pydata . org / pandas - docs / stable / user_guide / indexing . html # returning - a - view - versus - a - copy dfTors [ 'SPC_time' ] = dfTors [ 'UTC_time' ] - timedelta ( hours = 12 ) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (8.9 seconds) --> Starting to read in tornado track data --> Completed reading in tornado data for 1950-2018 (17.77 seconds) --> Starting to assign tornadoes to storms --> Completed assigning tornadoes to storm (516.59 seconds) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' < cartopy . mpl . geoaxes . GeoAxesSubplot at 0 x1815d948 > hurdat_atl = tracks . TrackDataset ( basin = 'north_atlantic' , source = 'hurdat' , include_btk = False ) #Retrieve Hurricane Michael from 2018 storm = hurdat_atl . get_storm (( 'michael' , 2018 )) #Retrieve the 2017 Atlantic hurricane season season = hurdat_atl . get_season ( 2017 ) #Printing the Storm object lists relevant data: print ( storm ) print ( hurdat_atl . search_name ( 'Michael' )) # hurdat_atl . plot_storm (( 'michael' , 2018 ), return_ax = True ) # hurdat_atl . ace_climo ( plot_year = 2018 , compare_years = 2017 ) # hurdat_atl . ace_climo ( rolling_sum = 30 , plot_year = 2018 , compare_years = 2017 ) # hurdat_atl . wind_pres_relationship ( storm = ( 'sandy' , 2012 )) # ibtracs = tracks . TrackDataset ( basin = 'all' , source = 'ibtracs' , ibtracs_mode = 'jtwc_neumann' , catarina = True ) # ibtracs . gridded_stats ( request = \"maximum wind\" , return_ax = True ) # ibtracs . gridded_stats ( request = \"number of storms\" , thresh = { 'dv_min' : 30 }, prop = { 'cmap' : 'plasma_r' }) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (10.66 seconds) < tropycal . tracks . Storm > Storm Summary : Maximum Wind : 140 knots Minimum Pressure : 919 hPa Start Date : 0600 UTC 07 October 2018 End Date : 1800 UTC 11 October 2018 Variables : date ( datetime ) [ 2018 - 10 - 06 18 : 00 : 00 .... 2018 - 10 - 15 18 : 00 : 00 ] extra_obs ( int32 ) [ 0 .... 0 ] special ( str ) [ .... ] type ( str ) [ LO .... EX ] lat ( float64 ) [ 17 . 8 .... 41 . 2 ] lon ( float64 ) [ - 86 . 6 .... - 10 . 0 ] vmax ( int32 ) [ 25 .... 35 ] mslp ( int32 ) [ 1006 .... 1001 ] wmo_basin ( str ) [ north_atlantic .... north_atlantic ] More Information : id : AL142018 operational_id : AL142018 name : MICHAEL year : 2018 season : 2018 basin : north_atlantic source_info : NHC Hurricane Database source : hurdat ace : 12 . 5 realtime : False [ 2000 , 2012 , 2018 ] C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Starting to read in ibtracs data --> Completed reading in ibtracs data (518.89 seconds) --> Getting filtered storm tracks --> Grouping by lat/lon/storm C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tracks \\ tools . py : 101 : RuntimeWarning : All - NaN slice encountered return thresh , lambda x : np . nanmax ( x ) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ tropycal \\ tracks \\ tools . py : 101 : RuntimeWarning : All - NaN axis encountered return thresh , lambda x : np . nanmax ( x ) C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 307 : UserWarning : The . xlabels_top attribute is deprecated . Please use . top_labels to toggle visibility instead . warnings . warn ( 'The .xlabels_top attribute is deprecated. Please ' C : \\ Users \\ User \\ anaconda3 \\ lib \\ site - packages \\ cartopy \\ mpl \\ gridliner . py : 343 : UserWarning : The . ylabels_right attribute is deprecated . Please use . right_labels to toggle visibility instead . warnings . warn ( 'The .ylabels_right attribute is deprecated. Please ' --> Generating plot --> Getting filtered storm tracks --> Grouping by lat/lon/storm --> Generating plot","title":"Outras Visualiza\u00e7\u00f5es"},{"location":"Exploratory_Data_Analysis/","text":"Exploratory Data Analysis Group: Alysson Esp\u00edndola de S\u00e1 Silveira, Rener de Souza Oliveira, Yuri Luis Faria Silva # !apt-get install libgeos-3.5.0 # !apt-get install libgeos-dev # !pip install https://github.com/matplotlib/basemap/archive/master.zip # !pip install pyproj==1.9.6 # !pip install netCDF4 # !pip install jinja2 # !pip install geopy # !pip install xarray # !pip install networkx # !pip install requests # !pip install cartopy # !pip install tropycal #Uncomment the above lines and execute this cell; then go to \"Runtime\"->\"Restart Runtime\". % matplotlib inline import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as c from mpl_toolkits.basemap import Basemap , shiftgrid import pandas as pd import netCDF4 as nc 1 - Missing Data - netCDF4 This part of our data is in .nc file extension, which stands for netCDF4. It's widely used in climatology and has python support for reading it. sst_mean = nc . Dataset ( 'Datasets/sst.mean.nc' , 'r' ) rhum_mean = nc . Dataset ( 'Datasets/rhum.mean.nc' , 'r' ) wspd_mean = nc . Dataset ( 'Datasets/wspd.mean.nc' , 'r' ) slp_mean = nc . Dataset ( 'Datasets/slp.mean.nc' , 'r' ) vwnd_mean = nc . Dataset ( 'Datasets/vwnd.mean.nc' , 'r' ) cldc_mean = nc . Dataset ( \"Datasets/cldc.mean.nc\" , 'r' ) lats = sst_mean . variables [ 'lat' ][:] lons = sst_mean . variables [ 'lon' ][:] time = sst_mean . variables [ 'time' ][:] sst = sst_mean . variables [ 'sst' ][:,:,:] rhum = rhum_mean . variables [ 'rhum' ][:,:,:] wspd = wspd_mean . variables [ 'wspd' ][:,:,:] slp = slp_mean . variables [ 'slp' ][:,:,:] vwnd = vwnd_mean . variables [ 'vwnd' ][:,:,:] cldc = cldc_mean . variables [ 'cldc' ][:,:,:] sst_mean . close () wspd_mean . close () rhum_mean . close () slp_mean . close () vwnd_mean . close () cldc_mean . close () period = pd . date_range ( start = \"1800-01-01\" , end = \"2020-07-01\" , freq = \"MS\" ) . to_pydatetime () . tolist () def get_missing ( data : list , labels : list ) -> dict : missing = {} lenght = data [ 0 ] . shape [ 0 ] for j , item in enumerate ( data ): missing [ labels [ j ]] = [] for i in range ( lenght ): missing [ labels [ j ]] . append ( 100 * np . sum ( item [ i ] . mask ) / item [ i ] . data . size ) return missing missing = get_missing ([ sst , wspd , rhum , slp , vwnd , cldc ],[ 'sst' , 'wspd' , 'rhum' , 'slp' , 'vwnd' , 'cldc' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Missing Data - Global\" , fontsize = 15 ) ax . set_xlabel ( \"Year\" , fontsize = 12 ) ax . set_ylabel ( \" % o f missing data\" , fontsize = 12 ) for key , value in missing . items (): ax . plot ( period , missing [ key ], label = key ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( 'missing.png' ) plt . show () print ( type ( sst )) <class 'numpy.ma.core.MaskedArray'> Legend: sst: Sea Surface Temperature wspd: Scalar Wind Speed rhum: Relative Humidity slp: Sea Level Pressure vwnd: V-wind component cldc: Cloudiness Since continents represent approximately 29,1\\% of earth surface and the data cover just oceans, The continents are filled with missing values. So, naturally, this percentage is the lower bound of missing data. Note that the data types are \"numpy.ma.core.MaskedArray\" which has an attribute \"mask\", which is an indicator variable of missingness; That's gonna help us deal with it. As we can see in the above plot, we have decades with missingness levels above 90%. Soon, we're gonna analyze focusing on Pacific and North Atlantic, which is our study area. sst_mean = nc . Dataset ( 'Datasets/sst.mean.nc' , 'r' ) x = sst_mean [ 'sst' ][:] plt . hist ( x [:, 70 , 40 ][ x [:, 70 , 40 ] . data < 20000 ]) #testes com masked array (array([ 3., 3., 12., 20., 30., 36., 32., 23., 30., 6.]), array([-0.30999756, 0.36400145, 1.0380005 , 1.7119995 , 2.3859985 , 3.0599976 , 3.7339966 , 4.4079957 , 5.0819945 , 5.755994 , 6.4299927 ], dtype=float32), <a list of 10 Patch objects>) x . shape (2647, 90, 180) sst_at = sst [:, 34 : 40 , 51 : 82 ] #10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W wspd_at = wspd [:, 34 : 40 , 51 : 82 ] rhum_at = rhum [:, 34 : 40 , 51 : 82 ] slp_at = slp [:, 34 : 40 , 51 : 82 ] vwnd_at = vwnd [:, 34 : 40 , 51 : 82 ] cldc_at = cldc [:, 34 : 40 , 51 : 82 ] # sst_pac = sst[:,14:45,0:41] #0\u00b0-60\u00b0N, 100\u00b0W-180\u00b0W # wspd_pac = wspd[:,14:45,0:41] # rhum_pac = rhum[:,14:45,0:41] missing_at = get_missing ([ sst_at , wspd_at , rhum_at , slp_at , vwnd_at , cldc_at ],[ 'sst_at' , 'wspd_at' , 'rhum_at' , 'slp_at' , 'vwnd_at' , 'cldc_at' ]) # missing_pac = get_missing([sst_pac,wspd_pac,rhum_pac],['sst_pac','wspd_pac','rhum_pac']) # fig,(ax,ax1) = plt.subplots(2,1,figsize=(10,8)) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Missing Data - North Atlantic MDR*\" , fontsize = 15 ) ax . set_xlabel ( \"Year\" , fontsize = 12 ) ax . set_ylabel ( \" % o f missing data\" , fontsize = 12 ) for key , value in missing_at . items (): ax . plot ( period , missing_at [ key ], label = key [ 0 : - 3 ]) plt . axvline ( x = period [ 1860 ], label = \"Jan/1955\" , color = 'black' , lw = 3 , ls = '--' ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( 'missing_mdr.png' ) *MDR stands for Main Development Region which refers to the rectangle 10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W. We can see that after the decade 1950-1960, we have more a more complete dataset in our study region. So, to understand the relation of the variables, we're gonna work starting this decade. But, nothing keeps us from working with older data, since values don't vary a lot when it's close; If we wanted to work with long-term trends we could cut the data starting from 1920, and just apply the mean, even with \\approx70 % of missing data. that's because we have an indicator array, which can help with modeling. Also, this percentage is a little lower because of the continental area in the MDR cut considered. Below, we have an example of the distribution of SST data in January 1955. 2 - Visualization #Transforms longitude ranges from [0,360] para [-180,180] --> useful for plot sst [:], lonsn = shiftgrid ( 180 , sst [:], lons , start = False ) wspd [:], lonsn = shiftgrid ( 180 , wspd [:], lons , start = False ) # shum[:],lonsn = shiftgrid(180,shum[:],lons,start=False) rhum [:], lonsn = shiftgrid ( 180 , rhum [:], lons , start = False ) lons = lonsn #Reference: https://annefou.github.io/metos_python/04-plotting/ time_index = 1860 fig = plt . figure ( figsize = [ 12 , 15 ]) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . set_title ( \"sst: {} \" . format ( period [ time_index ] . date ()), fontsize = 16 ) map = Basemap ( projection = 'cyl' , llcrnrlat =- 90 , urcrnrlat = 90 , \\ llcrnrlon =- 180 , urcrnrlon = 180 , resolution = 'c' , ax = ax ) map . drawcoastlines () map . fillcontinents ( color = '#ffe2ab' ) map . drawparallels ( np . arange ( - 90. , 120. , 30. ), labels = [ 1 , 0 , 0 , 0 ]) map . drawmeridians ( np . arange ( - 180. , 180. , 60. ), labels = [ 0 , 0 , 0 , 1 ]) llons , llats = np . meshgrid ( lons , lats ) x , y = map ( llons , llats ) cmap = c . ListedColormap ([ '#35978f' , '#ffffcc' , '#ffeda0' , '#fed976' , '#feb24c' , '#fd8d3c' , '#fc4e2a' , '#e31a1c' , '#bd0026' , '#800026' ]) bounds = list ( np . arange ( - 5 , 37 , 1 )) # bounds=list(np.arange(10,100,5)) norm = c . BoundaryNorm ( bounds , ncolors = cmap . N ) cs = map . contourf ( x , y , sst [ time_index ], cmap = cmap , norm = norm , levels = bounds ) fig . colorbar ( cs , cmap = cmap , norm = norm , boundaries = bounds , ticks = bounds , ax = ax , orientation = 'horizontal' ); A good way to visualize this data is with animation, but the above plot gives us a very good glimpse of what kind of data we're working on. print ( period [ 1788 ]) 1949-01-01 00:00:00 print ( period [ 1860 ]) # -- 1955 January def get_mean ( data ): size = data . shape [ 0 ] new = np . array ([]) for i in range ( size ): new = np . append ( new , np . mean ( data [ i ,:,:])) return new #We're gonna start from the time index 1788, representing January 1949, to match with our PDI dataset. data_at = pd . DataFrame ( get_mean ( sst_at [ 1788 :,:,:]), columns = [ \"sst\" ]) period_df = pd . DataFrame ( period [ 1788 :], columns = [ \"Date\" ]) period_df [ 'Year' ] = period_df . Date . map ( lambda x : x . year ) period_df [ 'Month' ] = period_df . Date . map ( lambda x : x . month ) data_at [ 'rhum' ] = pd . DataFrame ( get_mean ( rhum_at [ 1788 :,:,:]), columns = [ \"rhum\" ]) data_at [ 'slp' ] = pd . DataFrame ( get_mean ( slp_at [ 1788 :,:,:]), columns = [ \"slp\" ]) data_at [ 'wspd' ] = pd . DataFrame ( get_mean ( wspd_at [ 1788 :,:,:]), columns = [ \"wspd\" ]) data_at [ 'vwnd' ] = pd . DataFrame ( get_mean ( vwnd_at [ 1788 :,:,:]), columns = [ \"vwnd\" ]) data_at [ 'cldc' ] = pd . DataFrame ( get_mean ( cldc_at [ 1788 :,:,:]), columns = [ \"cldc\" ]) atlantic_mdr = pd . concat ([ period_df , data_at ], axis = 1 ) cum_sum = {} for i in range ( 1 , 13 ): cum_sum [ i ] = 0 k = 0 #year count for i in range ( 0 , atlantic_mdr . shape [ 0 ] - 12 ): month = atlantic_mdr . iloc [ i ,:] . Month if month % 12 == 1 : k += 1 cum_sum [ month ] += atlantic_mdr . iloc [ i , 3 ] atlantic_mdr . loc [ atlantic_mdr . index [ i ], 'sst_anomaly' ] = atlantic_mdr . iloc [ i , 3 ] - cum_sum [ month ] / k atlantic_mdr . drop ( 'sst_anomaly' , axis = 1 ) . to_csv ( 'Datasets/atlantic_mdr.csv' , index = False ) atlantic_mdr . iloc [ 12 : 24 ,:] 1955-01-01 00:00:00 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Year Month sst rhum slp wspd vwnd cldc sst_anomaly 12 1950-01-01 1950 1 26.618658 80.166064 1012.699219 6.274566 -2.709708 4.139401 0.401114 13 1950-02-01 1950 2 25.996199 78.348542 1012.883875 6.633078 -2.401613 3.965875 0.150126 14 1950-03-01 1950 3 26.414721 80.421051 1013.134516 5.954330 -2.062759 4.087092 0.269065 15 1950-04-01 1950 4 27.401395 81.788231 1011.096354 4.909927 -0.454326 3.146995 0.092152 16 1950-05-01 1950 5 28.472969 80.007482 1010.577563 5.476332 0.516981 4.291411 0.073090 17 1950-06-01 1950 6 28.896216 81.928657 1009.393293 4.850322 2.076482 4.650387 -0.032209 18 1950-07-01 1950 7 29.033083 81.664352 1007.978285 5.042128 2.375750 5.184904 0.050943 19 1950-08-01 1950 8 28.283786 82.927725 1007.856950 4.906136 1.956808 5.109233 -0.403537 20 1950-09-01 1950 9 28.832019 82.908388 1007.869104 4.910579 0.525868 5.305648 0.051078 21 1950-10-01 1950 10 28.090939 80.471020 1010.697798 5.345116 -1.037250 4.423469 -0.236270 22 1950-11-01 1950 11 27.558145 80.553408 1009.745018 6.262806 -2.024746 3.765682 -0.195893 23 1950-12-01 1950 12 26.741317 79.440282 1012.071718 7.539570 -4.750430 4.100716 -0.222622 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 5 )) ax . plot ( np . arange ( 1949 , 2021 , 1 ), atlantic_mdr . groupby ([ 'Year' ]) . agg ({ 'sst_anomaly' : np . mean })[ 'sst_anomaly' ]) ax . set_title ( \"Deviations in sea temperature from the historical average\" , fontsize = 14 ) ax . set_xlabel ( \"Year\" , fontsize = 12 ) ax . set_ylabel ( \"Temperature Deviation\" , fontsize = 12 ); corr = atlantic_mdr . corr () corr . style . background_gradient ( cmap = 'coolwarm' ) #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col0 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col1 { background-color: #9dbdff; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col2 { background-color: #e8d6cc; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col3 { background-color: #b3cdfb; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col4 { background-color: #d8dce2; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col5 { background-color: #c9d7f0; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col6 { background-color: #dfdbd9; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col7 { background-color: #d9dce1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col0 { background-color: #4358cb; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col1 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col2 { background-color: #f49a7b; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col3 { background-color: #f7b99e; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col4 { background-color: #81a4fb; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col5 { background-color: #e5d8d1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col6 { background-color: #f2cbb7; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col7 { background-color: #455cce; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col0 { background-color: #7a9df8; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col1 { background-color: #f7b396; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col2 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col3 { background-color: #f6a385; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col4 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col5 { background-color: #d75445; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col6 { background-color: #f3c7b1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col7 { background-color: #a6c4fe; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col0 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col1 { background-color: #f3c7b1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col2 { background-color: #f39778; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col3 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col4 { background-color: #5d7ce6; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col5 { background-color: #f6a385; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col6 { background-color: #f4c6af; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col7 { background-color: #4f69d9; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col0 { background-color: #5572df; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col1 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col2 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col3 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col4 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col5 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col6 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col7 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col0 { background-color: #3c4ec2; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col1 { background-color: #c3d5f4; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col2 { background-color: #d75445; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col3 { background-color: #f7af91; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col4 { background-color: #3e51c5; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col5 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col6 { background-color: #dedcdb; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col7 { background-color: #5673e0; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col0 { background-color: #94b6ff; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col1 { background-color: #edd2c3; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col2 { background-color: #f7b599; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col3 { background-color: #f5c0a7; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col4 { background-color: #6b8df0; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col5 { background-color: #efcfbf; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col6 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col7 { background-color: #536edd; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col0 { background-color: #d9dce1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col1 { background-color: #9fbfff; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col2 { background-color: #f3c7b1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col3 { background-color: #c1d4f4; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col4 { background-color: #cbd8ee; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col5 { background-color: #d7dce3; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col6 { background-color: #bad0f8; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col7 { background-color: #b40426; color: #f1f1f1; } Year Month sst rhum slp vwnd cldc sst_anomaly Year 1.000000 -0.010203 0.164281 -0.042398 0.048908 -0.035567 0.241832 0.465285 Month -0.010203 1.000000 0.541188 0.447957 -0.434216 0.151316 0.380311 0.001361 sst 0.164281 0.541188 1.000000 0.549282 -0.834263 0.813505 0.402424 0.296363 rhum -0.042398 0.447957 0.549282 1.000000 -0.628214 0.502711 0.412368 0.033063 slp 0.048908 -0.434216 -0.834263 -0.628214 1.000000 -0.812037 -0.548567 -0.038420 vwnd -0.035567 0.151316 0.813505 0.502711 -0.812037 1.000000 0.236100 0.056093 cldc 0.241832 0.380311 0.402424 0.412368 -0.548567 0.236100 1.000000 0.043181 sst_anomaly 0.465285 0.001361 0.296363 0.033063 -0.038420 0.056093 0.043181 1.000000 3 - The Hurricane Dataset df = pd . read_csv ( 'Datasets/atlantic_new.csv' ) df2 = pd . read_csv ( 'Datasets/pacific_new.csv' ) fig , axs = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) df . hist ( 'month' , ax = axs [ 0 ]) # plt.hist(df.month, ax=axs[0]) axs [ 0 ] . set_title ( \"Monthly Hurricane Counts - Atlantic\" ) #tepo df2 . hist ( 'month' , ax = axs [ 1 ]) axs [ 1 ] . set_title ( \"Monthly Hurricane Counts - Pacific\" ); df [ df . year == 2011 ] . sort_values ( by = 'Maximum Wind' , ascending = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind NE Moderate Wind SE Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW month year 47316 AL162011 OPHELIA 2011-10-02 0 HU 32.8 -62.5 120 940 ... 60 60 40 40 30 30 20 20 10 2011 47154 AL122011 KATIA 2011-09-06 0 HU 25.6 -64.0 120 942 ... 80 70 60 70 50 50 40 50 9 2011 47155 AL122011 KATIA 2011-09-06 600 HU 26.2 -64.8 115 946 ... 80 70 60 70 50 50 40 50 9 2011 47315 AL162011 OPHELIA 2011-10-01 1800 HU 30.7 -62.9 110 946 ... 60 60 40 40 30 30 20 20 10 2011 47153 AL122011 KATIA 2011-09-05 1800 HU 24.8 -63.4 110 946 ... 80 70 60 70 50 50 40 50 9 2011 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 47201 AL132011 LEE 2011-09-06 600 EX 33.4 -85.3 20 997 ... 0 0 0 0 0 0 0 0 9 2011 47119 AL122011 KATIA 2011-08-28 600 LO 9.4 -20.3 20 1011 ... 0 0 0 0 0 0 0 0 8 2011 47118 AL122011 KATIA 2011-08-28 0 LO 9.5 -19.0 20 1012 ... 0 0 0 0 0 0 0 0 8 2011 47202 AL132011 LEE 2011-09-06 1200 EX 34.2 -85.1 15 1000 ... 0 0 0 0 0 0 0 0 9 2011 47203 AL132011 LEE 2011-09-06 1800 EX 34.9 -85.3 15 1004 ... 0 0 0 0 0 0 0 0 9 2011 557 rows \u00d7 24 columns # from cartopy import features.Border import tropycal.tracks as tracks hurdat = tracks . TrackDataset ( basin = 'north_atlantic' ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (5.41 seconds) import cartopy.crs as ccrs katia = hurdat . get_storm (( 'katia' , 2011 )) ophelia = hurdat . get_storm (( 'ophelia' , 2011 )) katia . plot () ophelia . plot () df . sort_values ( by = 'year' , ascending = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind NE Moderate Wind SE Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW month year 49104 AL122015 KATE 2015-11-13 1200 EX 40.7 -45.4 45 987 ... 0 0 0 0 0 0 0 0 11 2015 48881 AL062015 FRED 2015-08-30 1800 TS 14.0 -20.7 55 998 ... 30 30 10 20 0 0 0 0 8 2015 48873 AL052015 ERIKA 2015-08-27 1200 TS 16.5 -62.2 45 1004 ... 0 0 0 0 0 0 0 0 8 2015 48874 AL052015 ERIKA 2015-08-27 1800 TS 16.6 -63.6 45 1006 ... 0 0 0 0 0 0 0 0 8 2015 48875 AL052015 ERIKA 2015-08-28 0 TS 17.2 -65.1 45 1006 ... 0 0 0 0 0 0 0 0 8 2015 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 69 AL051851 UNNAMED 1851-09-14 0 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 70 AL051851 UNNAMED 1851-09-14 600 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 71 AL051851 UNNAMED 1851-09-14 1200 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 72 AL051851 UNNAMED 1851-09-14 1800 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 0 AL011851 UNNAMED 1851-06-25 0 HU 28.0 -94.8 80 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 6 1851 49105 rows \u00d7 24 columns","title":"EDA"},{"location":"Exploratory_Data_Analysis/#exploratory-data-analysis","text":"Group: Alysson Esp\u00edndola de S\u00e1 Silveira, Rener de Souza Oliveira, Yuri Luis Faria Silva # !apt-get install libgeos-3.5.0 # !apt-get install libgeos-dev # !pip install https://github.com/matplotlib/basemap/archive/master.zip # !pip install pyproj==1.9.6 # !pip install netCDF4 # !pip install jinja2 # !pip install geopy # !pip install xarray # !pip install networkx # !pip install requests # !pip install cartopy # !pip install tropycal #Uncomment the above lines and execute this cell; then go to \"Runtime\"->\"Restart Runtime\". % matplotlib inline import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as c from mpl_toolkits.basemap import Basemap , shiftgrid import pandas as pd import netCDF4 as nc","title":"Exploratory Data Analysis"},{"location":"Exploratory_Data_Analysis/#1-missing-data-netcdf4","text":"This part of our data is in .nc file extension, which stands for netCDF4. It's widely used in climatology and has python support for reading it. sst_mean = nc . Dataset ( 'Datasets/sst.mean.nc' , 'r' ) rhum_mean = nc . Dataset ( 'Datasets/rhum.mean.nc' , 'r' ) wspd_mean = nc . Dataset ( 'Datasets/wspd.mean.nc' , 'r' ) slp_mean = nc . Dataset ( 'Datasets/slp.mean.nc' , 'r' ) vwnd_mean = nc . Dataset ( 'Datasets/vwnd.mean.nc' , 'r' ) cldc_mean = nc . Dataset ( \"Datasets/cldc.mean.nc\" , 'r' ) lats = sst_mean . variables [ 'lat' ][:] lons = sst_mean . variables [ 'lon' ][:] time = sst_mean . variables [ 'time' ][:] sst = sst_mean . variables [ 'sst' ][:,:,:] rhum = rhum_mean . variables [ 'rhum' ][:,:,:] wspd = wspd_mean . variables [ 'wspd' ][:,:,:] slp = slp_mean . variables [ 'slp' ][:,:,:] vwnd = vwnd_mean . variables [ 'vwnd' ][:,:,:] cldc = cldc_mean . variables [ 'cldc' ][:,:,:] sst_mean . close () wspd_mean . close () rhum_mean . close () slp_mean . close () vwnd_mean . close () cldc_mean . close () period = pd . date_range ( start = \"1800-01-01\" , end = \"2020-07-01\" , freq = \"MS\" ) . to_pydatetime () . tolist () def get_missing ( data : list , labels : list ) -> dict : missing = {} lenght = data [ 0 ] . shape [ 0 ] for j , item in enumerate ( data ): missing [ labels [ j ]] = [] for i in range ( lenght ): missing [ labels [ j ]] . append ( 100 * np . sum ( item [ i ] . mask ) / item [ i ] . data . size ) return missing missing = get_missing ([ sst , wspd , rhum , slp , vwnd , cldc ],[ 'sst' , 'wspd' , 'rhum' , 'slp' , 'vwnd' , 'cldc' ]) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Missing Data - Global\" , fontsize = 15 ) ax . set_xlabel ( \"Year\" , fontsize = 12 ) ax . set_ylabel ( \" % o f missing data\" , fontsize = 12 ) for key , value in missing . items (): ax . plot ( period , missing [ key ], label = key ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( 'missing.png' ) plt . show () print ( type ( sst )) <class 'numpy.ma.core.MaskedArray'> Legend: sst: Sea Surface Temperature wspd: Scalar Wind Speed rhum: Relative Humidity slp: Sea Level Pressure vwnd: V-wind component cldc: Cloudiness Since continents represent approximately 29,1\\% of earth surface and the data cover just oceans, The continents are filled with missing values. So, naturally, this percentage is the lower bound of missing data. Note that the data types are \"numpy.ma.core.MaskedArray\" which has an attribute \"mask\", which is an indicator variable of missingness; That's gonna help us deal with it. As we can see in the above plot, we have decades with missingness levels above 90%. Soon, we're gonna analyze focusing on Pacific and North Atlantic, which is our study area. sst_mean = nc . Dataset ( 'Datasets/sst.mean.nc' , 'r' ) x = sst_mean [ 'sst' ][:] plt . hist ( x [:, 70 , 40 ][ x [:, 70 , 40 ] . data < 20000 ]) #testes com masked array (array([ 3., 3., 12., 20., 30., 36., 32., 23., 30., 6.]), array([-0.30999756, 0.36400145, 1.0380005 , 1.7119995 , 2.3859985 , 3.0599976 , 3.7339966 , 4.4079957 , 5.0819945 , 5.755994 , 6.4299927 ], dtype=float32), <a list of 10 Patch objects>) x . shape (2647, 90, 180) sst_at = sst [:, 34 : 40 , 51 : 82 ] #10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W wspd_at = wspd [:, 34 : 40 , 51 : 82 ] rhum_at = rhum [:, 34 : 40 , 51 : 82 ] slp_at = slp [:, 34 : 40 , 51 : 82 ] vwnd_at = vwnd [:, 34 : 40 , 51 : 82 ] cldc_at = cldc [:, 34 : 40 , 51 : 82 ] # sst_pac = sst[:,14:45,0:41] #0\u00b0-60\u00b0N, 100\u00b0W-180\u00b0W # wspd_pac = wspd[:,14:45,0:41] # rhum_pac = rhum[:,14:45,0:41] missing_at = get_missing ([ sst_at , wspd_at , rhum_at , slp_at , vwnd_at , cldc_at ],[ 'sst_at' , 'wspd_at' , 'rhum_at' , 'slp_at' , 'vwnd_at' , 'cldc_at' ]) # missing_pac = get_missing([sst_pac,wspd_pac,rhum_pac],['sst_pac','wspd_pac','rhum_pac']) # fig,(ax,ax1) = plt.subplots(2,1,figsize=(10,8)) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 4 )) ax . set_title ( \"Missing Data - North Atlantic MDR*\" , fontsize = 15 ) ax . set_xlabel ( \"Year\" , fontsize = 12 ) ax . set_ylabel ( \" % o f missing data\" , fontsize = 12 ) for key , value in missing_at . items (): ax . plot ( period , missing_at [ key ], label = key [ 0 : - 3 ]) plt . axvline ( x = period [ 1860 ], label = \"Jan/1955\" , color = 'black' , lw = 3 , ls = '--' ) ax . legend ( loc = 'best' , fontsize = 12 ); fig . savefig ( 'missing_mdr.png' ) *MDR stands for Main Development Region which refers to the rectangle 10\u00b0-20\u00b0N, 80\u00b0-20\u00b0W. We can see that after the decade 1950-1960, we have more a more complete dataset in our study region. So, to understand the relation of the variables, we're gonna work starting this decade. But, nothing keeps us from working with older data, since values don't vary a lot when it's close; If we wanted to work with long-term trends we could cut the data starting from 1920, and just apply the mean, even with \\approx70 % of missing data. that's because we have an indicator array, which can help with modeling. Also, this percentage is a little lower because of the continental area in the MDR cut considered. Below, we have an example of the distribution of SST data in January 1955.","title":"1 - Missing Data - netCDF4"},{"location":"Exploratory_Data_Analysis/#2-visualization","text":"#Transforms longitude ranges from [0,360] para [-180,180] --> useful for plot sst [:], lonsn = shiftgrid ( 180 , sst [:], lons , start = False ) wspd [:], lonsn = shiftgrid ( 180 , wspd [:], lons , start = False ) # shum[:],lonsn = shiftgrid(180,shum[:],lons,start=False) rhum [:], lonsn = shiftgrid ( 180 , rhum [:], lons , start = False ) lons = lonsn #Reference: https://annefou.github.io/metos_python/04-plotting/ time_index = 1860 fig = plt . figure ( figsize = [ 12 , 15 ]) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . set_title ( \"sst: {} \" . format ( period [ time_index ] . date ()), fontsize = 16 ) map = Basemap ( projection = 'cyl' , llcrnrlat =- 90 , urcrnrlat = 90 , \\ llcrnrlon =- 180 , urcrnrlon = 180 , resolution = 'c' , ax = ax ) map . drawcoastlines () map . fillcontinents ( color = '#ffe2ab' ) map . drawparallels ( np . arange ( - 90. , 120. , 30. ), labels = [ 1 , 0 , 0 , 0 ]) map . drawmeridians ( np . arange ( - 180. , 180. , 60. ), labels = [ 0 , 0 , 0 , 1 ]) llons , llats = np . meshgrid ( lons , lats ) x , y = map ( llons , llats ) cmap = c . ListedColormap ([ '#35978f' , '#ffffcc' , '#ffeda0' , '#fed976' , '#feb24c' , '#fd8d3c' , '#fc4e2a' , '#e31a1c' , '#bd0026' , '#800026' ]) bounds = list ( np . arange ( - 5 , 37 , 1 )) # bounds=list(np.arange(10,100,5)) norm = c . BoundaryNorm ( bounds , ncolors = cmap . N ) cs = map . contourf ( x , y , sst [ time_index ], cmap = cmap , norm = norm , levels = bounds ) fig . colorbar ( cs , cmap = cmap , norm = norm , boundaries = bounds , ticks = bounds , ax = ax , orientation = 'horizontal' ); A good way to visualize this data is with animation, but the above plot gives us a very good glimpse of what kind of data we're working on. print ( period [ 1788 ]) 1949-01-01 00:00:00 print ( period [ 1860 ]) # -- 1955 January def get_mean ( data ): size = data . shape [ 0 ] new = np . array ([]) for i in range ( size ): new = np . append ( new , np . mean ( data [ i ,:,:])) return new #We're gonna start from the time index 1788, representing January 1949, to match with our PDI dataset. data_at = pd . DataFrame ( get_mean ( sst_at [ 1788 :,:,:]), columns = [ \"sst\" ]) period_df = pd . DataFrame ( period [ 1788 :], columns = [ \"Date\" ]) period_df [ 'Year' ] = period_df . Date . map ( lambda x : x . year ) period_df [ 'Month' ] = period_df . Date . map ( lambda x : x . month ) data_at [ 'rhum' ] = pd . DataFrame ( get_mean ( rhum_at [ 1788 :,:,:]), columns = [ \"rhum\" ]) data_at [ 'slp' ] = pd . DataFrame ( get_mean ( slp_at [ 1788 :,:,:]), columns = [ \"slp\" ]) data_at [ 'wspd' ] = pd . DataFrame ( get_mean ( wspd_at [ 1788 :,:,:]), columns = [ \"wspd\" ]) data_at [ 'vwnd' ] = pd . DataFrame ( get_mean ( vwnd_at [ 1788 :,:,:]), columns = [ \"vwnd\" ]) data_at [ 'cldc' ] = pd . DataFrame ( get_mean ( cldc_at [ 1788 :,:,:]), columns = [ \"cldc\" ]) atlantic_mdr = pd . concat ([ period_df , data_at ], axis = 1 ) cum_sum = {} for i in range ( 1 , 13 ): cum_sum [ i ] = 0 k = 0 #year count for i in range ( 0 , atlantic_mdr . shape [ 0 ] - 12 ): month = atlantic_mdr . iloc [ i ,:] . Month if month % 12 == 1 : k += 1 cum_sum [ month ] += atlantic_mdr . iloc [ i , 3 ] atlantic_mdr . loc [ atlantic_mdr . index [ i ], 'sst_anomaly' ] = atlantic_mdr . iloc [ i , 3 ] - cum_sum [ month ] / k atlantic_mdr . drop ( 'sst_anomaly' , axis = 1 ) . to_csv ( 'Datasets/atlantic_mdr.csv' , index = False ) atlantic_mdr . iloc [ 12 : 24 ,:] 1955-01-01 00:00:00 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Year Month sst rhum slp wspd vwnd cldc sst_anomaly 12 1950-01-01 1950 1 26.618658 80.166064 1012.699219 6.274566 -2.709708 4.139401 0.401114 13 1950-02-01 1950 2 25.996199 78.348542 1012.883875 6.633078 -2.401613 3.965875 0.150126 14 1950-03-01 1950 3 26.414721 80.421051 1013.134516 5.954330 -2.062759 4.087092 0.269065 15 1950-04-01 1950 4 27.401395 81.788231 1011.096354 4.909927 -0.454326 3.146995 0.092152 16 1950-05-01 1950 5 28.472969 80.007482 1010.577563 5.476332 0.516981 4.291411 0.073090 17 1950-06-01 1950 6 28.896216 81.928657 1009.393293 4.850322 2.076482 4.650387 -0.032209 18 1950-07-01 1950 7 29.033083 81.664352 1007.978285 5.042128 2.375750 5.184904 0.050943 19 1950-08-01 1950 8 28.283786 82.927725 1007.856950 4.906136 1.956808 5.109233 -0.403537 20 1950-09-01 1950 9 28.832019 82.908388 1007.869104 4.910579 0.525868 5.305648 0.051078 21 1950-10-01 1950 10 28.090939 80.471020 1010.697798 5.345116 -1.037250 4.423469 -0.236270 22 1950-11-01 1950 11 27.558145 80.553408 1009.745018 6.262806 -2.024746 3.765682 -0.195893 23 1950-12-01 1950 12 26.741317 79.440282 1012.071718 7.539570 -4.750430 4.100716 -0.222622 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 5 )) ax . plot ( np . arange ( 1949 , 2021 , 1 ), atlantic_mdr . groupby ([ 'Year' ]) . agg ({ 'sst_anomaly' : np . mean })[ 'sst_anomaly' ]) ax . set_title ( \"Deviations in sea temperature from the historical average\" , fontsize = 14 ) ax . set_xlabel ( \"Year\" , fontsize = 12 ) ax . set_ylabel ( \"Temperature Deviation\" , fontsize = 12 ); corr = atlantic_mdr . corr () corr . style . background_gradient ( cmap = 'coolwarm' ) #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col0 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col1 { background-color: #9dbdff; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col2 { background-color: #e8d6cc; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col3 { background-color: #b3cdfb; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col4 { background-color: #d8dce2; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col5 { background-color: #c9d7f0; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col6 { background-color: #dfdbd9; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row0_col7 { background-color: #d9dce1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col0 { background-color: #4358cb; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col1 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col2 { background-color: #f49a7b; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col3 { background-color: #f7b99e; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col4 { background-color: #81a4fb; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col5 { background-color: #e5d8d1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col6 { background-color: #f2cbb7; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row1_col7 { background-color: #455cce; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col0 { background-color: #7a9df8; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col1 { background-color: #f7b396; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col2 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col3 { background-color: #f6a385; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col4 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col5 { background-color: #d75445; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col6 { background-color: #f3c7b1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row2_col7 { background-color: #a6c4fe; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col0 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col1 { background-color: #f3c7b1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col2 { background-color: #f39778; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col3 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col4 { background-color: #5d7ce6; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col5 { background-color: #f6a385; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col6 { background-color: #f4c6af; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row3_col7 { background-color: #4f69d9; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col0 { background-color: #5572df; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col1 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col2 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col3 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col4 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col5 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col6 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row4_col7 { background-color: #3b4cc0; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col0 { background-color: #3c4ec2; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col1 { background-color: #c3d5f4; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col2 { background-color: #d75445; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col3 { background-color: #f7af91; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col4 { background-color: #3e51c5; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col5 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col6 { background-color: #dedcdb; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row5_col7 { background-color: #5673e0; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col0 { background-color: #94b6ff; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col1 { background-color: #edd2c3; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col2 { background-color: #f7b599; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col3 { background-color: #f5c0a7; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col4 { background-color: #6b8df0; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col5 { background-color: #efcfbf; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col6 { background-color: #b40426; color: #f1f1f1; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row6_col7 { background-color: #536edd; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col0 { background-color: #d9dce1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col1 { background-color: #9fbfff; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col2 { background-color: #f3c7b1; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col3 { background-color: #c1d4f4; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col4 { background-color: #cbd8ee; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col5 { background-color: #d7dce3; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col6 { background-color: #bad0f8; color: #000000; } #T_43e92b6a_e656_11ea_b667_5cc9d3626684row7_col7 { background-color: #b40426; color: #f1f1f1; } Year Month sst rhum slp vwnd cldc sst_anomaly Year 1.000000 -0.010203 0.164281 -0.042398 0.048908 -0.035567 0.241832 0.465285 Month -0.010203 1.000000 0.541188 0.447957 -0.434216 0.151316 0.380311 0.001361 sst 0.164281 0.541188 1.000000 0.549282 -0.834263 0.813505 0.402424 0.296363 rhum -0.042398 0.447957 0.549282 1.000000 -0.628214 0.502711 0.412368 0.033063 slp 0.048908 -0.434216 -0.834263 -0.628214 1.000000 -0.812037 -0.548567 -0.038420 vwnd -0.035567 0.151316 0.813505 0.502711 -0.812037 1.000000 0.236100 0.056093 cldc 0.241832 0.380311 0.402424 0.412368 -0.548567 0.236100 1.000000 0.043181 sst_anomaly 0.465285 0.001361 0.296363 0.033063 -0.038420 0.056093 0.043181 1.000000","title":"2 - Visualization"},{"location":"Exploratory_Data_Analysis/#3-the-hurricane-dataset","text":"df = pd . read_csv ( 'Datasets/atlantic_new.csv' ) df2 = pd . read_csv ( 'Datasets/pacific_new.csv' ) fig , axs = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) df . hist ( 'month' , ax = axs [ 0 ]) # plt.hist(df.month, ax=axs[0]) axs [ 0 ] . set_title ( \"Monthly Hurricane Counts - Atlantic\" ) #tepo df2 . hist ( 'month' , ax = axs [ 1 ]) axs [ 1 ] . set_title ( \"Monthly Hurricane Counts - Pacific\" ); df [ df . year == 2011 ] . sort_values ( by = 'Maximum Wind' , ascending = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind NE Moderate Wind SE Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW month year 47316 AL162011 OPHELIA 2011-10-02 0 HU 32.8 -62.5 120 940 ... 60 60 40 40 30 30 20 20 10 2011 47154 AL122011 KATIA 2011-09-06 0 HU 25.6 -64.0 120 942 ... 80 70 60 70 50 50 40 50 9 2011 47155 AL122011 KATIA 2011-09-06 600 HU 26.2 -64.8 115 946 ... 80 70 60 70 50 50 40 50 9 2011 47315 AL162011 OPHELIA 2011-10-01 1800 HU 30.7 -62.9 110 946 ... 60 60 40 40 30 30 20 20 10 2011 47153 AL122011 KATIA 2011-09-05 1800 HU 24.8 -63.4 110 946 ... 80 70 60 70 50 50 40 50 9 2011 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 47201 AL132011 LEE 2011-09-06 600 EX 33.4 -85.3 20 997 ... 0 0 0 0 0 0 0 0 9 2011 47119 AL122011 KATIA 2011-08-28 600 LO 9.4 -20.3 20 1011 ... 0 0 0 0 0 0 0 0 8 2011 47118 AL122011 KATIA 2011-08-28 0 LO 9.5 -19.0 20 1012 ... 0 0 0 0 0 0 0 0 8 2011 47202 AL132011 LEE 2011-09-06 1200 EX 34.2 -85.1 15 1000 ... 0 0 0 0 0 0 0 0 9 2011 47203 AL132011 LEE 2011-09-06 1800 EX 34.9 -85.3 15 1004 ... 0 0 0 0 0 0 0 0 9 2011 557 rows \u00d7 24 columns # from cartopy import features.Border import tropycal.tracks as tracks hurdat = tracks . TrackDataset ( basin = 'north_atlantic' ) --> Starting to read in HURDAT2 data --> Completed reading in HURDAT2 data (5.41 seconds) import cartopy.crs as ccrs katia = hurdat . get_storm (( 'katia' , 2011 )) ophelia = hurdat . get_storm (( 'ophelia' , 2011 )) katia . plot () ophelia . plot () df . sort_values ( by = 'year' , ascending = 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Name Date Time Event Status Latitude Longitude Maximum Wind Minimum Pressure ... Moderate Wind NE Moderate Wind SE Moderate Wind SW Moderate Wind NW High Wind NE High Wind SE High Wind SW High Wind NW month year 49104 AL122015 KATE 2015-11-13 1200 EX 40.7 -45.4 45 987 ... 0 0 0 0 0 0 0 0 11 2015 48881 AL062015 FRED 2015-08-30 1800 TS 14.0 -20.7 55 998 ... 30 30 10 20 0 0 0 0 8 2015 48873 AL052015 ERIKA 2015-08-27 1200 TS 16.5 -62.2 45 1004 ... 0 0 0 0 0 0 0 0 8 2015 48874 AL052015 ERIKA 2015-08-27 1800 TS 16.6 -63.6 45 1006 ... 0 0 0 0 0 0 0 0 8 2015 48875 AL052015 ERIKA 2015-08-28 0 TS 17.2 -65.1 45 1006 ... 0 0 0 0 0 0 0 0 8 2015 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 69 AL051851 UNNAMED 1851-09-14 0 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 70 AL051851 UNNAMED 1851-09-14 600 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 71 AL051851 UNNAMED 1851-09-14 1200 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 72 AL051851 UNNAMED 1851-09-14 1800 TS 32.5 -73.5 50 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 9 1851 0 AL011851 UNNAMED 1851-06-25 0 HU 28.0 -94.8 80 -999 ... -999 -999 -999 -999 -999 -999 -999 -999 6 1851 49105 rows \u00d7 24 columns","title":"3 - The Hurricane Dataset"},{"location":"NN-TrackPrediction/","text":"Redes Neurais para Previs\u00e3o de Trajet\u00f3ria Neste notebook, usaremos redes neurais para previs\u00e3o de trajet\u00f3ria futura de uma tempestade. A ideia geral \u00e9 dado as informa\u00e7\u00f5es de um furac\u00e3o de algumas horas atr\u00e1s e do presente, projetaremos a sua posi\u00e7\u00e3o futura. import numpy as np import matplotlib.pyplot as plt import numpy as np import pandas as pd import datetime as dt # from sklearn.model_selection import train_test_split import seaborn as sns sns . set () from sklearn.metrics import r2_score % matplotlib inline import tensorflow as tf tf . keras . backend . clear_session () print ( tf . __version__ ) 2.0.0 Prepara\u00e7\u00e3o dos dados Iremos abaixo criar fun\u00e7\u00f5es para preparar nossos dados para o modelo, e aplic\u00e1-las ao dataframe original #Leitura dos dados data = pd . read_csv ( 'Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data . columns Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Date_c', 'Year', 'Month', 'Day', 'Latitude_c', 'Longitude_c', 'Duration', 'sst', 'rhum', 'wspd', 'slp', 'cldc'], dtype='object') ## Fun\u00e7\u00f5es de Padroniza\u00e7\u00e3o: def standard_scale ( data , cols ): df = data . copy () for col in cols : mean = df [ col ] . mean () std = df [ col ] . std () df . loc [:, col ] = ( df [ col ] - mean ) / std return df [ cols ] def standard_scale_back ( scaled , original , cols ): df = scaled . copy () for col in cols : mean = original [ col ] . mean () std = original [ col ] . std () df . loc [:, col ] = df [ col ] * std + mean return df [ cols ] # Fun\u00e7\u00e3o que divide os dados por ID da tempestade def split ( df ): st = [] ids = df . ID . unique () for ID in ids : st . append ( df [ df . ID == ID ]) return st splitted = split ( data ) print ( len ( splitted )) 685 A fun\u00e7\u00e3o \"clean_data\" formata o preditor Tempo, o convertendo para horas (por padr\u00e3o, horas ap\u00f3s Jan/1951), e padroniza os dados de input. A fun\u00e7\u00e3o \"shift_data\", faz um shift dos dados e d\u00e1 como sa\u00edda as matriz tridimensional X e o vetor Y, onde X \u00e9 composto por matrizes s\\times n : \\left[\\begin{matrix} x_1^{(t-s+1)} & x_2^{(t-s+1)} & ...& x_n^{(t-s+1)}\\\\ x_1^{(t-s+2)} & x_2^{(t-s+2)} & ...& x_n^{(t-s+2)}\\\\ \\vdots &\\vdots &\\vdots &\\vdots\\\\ x_1^{(t)} & x_2^{(t)} & ...& x_n^{(t)}\\end{matrix}\\right] na qual: x_1,~x_2, ~..., ~x_n representam os n preditores usados (Usaremos 4 inicialmente: tempo latitude, longitude e velocidade de vento) x_i^{(k)} representa o preditor i no registro de tempo k . s \u00e9 o par\u00e2metros \"shift\", que representa em quantos per\u00edodos passados basearemos a previs\u00e3o futura. Por padr\u00e3o, inicialmente, deixamos s=3 . Como cada registro \u00e9 espa\u00e7ado por 6 horas, neste formado estamos usando daados do presente e de 12 horas atr\u00e1s, pra projetar o dado futuro. O vetor Y \u00e9 composto por: \\left[\\begin{matrix}lat^{(t+1)}&lon^{(t+1)}\\\\ lat^{(t+2)}&lon^{(t+2)}\\\\ \\vdots & \\vdots\\\\ lat^{(t+p)}&lon^{(t+p)}\\end{matrix}\\right] na qual: lat^{(k)} e lon^{(k)} representam latitude e longitude no registro de tempo k p \u00e9 o par\u00e2metro \"pred\", que diz quantos per\u00edodos \u00e0 frente iremos prever. Por padr\u00e3o, deixamos p=1 , assim a matriz se resume em um vetor [lat^{(t+1)},~ lon^{(t+1)}] . Usamos previs\u00f5es de 3 registros passados para prever o pr\u00f3ximo registro ap\u00f3s o \u00faltimo ponto de treino. Um trabalho futuro seria usar intervalos de previs\u00f5es maiores, mas vamos nos ater aqui a apenas 1 registro \u00e0 frete, o que equivale a 6 horas na grande maiorias dos pontos. cols1 = [ 'Hours' , 'Latitude' , 'Longitude' , 'Maximum Wind' ] data . loc [:, 'Time_new' ] = data . Date + data . Time . map ( lambda x : pd . Timedelta ( hours = x / 100 )) def clean_data ( df , input_cols = cols1 , year0 = 1951 ): df2 = df . copy () df2 . loc [:, 'Hours' ] = ( df2 . loc [:, 'Time_new' ] - pd . Timestamp ( year0 , 1 , 1 )) / pd . Timedelta ( '1 hour' ) df2 . loc [:, input_cols ] = standard_scale ( df2 , input_cols ) return df2 [[ 'ID' ] + input_cols ] def shift_data ( df , shift = 3 , pred = 1 ): x = [] y = [] df = df . set_index ( np . arange ( 0 , len ( df ))) for i in range ( 0 , len ( df ) - shift ): x_arr = [] for j in range ( i , i + shift ): x_arr . append ( df . loc [ j ,:]) if pred == 1 : y . append ( np . array ( df . loc [ i + shift : i + shift + pred - 1 ,[ 'Latitude' , 'Longitude' ]]) . ravel ()) else : y . append ( np . array ( df . loc [ i + shift : i + shift + pred - 1 ,[ 'Latitude' , 'Longitude' ]])) x . append ( np . array ( x_arr )) return np . array ( x ), np . array ( y ) data_cleaned = clean_data ( data ) data_cleaned . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Hours Latitude Longitude Maximum Wind count 2.238600e+04 2.238600e+04 2.238600e+04 2.238600e+04 mean -1.312271e-17 3.398752e-15 4.449279e-15 5.249630e-16 std 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 min -1.823465e+00 -1.939104e+00 -2.303286e+00 -1.644750e+00 25% -8.725997e-01 -8.005268e-01 -7.741070e-01 -8.529203e-01 50% 1.595638e-01 -4.466462e-02 -5.884587e-02 -2.590479e-01 75% 9.229854e-01 6.250867e-01 7.057436e-01 5.327820e-01 max 1.499554e+00 5.121988e+00 6.195990e+00 4.491932e+00 Treino, Valida\u00e7\u00e3o e Teste Para formatar os dados de treino, teste e valida\u00e7\u00e3o, primeiramente, usamos a fun\u00e7\u00e3o \"split\", que separa os dados em uma lista de dataframes onde cada um deles representa uma tempestade diferente. Ap\u00f3s isso atribuimos 70% da lista para treino, 20% para valida\u00e7\u00e3o e 10% para teste. Cada dataframe \u00e9 devidamente formatado para as matrizes acima usando a fun\u00e7\u00e3o \"shift_data\", e ap\u00f3s isso, unimos as matrizes de sa\u00edda para gerar a numpy array final, do modelo. Se fiz\u00e9ssemos o split diretamente, esse tipo de erro poderia acontecer. Suponha que uma tempestade tem seu \u00faltimo registro em 20 de Agosto em um determinado local, e uma outra tempestade se inicial dia 21 de Agosto em um local totalmente diferente. Um split direto poderia usar o registro de 20 de Agosto para prever o do dia 21, sendo que n\u00e3o h\u00e1 rela\u00e7\u00e3o entre eles. Essa separa\u00e7\u00e3o por tempestade \u00e9 importante para garantir a integridade dos nossos dados de modelagem, de forma que o treinamento seja feito tempestade por tempestade. def train_val_test_split ( df , t = 0.7 , v = 0.2 , input_cols = cols1 , shift = 3 , pred = 1 ): # t = fra\u00e7\u00e3o de treino # v = fra\u00e7\u00e3o de valida\u00e7\u00e3o # fra\u00e7\u00e3o de teste = 1-t-v # df = dataset j\u00e1 limpo por clean_data splitted_data = split ( df ) # Separamos os dados tempestade por tempestade para # evitar cruzamento de dados entre eventos diferentes n = len ( splitted ) train_storms = splitted_data [ 0 : int ( n * t )] val_storms = splitted_data [ int ( n * t ): int ( n * ( t + v ))] test_storms = splitted_data [ int ( n * ( 1 - t - v )):] #Geramos uma lista de matrizes, onde cada lista se refere #aos dados descocados de uma tempestade diferente xy_train = [ shift_data ( train [ input_cols ], shift , pred ) for train in train_storms ] xy_val = [ shift_data ( val [ input_cols ], shift , pred ) for val in val_storms ] xy_test = [ shift_data ( test [ input_cols ], shift , pred ) for test in test_storms ] # Concatena\u00e7\u00e3o das matrizes para gerar os dados finais xtrain = np . concatenate ([ x [ 0 ] for x in xy_train ], axis = 0 ) ytrain = np . concatenate ([ y [ 1 ] for y in xy_train ], axis = 0 ) xval = np . concatenate ([ x [ 0 ] for x in xy_val ], axis = 0 ) yval = np . concatenate ([ y [ 1 ] for y in xy_val ], axis = 0 ) xtest = np . concatenate ([ x [ 0 ] for x in xy_test ], axis = 0 ) ytest = np . concatenate ([ y [ 1 ] for y in xy_test ], axis = 0 ) return xtrain , ytrain , xval , yval , xtest , ytest %% time #O processo de split pode ser lento \u00e0 depender de seu computador xtrain , ytrain , xval , yval , xtest , ytest = train_val_test_split ( data_cleaned ) xtrain . shape , ytrain . shape , xval . shape , yval . shape , xtest . shape , ytest . shape CPU times : user 1 min 1 s , sys : 363 ms , total : 1 min 2 s Wall time : 1 min 1 s (( 14010 , 3 , 4 ), ( 14010 , 2 ), ( 4401 , 3 , 4 ), ( 4401 , 2 ), ( 18482 , 3 , 4 ), ( 18482 , 2 )) %% time ## Modelo que usa sst, rhum e slp (Temperatura do mar, Umidade e Press\u00e3o) cols2 = input_cols + [ 'sst' , 'rhum' , 'slp' , 'cldc' ] cleaned2 = clean_data ( data , cols2 ) xtrain2 , ytrain2 , xval2 , yval2 , xtest2 , ytest2 = train_val_test_split ( cleaned2 , input_cols = cols2 ) CPU times: user 1min 32s, sys: 470 ms, total: 1min 32s Wall time: 1min 31s xtrain2 . shape (14010, 3, 7) Modelos - Redes Neurais Vamos criar alguns modelos determin\u00edsticos e depois comparar a m\u00e9dias dos erros ao quadrado (MSE) nos dados de valida\u00e7\u00e3o e teste. (Uma esp\u00e9cie de cross-validation). Segundo os artigos na qual usamos como refer\u00eancia (Veja Bibliografia), apenas uma camada interna \u00e9 suficiente. Resta saber quantos neur\u00f4nios usar, para isso avaliaremos os modelos com 3, 6, e 9 neur\u00f4nios internos. Usaremos a fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoid na camada interna. e linear na camada de output, j\u00e1 que os dados de sa\u00edda s\u00e3o num\u00e9ricos cont\u00ednuos. Usamos tamb\u00e9m uma camada de dropout de 15% para evitar overfitting. Ap\u00f3s eleger o melhor, incluiremos um fator probabil\u00edstico para gerar intervalos de confian\u00e7as nas predi\u00e7\u00f5es. \u00cdndice de Modelos Model_ihj --> Modelo com entrada de i vari\u00e1veis, h neur\u00f4nios na camada interna e j pontos de sa\u00edda (no nosso caso j=2 sempre) Model_4h2 --> Modelo que usa as vari\u00e1veis Tempo, Latitude, Longitude e Velocidame M\u00e1xima de Vento (4 vari\u00e1veis) de tr\u00eas registros passados para prever a Latitude e Longitude do pr\u00f3ximo registro (2 sa\u00eddas) usando h neur\u00f4nios na camada interna. Aqui geraremos 3 modelos para os valores de h sendo 3, 6 e 9. Model_8h2 --> Modelo similar ao anterior, por\u00e9m utiliza quatro vari\u00e1veis a mais como entrada: sst, rhum, slp, cldc; Representando Temperatura a n\u00edvel do mar, Umidade, Press\u00e3o a n\u00edvel do mar e Cobertura de Nuvens. # Modelos tipo 4h2 model_432 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 4 )), tf . keras . layers . Dense ( 3 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_462 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 4 )), tf . keras . layers . Dense ( 6 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_492 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 4 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) # Modelos tipo 8h2 model_832 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 3 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_862 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 6 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_892 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) loss_fn = tf . keras . losses . MeanSquaredError () optimizer = tf . keras . optimizers . Adam () model_432 . compile ( optimizer = optimizer , loss = loss_fn ) model_462 . compile ( optimizer = optimizer , loss = loss_fn ) model_492 . compile ( optimizer = optimizer , loss = loss_fn ) model_832 . compile ( optimizer = optimizer , loss = loss_fn ) model_862 . compile ( optimizer = optimizer , loss = loss_fn ) model_892 . compile ( optimizer = optimizer , loss = loss_fn ) model_432 . summary () Model: \"sequential_6\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_6 (Flatten) (None, 12) 0 _________________________________________________________________ dense_12 (Dense) (None, 3) 39 _________________________________________________________________ dropout_6 (Dropout) (None, 3) 0 _________________________________________________________________ dense_13 (Dense) (None, 2) 8 ================================================================= Total params: 47 Trainable params: 47 Non-trainable params: 0 _________________________________________________________________ %% time history_432 = model_432 . fit ( xtrain , ytrain , validation_data = ( xval , yval ), epochs = 45 , verbose = 0 ) history_462 = model_462 . fit ( xtrain , ytrain , validation_data = ( xval , yval ), epochs = 45 , verbose = 0 ) history_492 = model_492 . fit ( xtrain , ytrain , validation_data = ( xval , yval ), epochs = 45 , verbose = 1 ) Train on 14010 samples, validate on 4401 samples Epoch 1/45 14010/14010 [==============================] - 1s 80us/sample - loss: 0.0757 - val_loss: 0.0101 Epoch 2/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0779 - val_loss: 0.0099 Epoch 3/45 14010/14010 [==============================] - 1s 92us/sample - loss: 0.0779 - val_loss: 0.0109 Epoch 4/45 14010/14010 [==============================] - 2s 121us/sample - loss: 0.0760 - val_loss: 0.0100 Epoch 5/45 14010/14010 [==============================] - 2s 111us/sample - loss: 0.0767 - val_loss: 0.0096 Epoch 6/45 14010/14010 [==============================] - 2s 152us/sample - loss: 0.0742 - val_loss: 0.0121 Epoch 7/45 14010/14010 [==============================] - 1s 90us/sample - loss: 0.0766 - val_loss: 0.0119 Epoch 8/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0758 - val_loss: 0.0108 Epoch 9/45 14010/14010 [==============================] - 1s 71us/sample - loss: 0.0779 - val_loss: 0.0107 Epoch 10/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0747 - val_loss: 0.0097 Epoch 11/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0771 - val_loss: 0.0106 Epoch 12/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0776 - val_loss: 0.0102 Epoch 13/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0786 - val_loss: 0.0102 Epoch 14/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0782 - val_loss: 0.0099 Epoch 15/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0783 - val_loss: 0.0106 Epoch 16/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0774 - val_loss: 0.0098 Epoch 17/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0758 - val_loss: 0.0102 Epoch 18/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0783 - val_loss: 0.0099 Epoch 19/45 14010/14010 [==============================] - 2s 115us/sample - loss: 0.0749 - val_loss: 0.0103 Epoch 20/45 14010/14010 [==============================] - 2s 134us/sample - loss: 0.0789 - val_loss: 0.0093 Epoch 21/45 14010/14010 [==============================] - 2s 135us/sample - loss: 0.0778 - val_loss: 0.0104 Epoch 22/45 14010/14010 [==============================] - 1s 83us/sample - loss: 0.0761 - val_loss: 0.0093 Epoch 23/45 14010/14010 [==============================] - 1s 91us/sample - loss: 0.0763 - val_loss: 0.0105 Epoch 24/45 14010/14010 [==============================] - 2s 115us/sample - loss: 0.0769 - val_loss: 0.0095 Epoch 25/45 14010/14010 [==============================] - 1s 99us/sample - loss: 0.0812 - val_loss: 0.0086 Epoch 26/45 14010/14010 [==============================] - 2s 114us/sample - loss: 0.0768 - val_loss: 0.0106 Epoch 27/45 14010/14010 [==============================] - 1s 100us/sample - loss: 0.0782 - val_loss: 0.0101 Epoch 28/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0767 - val_loss: 0.0083 Epoch 29/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0795 - val_loss: 0.0094 Epoch 30/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0779 - val_loss: 0.0104 Epoch 31/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0768 - val_loss: 0.0101 Epoch 32/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0764 - val_loss: 0.0094 Epoch 33/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0765 - val_loss: 0.0091 Epoch 34/45 14010/14010 [==============================] - 1s 76us/sample - loss: 0.0767 - val_loss: 0.0106 Epoch 35/45 14010/14010 [==============================] - 1s 70us/sample - loss: 0.0780 - val_loss: 0.0099 Epoch 36/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0755 - val_loss: 0.0094 Epoch 37/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0764 - val_loss: 0.0095 Epoch 38/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0782 - val_loss: 0.0098 Epoch 39/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0783 - val_loss: 0.0087 Epoch 40/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0737 - val_loss: 0.0091 Epoch 41/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0775 - val_loss: 0.0094 Epoch 42/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0792 - val_loss: 0.0101 Epoch 43/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0762 - val_loss: 0.0087 Epoch 44/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0754 - val_loss: 0.0100 Epoch 45/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0767 - val_loss: 0.0093 CPU times: user 3min 50s, sys: 15.8 s, total: 4min 6s Wall time: 2min 50s %% time history_832 = model_832 . fit ( xtrain2 , ytrain2 , validation_data = ( xval2 , yval2 ), epochs = 45 , verbose = 0 ) history_862 = model_862 . fit ( xtrain2 , ytrain2 , validation_data = ( xval2 , yval2 ), epochs = 45 , verbose = 0 ) history_892 = model_892 . fit ( xtrain2 , ytrain2 , validation_data = ( xval2 , yval2 ), epochs = 45 , verbose = 1 ) Train on 14010 samples, validate on 4401 samples Epoch 1/45 14010/14010 [==============================] - 2s 111us/sample - loss: 0.3210 - val_loss: 0.0874 Epoch 2/45 14010/14010 [==============================] - 1s 58us/sample - loss: 0.1670 - val_loss: 0.0666 Epoch 3/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.1460 - val_loss: 0.0563 Epoch 4/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.1326 - val_loss: 0.0473 Epoch 5/45 14010/14010 [==============================] - 1s 66us/sample - loss: 0.1232 - val_loss: 0.0398 Epoch 6/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.1170 - val_loss: 0.0334 Epoch 7/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.1055 - val_loss: 0.0274 Epoch 8/45 14010/14010 [==============================] - 1s 62us/sample - loss: 0.1025 - val_loss: 0.0236 Epoch 9/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0974 - val_loss: 0.0209 Epoch 10/45 14010/14010 [==============================] - 1s 63us/sample - loss: 0.0945 - val_loss: 0.0191 Epoch 11/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0924 - val_loss: 0.0179 Epoch 12/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0893 - val_loss: 0.0174 Epoch 13/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0884 - val_loss: 0.0182 Epoch 14/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0874 - val_loss: 0.0178 Epoch 15/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0844 - val_loss: 0.0163 Epoch 16/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0847 - val_loss: 0.0155 Epoch 17/45 14010/14010 [==============================] - 1s 58us/sample - loss: 0.0852 - val_loss: 0.0161 Epoch 18/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0871 - val_loss: 0.0157 Epoch 19/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0825 - val_loss: 0.0161 Epoch 20/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0817 - val_loss: 0.0148 Epoch 21/45 14010/14010 [==============================] - 1s 62us/sample - loss: 0.0853 - val_loss: 0.0149 Epoch 22/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0833 - val_loss: 0.0158 Epoch 23/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0804 - val_loss: 0.0156 Epoch 24/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0819 - val_loss: 0.0144 Epoch 25/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0836 - val_loss: 0.0142 Epoch 26/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0830 - val_loss: 0.0146 Epoch 27/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0834 - val_loss: 0.0146 Epoch 28/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0831 - val_loss: 0.0153 Epoch 29/45 14010/14010 [==============================] - 1s 62us/sample - loss: 0.0812 - val_loss: 0.0141 Epoch 30/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0820 - val_loss: 0.0141 Epoch 31/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0822 - val_loss: 0.0132 Epoch 32/45 14010/14010 [==============================] - 1s 58us/sample - loss: 0.0810 - val_loss: 0.0129 Epoch 33/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0824 - val_loss: 0.0147 Epoch 34/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0812 - val_loss: 0.0138 Epoch 35/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0816 - val_loss: 0.0138 Epoch 36/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0803 - val_loss: 0.0121 Epoch 37/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0767 - val_loss: 0.0133 Epoch 38/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0780 - val_loss: 0.0125 Epoch 39/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0783 - val_loss: 0.0133 Epoch 40/45 14010/14010 [==============================] - 1s 57us/sample - loss: 0.0795 - val_loss: 0.0121 Epoch 41/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0784 - val_loss: 0.0121 Epoch 42/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0807 - val_loss: 0.0121 Epoch 43/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0780 - val_loss: 0.0125 Epoch 44/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0811 - val_loss: 0.0144 Epoch 45/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0791 - val_loss: 0.0134 CPU times: user 3min 13s, sys: 12.5 s, total: 3min 25s Wall time: 2min 20s # Salvando os modelos para uso futuro model_432 . save ( 'Saved_NN_Models/model_432.h5' ) model_462 . save ( 'Saved_NN_Models/model_462.h5' ) model_492 . save ( 'Saved_NN_Models/model_492.h5' ) model_832 . save ( 'Saved_NN_Models/model_832.h5' ) model_862 . save ( 'Saved_NN_Models/model_862.h5' ) model_892 . save ( 'Saved_NN_Models/model_892.h5' ) # Recreando os modelos dos arquivos salvos # model_432 = tf.keras.models.load_model('Saved_NN_Models/model_432.h5') # model_462 = tf.keras.models.load_model('Saved_NN_Models/model_462.h5') # model_492 = tf.keras.models.load_model('Saved_NN_Models/model_492.h5') # model_832 = tf.keras.models.load_model('Saved_NN_Models/model_832.h5') # model_862 = tf.keras.models.load_model('Saved_NN_Models/model_862.h5') # model_892 = tf.keras.models.load_model('Saved_NN_Models/model_892.h5') models4 = [ model_432 , model_462 , model_492 ] models8 = [ model_832 , model_862 , model_892 ] history_list = [ history_432 , history_462 , history_492 , history_832 , history_862 , history_892 ] labels = [ \"4-3-2\" , '4-6-2' , '4-9-2' , \"8-3-2\" , '8-6-2' , '8-9-2' ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) test_mse = [] train_mse = [] val_mse = [] for model in models4 : test_mse . append ( model . evaluate ( xtest , ytest , verbose = 0 )) for model in models8 : test_mse . append ( model . evaluate ( xtest2 , ytest2 , verbose = 0 )) for history in history_list : train_mse . append ( np . mean ( history . history [ 'loss' ])) val_mse . append ( np . mean ( history . history [ 'val_loss' ])) wid = 0.8 / 3 - 0.0001 ax . set_title ( \"Perda (MSE) dos Modelos - Dados de Teste\" , fontsize = 16 ) ax . set_ylabel ( \"MSE\" , fontsize = 14 ) ax . bar ( np . arange ( 0 , 6 ) - wid , train_mse , label = \"Treino\" , width = wid ) ax . bar ( np . arange ( 0 , 6 ), val_mse , label = 'Valida\u00e7\u00e3o' , width = wid ) ax . bar ( np . arange ( 0 , 6 ) + wid , test_mse , label = \"Teste\" , width = wid ) ax . set_xticklabels ([ '0' ] + labels , fontsize = 12 ) ax . legend ( loc = 'best' , fontsize = 14 ); plt . savefig ( 'figs/NN_Models_MSE.jpg' ) Vemos que os modelos 4-9-2 e 8-9-2 performaram melhor em todos os conjuntos de dados. Vemos que 9 neur\u00f4nios internos performam muito bem. Vamos trabalhar com esses modelos a partir daqui. Em especial, com o 8-9-2 que apesar de ter uma maior perda do que o 4-9-2, ele utiliza uma maior quantidade de informa\u00e7\u00f5es clim\u00e1ticas. # plot accuracy and loss for the test set fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) ax . plot ( history_492 . history [ 'loss' ]) ax . plot ( history_492 . history [ 'val_loss' ]) ax . set_title ( '4-9-2 MSE' , fontsize = 16 ) ax . set_ylabel ( 'MSE' ) ax . set_xlabel ( 'epoch' ) ax . legend ([ 'Treino' , 'Valida\u00e7\u00e3o' ], fontsize = 14 , loc = 'best' ) ax1 . plot ( history_892 . history [ 'loss' ]) ax1 . plot ( history_892 . history [ 'val_loss' ]) ax1 . set_title ( '8-9-2 MSE' , fontsize = 16 ) ax1 . set_ylabel ( 'MSE' ) ax1 . set_xlabel ( 'epoch' ) ax1 . legend ([ 'Treino' , 'Valida\u00e7\u00e3o' ], fontsize = 14 , loc = 'best' ); plt . savefig ( 'figs/MSE-epoch.jpg' ) from sklearn.metrics import r2_score ypred = model_892 . predict ( xtest2 ) # from sklearn.linear_model import LinearRegression # lr = LinearRegression().fit(ytest,ypred) # lr.score(ytest,ypred) lat_r2 = r2_score ( ytest2 [:, 0 ], ypred [:, 0 ]) lon_r2 = r2_score ( ytest2 [:, 1 ], ypred [:, 1 ]) tot_r2 = r2_score ( ytest2 , ypred ) print ( f \"R2 Latitude Teste - { lat_r2 } \" ) print ( f \"R2 Longitude Teste - { lon_r2 } \" ) print ( f \"R2 Total Teste - { tot_r2 } \" ) fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 18 , 6 )) ax . set_title ( \"Latitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax . set_xlabel ( \"Real\" , fontsize = 13 ) ax . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax . scatter ( ytest2 [:, 0 ], ypred [:, 0 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lat_r2 , 3 ) } \" ) ax . legend ( loc = 'best' , fontsize = 13 ) ax1 . set_title ( \"Longitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax1 . set_xlabel ( \"Real\" , fontsize = 13 ) ax1 . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax1 . scatter ( ytest2 [:, 1 ], ypred [:, 1 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lon_r2 , 3 ) } \" ) ax1 . legend ( loc = 'best' , fontsize = 13 ); plt . savefig ( 'figs/lat_lon_teste.jpg' ) R2 Latitude Teste - 0.988149240276217 R2 Longitude Teste - 0.9864725794069319 R2 Total Teste - 0.9873109098415744 tf . keras . utils . plot_model ( model_892 , to_file = 'figs/model_892.png' , show_shapes = True , show_layer_names = True , rankdir = 'TB' , expand_nested = False , dpi = 96 ) # plt.savefig('figs/model892.jpg') data_test = data . loc [ int ( n * 0.9 ):,:] data_test . loc [:, 'Hours' ] = ( data_test . loc [:, 'Time_new' ] - pd . Timestamp ( 1951 , 1 , 1 )) / pd . Timedelta ( '1 hour' ) # data_test[data_test.Category==\"Category 3\"] irene = data_test [ data_test . ID == 'AL092011' ] irene . loc [:, cols2 ] = standard_scale ( irene , cols2 ) ir = irene . loc [:, cols2 ] def predict ( storm , model , shift = 3 , pred = 1 ): storm = storm . set_index ( np . arange ( 0 , len ( storm ))) y_pred = [] for i in range ( 0 , len ( storm ) - shift - 1 ): x = [] for j in range ( i , i + shift ): x . append ( storm . loc [ j ,:]) # if i == 0: # print(np.expand_dims(np.asarray(x), axis=0).shape) # print(np.expand_dims(np.asarray(x),axis=0)[0,0,0]) y_pred . append ( model . predict ( np . expand_dims ( np . asarray ( x ), axis = 0 )) . ravel ()) del x return np . array ( y_pred ) y_pred = predict ( ir , model_892 ) y_pred . shape (39, 2) ir_plot = ir . iloc [ 0 : - 4 ,:] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) fig . suptitle ( \"Hurricane Irene - 2011\" , fontsize = 16 , y = 1.08 ) ax [ 0 ] . set_title ( \"Latitude\" ) ax [ 1 ] . set_title ( \"Longitude\" ) print ( r2_score ( ir_plot . Latitude , y_pred [:, 0 ])) print ( r2_score ( ir_plot . Longitude , y_pred [:, 1 ])) ax [ 0 ] . scatter ( ir_plot . Hours , ir_plot . Latitude , label = 'True' ) ax [ 0 ] . scatter ( ir_plot . Hours , y_pred [:, 0 ], label = 'Predicted' ) ax [ 1 ] . scatter ( ir_plot . Hours , ir_plot . Longitude , label = 'True' ) ax [ 1 ] . scatter ( ir_plot . Hours , y_pred [:, 1 ], label = 'Predicted' ) ax [ 0 ] . legend ( loc = 'best' ) ax [ 1 ] . legend ( loc = 'best' ) 0 . 9124620701244499 0 . 8651593770302984 < matplotlib . legend . Legend at 0 x7fcb1e25b390 > # input_cols = ['Hours','Latitude','Longitude','Maximum Wind'] joaquin = data_test [ data_test . ID == 'AL112015' ] joaquin . loc [:, cols2 ] = standard_scale ( joaquin , cols2 ) y_jq = predict ( joaquin [ cols2 ], model_892 ) jq_plot = joaquin [ input_cols ] . iloc [ 0 : - 4 ,:] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) fig . suptitle ( \"Hurricane Joaquin - 2015\" , fontsize = 16 , y = 1.08 ) ax [ 0 ] . set_title ( \"Latitude\" ) ax [ 1 ] . set_title ( \"Longitude\" ) print ( r2_score ( jq_plot . Latitude , y_jq [:, 0 ])) print ( r2_score ( jq_plot . Longitude , y_jq [:, 1 ])) ax [ 0 ] . scatter ( jq_plot . Hours , jq_plot . Latitude , label = 'True' ) ax [ 0 ] . scatter ( jq_plot . Hours , y_jq [:, 0 ], label = 'Predicted' ) ax [ 1 ] . scatter ( jq_plot . Hours , jq_plot . Longitude , label = 'True' ) ax [ 1 ] . scatter ( jq_plot . Hours , y_jq [:, 1 ], label = 'Predicted' ) ax [ 0 ] . legend ( loc = 'best' ) ax [ 1 ] . legend ( loc = 'best' ) 0 . 9645696502595049 0 . 9816239209481076 < matplotlib . legend . Legend at 0 x7fcb1c9b9b50 >","title":"NNtrack"},{"location":"NN-TrackPrediction/#redes-neurais-para-previsao-de-trajetoria","text":"Neste notebook, usaremos redes neurais para previs\u00e3o de trajet\u00f3ria futura de uma tempestade. A ideia geral \u00e9 dado as informa\u00e7\u00f5es de um furac\u00e3o de algumas horas atr\u00e1s e do presente, projetaremos a sua posi\u00e7\u00e3o futura. import numpy as np import matplotlib.pyplot as plt import numpy as np import pandas as pd import datetime as dt # from sklearn.model_selection import train_test_split import seaborn as sns sns . set () from sklearn.metrics import r2_score % matplotlib inline import tensorflow as tf tf . keras . backend . clear_session () print ( tf . __version__ ) 2.0.0","title":"Redes Neurais para Previs\u00e3o de Trajet\u00f3ria"},{"location":"NN-TrackPrediction/#preparacao-dos-dados","text":"Iremos abaixo criar fun\u00e7\u00f5es para preparar nossos dados para o modelo, e aplic\u00e1-las ao dataframe original #Leitura dos dados data = pd . read_csv ( 'Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data . columns Index(['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Longitude', 'Maximum Wind', 'Minimum Pressure', 'Date_c', 'Year', 'Month', 'Day', 'Latitude_c', 'Longitude_c', 'Duration', 'sst', 'rhum', 'wspd', 'slp', 'cldc'], dtype='object') ## Fun\u00e7\u00f5es de Padroniza\u00e7\u00e3o: def standard_scale ( data , cols ): df = data . copy () for col in cols : mean = df [ col ] . mean () std = df [ col ] . std () df . loc [:, col ] = ( df [ col ] - mean ) / std return df [ cols ] def standard_scale_back ( scaled , original , cols ): df = scaled . copy () for col in cols : mean = original [ col ] . mean () std = original [ col ] . std () df . loc [:, col ] = df [ col ] * std + mean return df [ cols ] # Fun\u00e7\u00e3o que divide os dados por ID da tempestade def split ( df ): st = [] ids = df . ID . unique () for ID in ids : st . append ( df [ df . ID == ID ]) return st splitted = split ( data ) print ( len ( splitted )) 685 A fun\u00e7\u00e3o \"clean_data\" formata o preditor Tempo, o convertendo para horas (por padr\u00e3o, horas ap\u00f3s Jan/1951), e padroniza os dados de input. A fun\u00e7\u00e3o \"shift_data\", faz um shift dos dados e d\u00e1 como sa\u00edda as matriz tridimensional X e o vetor Y, onde X \u00e9 composto por matrizes s\\times n : \\left[\\begin{matrix} x_1^{(t-s+1)} & x_2^{(t-s+1)} & ...& x_n^{(t-s+1)}\\\\ x_1^{(t-s+2)} & x_2^{(t-s+2)} & ...& x_n^{(t-s+2)}\\\\ \\vdots &\\vdots &\\vdots &\\vdots\\\\ x_1^{(t)} & x_2^{(t)} & ...& x_n^{(t)}\\end{matrix}\\right] na qual: x_1,~x_2, ~..., ~x_n representam os n preditores usados (Usaremos 4 inicialmente: tempo latitude, longitude e velocidade de vento) x_i^{(k)} representa o preditor i no registro de tempo k . s \u00e9 o par\u00e2metros \"shift\", que representa em quantos per\u00edodos passados basearemos a previs\u00e3o futura. Por padr\u00e3o, inicialmente, deixamos s=3 . Como cada registro \u00e9 espa\u00e7ado por 6 horas, neste formado estamos usando daados do presente e de 12 horas atr\u00e1s, pra projetar o dado futuro. O vetor Y \u00e9 composto por: \\left[\\begin{matrix}lat^{(t+1)}&lon^{(t+1)}\\\\ lat^{(t+2)}&lon^{(t+2)}\\\\ \\vdots & \\vdots\\\\ lat^{(t+p)}&lon^{(t+p)}\\end{matrix}\\right] na qual: lat^{(k)} e lon^{(k)} representam latitude e longitude no registro de tempo k p \u00e9 o par\u00e2metro \"pred\", que diz quantos per\u00edodos \u00e0 frente iremos prever. Por padr\u00e3o, deixamos p=1 , assim a matriz se resume em um vetor [lat^{(t+1)},~ lon^{(t+1)}] . Usamos previs\u00f5es de 3 registros passados para prever o pr\u00f3ximo registro ap\u00f3s o \u00faltimo ponto de treino. Um trabalho futuro seria usar intervalos de previs\u00f5es maiores, mas vamos nos ater aqui a apenas 1 registro \u00e0 frete, o que equivale a 6 horas na grande maiorias dos pontos. cols1 = [ 'Hours' , 'Latitude' , 'Longitude' , 'Maximum Wind' ] data . loc [:, 'Time_new' ] = data . Date + data . Time . map ( lambda x : pd . Timedelta ( hours = x / 100 )) def clean_data ( df , input_cols = cols1 , year0 = 1951 ): df2 = df . copy () df2 . loc [:, 'Hours' ] = ( df2 . loc [:, 'Time_new' ] - pd . Timestamp ( year0 , 1 , 1 )) / pd . Timedelta ( '1 hour' ) df2 . loc [:, input_cols ] = standard_scale ( df2 , input_cols ) return df2 [[ 'ID' ] + input_cols ] def shift_data ( df , shift = 3 , pred = 1 ): x = [] y = [] df = df . set_index ( np . arange ( 0 , len ( df ))) for i in range ( 0 , len ( df ) - shift ): x_arr = [] for j in range ( i , i + shift ): x_arr . append ( df . loc [ j ,:]) if pred == 1 : y . append ( np . array ( df . loc [ i + shift : i + shift + pred - 1 ,[ 'Latitude' , 'Longitude' ]]) . ravel ()) else : y . append ( np . array ( df . loc [ i + shift : i + shift + pred - 1 ,[ 'Latitude' , 'Longitude' ]])) x . append ( np . array ( x_arr )) return np . array ( x ), np . array ( y ) data_cleaned = clean_data ( data ) data_cleaned . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Hours Latitude Longitude Maximum Wind count 2.238600e+04 2.238600e+04 2.238600e+04 2.238600e+04 mean -1.312271e-17 3.398752e-15 4.449279e-15 5.249630e-16 std 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 min -1.823465e+00 -1.939104e+00 -2.303286e+00 -1.644750e+00 25% -8.725997e-01 -8.005268e-01 -7.741070e-01 -8.529203e-01 50% 1.595638e-01 -4.466462e-02 -5.884587e-02 -2.590479e-01 75% 9.229854e-01 6.250867e-01 7.057436e-01 5.327820e-01 max 1.499554e+00 5.121988e+00 6.195990e+00 4.491932e+00","title":"Prepara\u00e7\u00e3o dos dados"},{"location":"NN-TrackPrediction/#treino-validacao-e-teste","text":"Para formatar os dados de treino, teste e valida\u00e7\u00e3o, primeiramente, usamos a fun\u00e7\u00e3o \"split\", que separa os dados em uma lista de dataframes onde cada um deles representa uma tempestade diferente. Ap\u00f3s isso atribuimos 70% da lista para treino, 20% para valida\u00e7\u00e3o e 10% para teste. Cada dataframe \u00e9 devidamente formatado para as matrizes acima usando a fun\u00e7\u00e3o \"shift_data\", e ap\u00f3s isso, unimos as matrizes de sa\u00edda para gerar a numpy array final, do modelo. Se fiz\u00e9ssemos o split diretamente, esse tipo de erro poderia acontecer. Suponha que uma tempestade tem seu \u00faltimo registro em 20 de Agosto em um determinado local, e uma outra tempestade se inicial dia 21 de Agosto em um local totalmente diferente. Um split direto poderia usar o registro de 20 de Agosto para prever o do dia 21, sendo que n\u00e3o h\u00e1 rela\u00e7\u00e3o entre eles. Essa separa\u00e7\u00e3o por tempestade \u00e9 importante para garantir a integridade dos nossos dados de modelagem, de forma que o treinamento seja feito tempestade por tempestade. def train_val_test_split ( df , t = 0.7 , v = 0.2 , input_cols = cols1 , shift = 3 , pred = 1 ): # t = fra\u00e7\u00e3o de treino # v = fra\u00e7\u00e3o de valida\u00e7\u00e3o # fra\u00e7\u00e3o de teste = 1-t-v # df = dataset j\u00e1 limpo por clean_data splitted_data = split ( df ) # Separamos os dados tempestade por tempestade para # evitar cruzamento de dados entre eventos diferentes n = len ( splitted ) train_storms = splitted_data [ 0 : int ( n * t )] val_storms = splitted_data [ int ( n * t ): int ( n * ( t + v ))] test_storms = splitted_data [ int ( n * ( 1 - t - v )):] #Geramos uma lista de matrizes, onde cada lista se refere #aos dados descocados de uma tempestade diferente xy_train = [ shift_data ( train [ input_cols ], shift , pred ) for train in train_storms ] xy_val = [ shift_data ( val [ input_cols ], shift , pred ) for val in val_storms ] xy_test = [ shift_data ( test [ input_cols ], shift , pred ) for test in test_storms ] # Concatena\u00e7\u00e3o das matrizes para gerar os dados finais xtrain = np . concatenate ([ x [ 0 ] for x in xy_train ], axis = 0 ) ytrain = np . concatenate ([ y [ 1 ] for y in xy_train ], axis = 0 ) xval = np . concatenate ([ x [ 0 ] for x in xy_val ], axis = 0 ) yval = np . concatenate ([ y [ 1 ] for y in xy_val ], axis = 0 ) xtest = np . concatenate ([ x [ 0 ] for x in xy_test ], axis = 0 ) ytest = np . concatenate ([ y [ 1 ] for y in xy_test ], axis = 0 ) return xtrain , ytrain , xval , yval , xtest , ytest %% time #O processo de split pode ser lento \u00e0 depender de seu computador xtrain , ytrain , xval , yval , xtest , ytest = train_val_test_split ( data_cleaned ) xtrain . shape , ytrain . shape , xval . shape , yval . shape , xtest . shape , ytest . shape CPU times : user 1 min 1 s , sys : 363 ms , total : 1 min 2 s Wall time : 1 min 1 s (( 14010 , 3 , 4 ), ( 14010 , 2 ), ( 4401 , 3 , 4 ), ( 4401 , 2 ), ( 18482 , 3 , 4 ), ( 18482 , 2 )) %% time ## Modelo que usa sst, rhum e slp (Temperatura do mar, Umidade e Press\u00e3o) cols2 = input_cols + [ 'sst' , 'rhum' , 'slp' , 'cldc' ] cleaned2 = clean_data ( data , cols2 ) xtrain2 , ytrain2 , xval2 , yval2 , xtest2 , ytest2 = train_val_test_split ( cleaned2 , input_cols = cols2 ) CPU times: user 1min 32s, sys: 470 ms, total: 1min 32s Wall time: 1min 31s xtrain2 . shape (14010, 3, 7)","title":"Treino, Valida\u00e7\u00e3o e Teste"},{"location":"NN-TrackPrediction/#modelos-redes-neurais","text":"Vamos criar alguns modelos determin\u00edsticos e depois comparar a m\u00e9dias dos erros ao quadrado (MSE) nos dados de valida\u00e7\u00e3o e teste. (Uma esp\u00e9cie de cross-validation). Segundo os artigos na qual usamos como refer\u00eancia (Veja Bibliografia), apenas uma camada interna \u00e9 suficiente. Resta saber quantos neur\u00f4nios usar, para isso avaliaremos os modelos com 3, 6, e 9 neur\u00f4nios internos. Usaremos a fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoid na camada interna. e linear na camada de output, j\u00e1 que os dados de sa\u00edda s\u00e3o num\u00e9ricos cont\u00ednuos. Usamos tamb\u00e9m uma camada de dropout de 15% para evitar overfitting. Ap\u00f3s eleger o melhor, incluiremos um fator probabil\u00edstico para gerar intervalos de confian\u00e7as nas predi\u00e7\u00f5es. \u00cdndice de Modelos Model_ihj --> Modelo com entrada de i vari\u00e1veis, h neur\u00f4nios na camada interna e j pontos de sa\u00edda (no nosso caso j=2 sempre) Model_4h2 --> Modelo que usa as vari\u00e1veis Tempo, Latitude, Longitude e Velocidame M\u00e1xima de Vento (4 vari\u00e1veis) de tr\u00eas registros passados para prever a Latitude e Longitude do pr\u00f3ximo registro (2 sa\u00eddas) usando h neur\u00f4nios na camada interna. Aqui geraremos 3 modelos para os valores de h sendo 3, 6 e 9. Model_8h2 --> Modelo similar ao anterior, por\u00e9m utiliza quatro vari\u00e1veis a mais como entrada: sst, rhum, slp, cldc; Representando Temperatura a n\u00edvel do mar, Umidade, Press\u00e3o a n\u00edvel do mar e Cobertura de Nuvens. # Modelos tipo 4h2 model_432 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 4 )), tf . keras . layers . Dense ( 3 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_462 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 4 )), tf . keras . layers . Dense ( 6 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_492 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 4 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) # Modelos tipo 8h2 model_832 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 3 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_862 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 6 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) model_892 = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 3 , 8 )), tf . keras . layers . Dense ( 9 , activation = 'sigmoid' ), tf . keras . layers . Dropout ( 0.15 ), tf . keras . layers . Dense ( 2 , activation = 'linear' ) ]) loss_fn = tf . keras . losses . MeanSquaredError () optimizer = tf . keras . optimizers . Adam () model_432 . compile ( optimizer = optimizer , loss = loss_fn ) model_462 . compile ( optimizer = optimizer , loss = loss_fn ) model_492 . compile ( optimizer = optimizer , loss = loss_fn ) model_832 . compile ( optimizer = optimizer , loss = loss_fn ) model_862 . compile ( optimizer = optimizer , loss = loss_fn ) model_892 . compile ( optimizer = optimizer , loss = loss_fn ) model_432 . summary () Model: \"sequential_6\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_6 (Flatten) (None, 12) 0 _________________________________________________________________ dense_12 (Dense) (None, 3) 39 _________________________________________________________________ dropout_6 (Dropout) (None, 3) 0 _________________________________________________________________ dense_13 (Dense) (None, 2) 8 ================================================================= Total params: 47 Trainable params: 47 Non-trainable params: 0 _________________________________________________________________ %% time history_432 = model_432 . fit ( xtrain , ytrain , validation_data = ( xval , yval ), epochs = 45 , verbose = 0 ) history_462 = model_462 . fit ( xtrain , ytrain , validation_data = ( xval , yval ), epochs = 45 , verbose = 0 ) history_492 = model_492 . fit ( xtrain , ytrain , validation_data = ( xval , yval ), epochs = 45 , verbose = 1 ) Train on 14010 samples, validate on 4401 samples Epoch 1/45 14010/14010 [==============================] - 1s 80us/sample - loss: 0.0757 - val_loss: 0.0101 Epoch 2/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0779 - val_loss: 0.0099 Epoch 3/45 14010/14010 [==============================] - 1s 92us/sample - loss: 0.0779 - val_loss: 0.0109 Epoch 4/45 14010/14010 [==============================] - 2s 121us/sample - loss: 0.0760 - val_loss: 0.0100 Epoch 5/45 14010/14010 [==============================] - 2s 111us/sample - loss: 0.0767 - val_loss: 0.0096 Epoch 6/45 14010/14010 [==============================] - 2s 152us/sample - loss: 0.0742 - val_loss: 0.0121 Epoch 7/45 14010/14010 [==============================] - 1s 90us/sample - loss: 0.0766 - val_loss: 0.0119 Epoch 8/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0758 - val_loss: 0.0108 Epoch 9/45 14010/14010 [==============================] - 1s 71us/sample - loss: 0.0779 - val_loss: 0.0107 Epoch 10/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0747 - val_loss: 0.0097 Epoch 11/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0771 - val_loss: 0.0106 Epoch 12/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0776 - val_loss: 0.0102 Epoch 13/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0786 - val_loss: 0.0102 Epoch 14/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0782 - val_loss: 0.0099 Epoch 15/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0783 - val_loss: 0.0106 Epoch 16/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0774 - val_loss: 0.0098 Epoch 17/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0758 - val_loss: 0.0102 Epoch 18/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0783 - val_loss: 0.0099 Epoch 19/45 14010/14010 [==============================] - 2s 115us/sample - loss: 0.0749 - val_loss: 0.0103 Epoch 20/45 14010/14010 [==============================] - 2s 134us/sample - loss: 0.0789 - val_loss: 0.0093 Epoch 21/45 14010/14010 [==============================] - 2s 135us/sample - loss: 0.0778 - val_loss: 0.0104 Epoch 22/45 14010/14010 [==============================] - 1s 83us/sample - loss: 0.0761 - val_loss: 0.0093 Epoch 23/45 14010/14010 [==============================] - 1s 91us/sample - loss: 0.0763 - val_loss: 0.0105 Epoch 24/45 14010/14010 [==============================] - 2s 115us/sample - loss: 0.0769 - val_loss: 0.0095 Epoch 25/45 14010/14010 [==============================] - 1s 99us/sample - loss: 0.0812 - val_loss: 0.0086 Epoch 26/45 14010/14010 [==============================] - 2s 114us/sample - loss: 0.0768 - val_loss: 0.0106 Epoch 27/45 14010/14010 [==============================] - 1s 100us/sample - loss: 0.0782 - val_loss: 0.0101 Epoch 28/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0767 - val_loss: 0.0083 Epoch 29/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0795 - val_loss: 0.0094 Epoch 30/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0779 - val_loss: 0.0104 Epoch 31/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0768 - val_loss: 0.0101 Epoch 32/45 14010/14010 [==============================] - 1s 77us/sample - loss: 0.0764 - val_loss: 0.0094 Epoch 33/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0765 - val_loss: 0.0091 Epoch 34/45 14010/14010 [==============================] - 1s 76us/sample - loss: 0.0767 - val_loss: 0.0106 Epoch 35/45 14010/14010 [==============================] - 1s 70us/sample - loss: 0.0780 - val_loss: 0.0099 Epoch 36/45 14010/14010 [==============================] - 1s 74us/sample - loss: 0.0755 - val_loss: 0.0094 Epoch 37/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0764 - val_loss: 0.0095 Epoch 38/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0782 - val_loss: 0.0098 Epoch 39/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0783 - val_loss: 0.0087 Epoch 40/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0737 - val_loss: 0.0091 Epoch 41/45 14010/14010 [==============================] - 1s 72us/sample - loss: 0.0775 - val_loss: 0.0094 Epoch 42/45 14010/14010 [==============================] - 1s 75us/sample - loss: 0.0792 - val_loss: 0.0101 Epoch 43/45 14010/14010 [==============================] - 1s 78us/sample - loss: 0.0762 - val_loss: 0.0087 Epoch 44/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0754 - val_loss: 0.0100 Epoch 45/45 14010/14010 [==============================] - 1s 73us/sample - loss: 0.0767 - val_loss: 0.0093 CPU times: user 3min 50s, sys: 15.8 s, total: 4min 6s Wall time: 2min 50s %% time history_832 = model_832 . fit ( xtrain2 , ytrain2 , validation_data = ( xval2 , yval2 ), epochs = 45 , verbose = 0 ) history_862 = model_862 . fit ( xtrain2 , ytrain2 , validation_data = ( xval2 , yval2 ), epochs = 45 , verbose = 0 ) history_892 = model_892 . fit ( xtrain2 , ytrain2 , validation_data = ( xval2 , yval2 ), epochs = 45 , verbose = 1 ) Train on 14010 samples, validate on 4401 samples Epoch 1/45 14010/14010 [==============================] - 2s 111us/sample - loss: 0.3210 - val_loss: 0.0874 Epoch 2/45 14010/14010 [==============================] - 1s 58us/sample - loss: 0.1670 - val_loss: 0.0666 Epoch 3/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.1460 - val_loss: 0.0563 Epoch 4/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.1326 - val_loss: 0.0473 Epoch 5/45 14010/14010 [==============================] - 1s 66us/sample - loss: 0.1232 - val_loss: 0.0398 Epoch 6/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.1170 - val_loss: 0.0334 Epoch 7/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.1055 - val_loss: 0.0274 Epoch 8/45 14010/14010 [==============================] - 1s 62us/sample - loss: 0.1025 - val_loss: 0.0236 Epoch 9/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0974 - val_loss: 0.0209 Epoch 10/45 14010/14010 [==============================] - 1s 63us/sample - loss: 0.0945 - val_loss: 0.0191 Epoch 11/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0924 - val_loss: 0.0179 Epoch 12/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0893 - val_loss: 0.0174 Epoch 13/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0884 - val_loss: 0.0182 Epoch 14/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0874 - val_loss: 0.0178 Epoch 15/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0844 - val_loss: 0.0163 Epoch 16/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0847 - val_loss: 0.0155 Epoch 17/45 14010/14010 [==============================] - 1s 58us/sample - loss: 0.0852 - val_loss: 0.0161 Epoch 18/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0871 - val_loss: 0.0157 Epoch 19/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0825 - val_loss: 0.0161 Epoch 20/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0817 - val_loss: 0.0148 Epoch 21/45 14010/14010 [==============================] - 1s 62us/sample - loss: 0.0853 - val_loss: 0.0149 Epoch 22/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0833 - val_loss: 0.0158 Epoch 23/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0804 - val_loss: 0.0156 Epoch 24/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0819 - val_loss: 0.0144 Epoch 25/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0836 - val_loss: 0.0142 Epoch 26/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0830 - val_loss: 0.0146 Epoch 27/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0834 - val_loss: 0.0146 Epoch 28/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0831 - val_loss: 0.0153 Epoch 29/45 14010/14010 [==============================] - 1s 62us/sample - loss: 0.0812 - val_loss: 0.0141 Epoch 30/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0820 - val_loss: 0.0141 Epoch 31/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0822 - val_loss: 0.0132 Epoch 32/45 14010/14010 [==============================] - 1s 58us/sample - loss: 0.0810 - val_loss: 0.0129 Epoch 33/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0824 - val_loss: 0.0147 Epoch 34/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0812 - val_loss: 0.0138 Epoch 35/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0816 - val_loss: 0.0138 Epoch 36/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0803 - val_loss: 0.0121 Epoch 37/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0767 - val_loss: 0.0133 Epoch 38/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0780 - val_loss: 0.0125 Epoch 39/45 14010/14010 [==============================] - 1s 60us/sample - loss: 0.0783 - val_loss: 0.0133 Epoch 40/45 14010/14010 [==============================] - 1s 57us/sample - loss: 0.0795 - val_loss: 0.0121 Epoch 41/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0784 - val_loss: 0.0121 Epoch 42/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0807 - val_loss: 0.0121 Epoch 43/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0780 - val_loss: 0.0125 Epoch 44/45 14010/14010 [==============================] - 1s 61us/sample - loss: 0.0811 - val_loss: 0.0144 Epoch 45/45 14010/14010 [==============================] - 1s 59us/sample - loss: 0.0791 - val_loss: 0.0134 CPU times: user 3min 13s, sys: 12.5 s, total: 3min 25s Wall time: 2min 20s # Salvando os modelos para uso futuro model_432 . save ( 'Saved_NN_Models/model_432.h5' ) model_462 . save ( 'Saved_NN_Models/model_462.h5' ) model_492 . save ( 'Saved_NN_Models/model_492.h5' ) model_832 . save ( 'Saved_NN_Models/model_832.h5' ) model_862 . save ( 'Saved_NN_Models/model_862.h5' ) model_892 . save ( 'Saved_NN_Models/model_892.h5' ) # Recreando os modelos dos arquivos salvos # model_432 = tf.keras.models.load_model('Saved_NN_Models/model_432.h5') # model_462 = tf.keras.models.load_model('Saved_NN_Models/model_462.h5') # model_492 = tf.keras.models.load_model('Saved_NN_Models/model_492.h5') # model_832 = tf.keras.models.load_model('Saved_NN_Models/model_832.h5') # model_862 = tf.keras.models.load_model('Saved_NN_Models/model_862.h5') # model_892 = tf.keras.models.load_model('Saved_NN_Models/model_892.h5') models4 = [ model_432 , model_462 , model_492 ] models8 = [ model_832 , model_862 , model_892 ] history_list = [ history_432 , history_462 , history_492 , history_832 , history_862 , history_892 ] labels = [ \"4-3-2\" , '4-6-2' , '4-9-2' , \"8-3-2\" , '8-6-2' , '8-9-2' ] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) test_mse = [] train_mse = [] val_mse = [] for model in models4 : test_mse . append ( model . evaluate ( xtest , ytest , verbose = 0 )) for model in models8 : test_mse . append ( model . evaluate ( xtest2 , ytest2 , verbose = 0 )) for history in history_list : train_mse . append ( np . mean ( history . history [ 'loss' ])) val_mse . append ( np . mean ( history . history [ 'val_loss' ])) wid = 0.8 / 3 - 0.0001 ax . set_title ( \"Perda (MSE) dos Modelos - Dados de Teste\" , fontsize = 16 ) ax . set_ylabel ( \"MSE\" , fontsize = 14 ) ax . bar ( np . arange ( 0 , 6 ) - wid , train_mse , label = \"Treino\" , width = wid ) ax . bar ( np . arange ( 0 , 6 ), val_mse , label = 'Valida\u00e7\u00e3o' , width = wid ) ax . bar ( np . arange ( 0 , 6 ) + wid , test_mse , label = \"Teste\" , width = wid ) ax . set_xticklabels ([ '0' ] + labels , fontsize = 12 ) ax . legend ( loc = 'best' , fontsize = 14 ); plt . savefig ( 'figs/NN_Models_MSE.jpg' ) Vemos que os modelos 4-9-2 e 8-9-2 performaram melhor em todos os conjuntos de dados. Vemos que 9 neur\u00f4nios internos performam muito bem. Vamos trabalhar com esses modelos a partir daqui. Em especial, com o 8-9-2 que apesar de ter uma maior perda do que o 4-9-2, ele utiliza uma maior quantidade de informa\u00e7\u00f5es clim\u00e1ticas. # plot accuracy and loss for the test set fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) ax . plot ( history_492 . history [ 'loss' ]) ax . plot ( history_492 . history [ 'val_loss' ]) ax . set_title ( '4-9-2 MSE' , fontsize = 16 ) ax . set_ylabel ( 'MSE' ) ax . set_xlabel ( 'epoch' ) ax . legend ([ 'Treino' , 'Valida\u00e7\u00e3o' ], fontsize = 14 , loc = 'best' ) ax1 . plot ( history_892 . history [ 'loss' ]) ax1 . plot ( history_892 . history [ 'val_loss' ]) ax1 . set_title ( '8-9-2 MSE' , fontsize = 16 ) ax1 . set_ylabel ( 'MSE' ) ax1 . set_xlabel ( 'epoch' ) ax1 . legend ([ 'Treino' , 'Valida\u00e7\u00e3o' ], fontsize = 14 , loc = 'best' ); plt . savefig ( 'figs/MSE-epoch.jpg' ) from sklearn.metrics import r2_score ypred = model_892 . predict ( xtest2 ) # from sklearn.linear_model import LinearRegression # lr = LinearRegression().fit(ytest,ypred) # lr.score(ytest,ypred) lat_r2 = r2_score ( ytest2 [:, 0 ], ypred [:, 0 ]) lon_r2 = r2_score ( ytest2 [:, 1 ], ypred [:, 1 ]) tot_r2 = r2_score ( ytest2 , ypred ) print ( f \"R2 Latitude Teste - { lat_r2 } \" ) print ( f \"R2 Longitude Teste - { lon_r2 } \" ) print ( f \"R2 Total Teste - { tot_r2 } \" ) fig , ( ax , ax1 ) = plt . subplots ( 1 , 2 , figsize = ( 18 , 6 )) ax . set_title ( \"Latitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax . set_xlabel ( \"Real\" , fontsize = 13 ) ax . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax . scatter ( ytest2 [:, 0 ], ypred [:, 0 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lat_r2 , 3 ) } \" ) ax . legend ( loc = 'best' , fontsize = 13 ) ax1 . set_title ( \"Longitude Real vs Prevista (Teste 8-9-2)\" , fontsize = 16 ) ax1 . set_xlabel ( \"Real\" , fontsize = 13 ) ax1 . set_ylabel ( \"Prevista\" , fontsize = 13 ) ax1 . scatter ( ytest2 [:, 1 ], ypred [:, 1 ], alpha = 0.75 , color = 'g' , label = f \"R2 = { round ( lon_r2 , 3 ) } \" ) ax1 . legend ( loc = 'best' , fontsize = 13 ); plt . savefig ( 'figs/lat_lon_teste.jpg' ) R2 Latitude Teste - 0.988149240276217 R2 Longitude Teste - 0.9864725794069319 R2 Total Teste - 0.9873109098415744 tf . keras . utils . plot_model ( model_892 , to_file = 'figs/model_892.png' , show_shapes = True , show_layer_names = True , rankdir = 'TB' , expand_nested = False , dpi = 96 ) # plt.savefig('figs/model892.jpg') data_test = data . loc [ int ( n * 0.9 ):,:] data_test . loc [:, 'Hours' ] = ( data_test . loc [:, 'Time_new' ] - pd . Timestamp ( 1951 , 1 , 1 )) / pd . Timedelta ( '1 hour' ) # data_test[data_test.Category==\"Category 3\"] irene = data_test [ data_test . ID == 'AL092011' ] irene . loc [:, cols2 ] = standard_scale ( irene , cols2 ) ir = irene . loc [:, cols2 ] def predict ( storm , model , shift = 3 , pred = 1 ): storm = storm . set_index ( np . arange ( 0 , len ( storm ))) y_pred = [] for i in range ( 0 , len ( storm ) - shift - 1 ): x = [] for j in range ( i , i + shift ): x . append ( storm . loc [ j ,:]) # if i == 0: # print(np.expand_dims(np.asarray(x), axis=0).shape) # print(np.expand_dims(np.asarray(x),axis=0)[0,0,0]) y_pred . append ( model . predict ( np . expand_dims ( np . asarray ( x ), axis = 0 )) . ravel ()) del x return np . array ( y_pred ) y_pred = predict ( ir , model_892 ) y_pred . shape (39, 2) ir_plot = ir . iloc [ 0 : - 4 ,:] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) fig . suptitle ( \"Hurricane Irene - 2011\" , fontsize = 16 , y = 1.08 ) ax [ 0 ] . set_title ( \"Latitude\" ) ax [ 1 ] . set_title ( \"Longitude\" ) print ( r2_score ( ir_plot . Latitude , y_pred [:, 0 ])) print ( r2_score ( ir_plot . Longitude , y_pred [:, 1 ])) ax [ 0 ] . scatter ( ir_plot . Hours , ir_plot . Latitude , label = 'True' ) ax [ 0 ] . scatter ( ir_plot . Hours , y_pred [:, 0 ], label = 'Predicted' ) ax [ 1 ] . scatter ( ir_plot . Hours , ir_plot . Longitude , label = 'True' ) ax [ 1 ] . scatter ( ir_plot . Hours , y_pred [:, 1 ], label = 'Predicted' ) ax [ 0 ] . legend ( loc = 'best' ) ax [ 1 ] . legend ( loc = 'best' ) 0 . 9124620701244499 0 . 8651593770302984 < matplotlib . legend . Legend at 0 x7fcb1e25b390 > # input_cols = ['Hours','Latitude','Longitude','Maximum Wind'] joaquin = data_test [ data_test . ID == 'AL112015' ] joaquin . loc [:, cols2 ] = standard_scale ( joaquin , cols2 ) y_jq = predict ( joaquin [ cols2 ], model_892 ) jq_plot = joaquin [ input_cols ] . iloc [ 0 : - 4 ,:] fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) fig . suptitle ( \"Hurricane Joaquin - 2015\" , fontsize = 16 , y = 1.08 ) ax [ 0 ] . set_title ( \"Latitude\" ) ax [ 1 ] . set_title ( \"Longitude\" ) print ( r2_score ( jq_plot . Latitude , y_jq [:, 0 ])) print ( r2_score ( jq_plot . Longitude , y_jq [:, 1 ])) ax [ 0 ] . scatter ( jq_plot . Hours , jq_plot . Latitude , label = 'True' ) ax [ 0 ] . scatter ( jq_plot . Hours , y_jq [:, 0 ], label = 'Predicted' ) ax [ 1 ] . scatter ( jq_plot . Hours , jq_plot . Longitude , label = 'True' ) ax [ 1 ] . scatter ( jq_plot . Hours , y_jq [:, 1 ], label = 'Predicted' ) ax [ 0 ] . legend ( loc = 'best' ) ax [ 1 ] . legend ( loc = 'best' ) 0 . 9645696502595049 0 . 9816239209481076 < matplotlib . legend . Legend at 0 x7fcb1c9b9b50 >","title":"Modelos - Redes Neurais"},{"location":"PowerDissipationIndex/","text":"Power Dissipation Index (PDI) Analysis Definitions PDI is an index that represents the destructive power of a storm combining together, intensity, duration, and frequency. References: Emanuel, 2005 and Emanuel, 2007 In the references, Kerry Emanuel defines the index as: PDI\\equiv\\int_0^{\\tau}V^3_{max}dt~, where V_{max} is the maximum sustained wind speed, and \\tau is the lifetime of the storm event. The PDI Dataset We're gonna use the PDI calculated by National Oceanic & Atmospheric Administration (NOAA) which data is avaible at Our World in Data . It covers the North Atlantic, Caribbean and Gulf of Mexico storms. The data has been smoothed through a five-year weighted average plotted at the center, in order to remove interannual variability. We're gonna o the same smooth with our climate dataset of Atlantic MDR. import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score import scipy.stats as stats from math import * from statsmodels.graphics.tsaplots import plot_acf , plot_pacf from statsmodels.tsa.holtwinters import ExponentialSmoothing , HoltWintersResults from statsmodels.tsa.stattools import adfuller from sklearn.model_selection import TimeSeriesSplit import datetime as dt sns . set () % matplotlib inline raw_pdi = pd . read_csv ( 'Datasets/cyclone-power-dissipation-index.csv' ) raw_pdi . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entity Code Year Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) 0 North Atlantic NaN 1951 2.7846 1 North Atlantic NaN 1952 2.3445 2 North Atlantic NaN 1953 2.2639 3 North Atlantic NaN 1954 2.4730 4 North Atlantic NaN 1955 2.4041 raw_pdi . dtypes Entity object Code float64 Year int64 Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) float64 dtype: object PDI = raw_pdi [[ 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' ]] . rename ( columns = { 'Year' : 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' : 'PDI' }) PDI = PDI . set_index ( 'Year' ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 9 )) axs [ 0 , 0 ] . set_title ( \"Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 0 ] . set_ylabel ( \"PDI\" , fontsize = 16 ) axs [ 0 , 0 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 0 ] . plot ( PDI . index , PDI . PDI , lw = 4 ); axs [ 0 , 1 ] . set_title ( \"Logarithm of Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 1 ] . set_ylabel ( \"log(PDI)\" , fontsize = 16 ) axs [ 0 , 1 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 1 ] . plot ( PDI . index , np . log ( PDI . PDI ), lw = 4 ) sns . distplot ( PDI . PDI , ax = axs [ 1 , 0 ]) sns . distplot ( np . log ( PDI . PDI ), ax = axs [ 1 , 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ef52a30> Atlantic MDR Climate Data Let's do the same smoothing operation in our \"atlantic_mdr\" dataset. We're gonna apply a 1\u20133\u20134\u20133\u20131 weighted average Emanuel, 2007 on the data, to get out of interannual variability. atlantic_mdr = pd . read_csv ( './Datasets/atlantic_mdr.csv' ) def smooth ( col ): n = len ( col ) new_col = np . zeros ([ n , 1 ]) w = np . array ([[ 1 , 3 , 4 , 3 , 1 ]]) for i in range ( 2 , n - 2 ): new_col [ i ] = w . dot ( np . array ( col [ i - 2 : i + 3 ]) . reshape ( - 1 , 1 )) / 12 return new_col . ravel () # atlantic_mdr = atlantic_mdr[(atlantic_mdr.Month>=8) & (atlantic_mdr.Month<=10)] mdr_annual = atlantic_mdr . groupby ( 'Year' ) . agg ({ 'sst' : np . mean , 'rhum' : np . mean , 'wspd' : np . mean , 'slp' : np . mean , 'vwnd' : np . mean , 'cldc' : np . mean }) for col in mdr_annual . columns : mdr_annual . loc [:, col ] = smooth ( mdr_annual [ col ]) mdr_annual = mdr_annual . loc [ 1951 : 2013 ,:] # mdr_annual mdr_annual [ 'PDI' ] = np . array ( PDI . PDI ) . reshape ( - 1 , 1 ) # mdr_annual Analysing Correlation # corr = mdr_annual.corr() # corr.style.background_gradient(cmap='coolwarm') # corr = df.corr() # corr.style.background_gradient(cmap='coolwarm') f , axs = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( mdr_annual . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 0 ]); df = mdr_annual . copy () df [ 'PDI' ] = np . log ( df [ 'PDI' ]) df = df . rename ( columns = { \"PDI\" : \"logPDI\" }) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( df . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 1 ]); plt . show () fig , ax = plt . subplots ( figsize = ( 10 , 6 )) plt . title ( \"North Atlantic MDR SST vs PDI\" , fontsize = 18 ) plt . xlabel ( \"Year\" , fontsize = 14 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) ax2 = ax . twinx () ax . plot ( mdr_annual . sst , lw = 4 , label = \"SST\" ); ax . set_ylabel ( \"Sea Surface Temperature\" , fontsize = 14 ) ax2 . plot ( mdr_annual . PDI , lw = 4 , label = \"PDI\" , color = 'orange' ); ax2 . set_ylabel ( \"Power Dissipation Index\" , fontsize = 14 ) fig . legend ( loc = \"upper left\" , fontsize = 14 , bbox_to_anchor = ( 0 , 1 ), bbox_transform = ax . transAxes ); # sns.scatterplot(x=np.log(mdr_annual['sst']), y=np.log(mdr_annual['PDI'])) # sns.scatterplot(x=X.sst, y=mdr_annual['PDI']) Simple Linear Model Quanto \u00e0 trajet\u00f3ria do PDI, \u00e9 observ\u00e1vel uma leve tend\u00eancia positiva. # Aparentemente o PDI cresceu: X = mdr_annual . drop ( columns = \"PDI\" ) X = sm . add_constant ( X ) Y = np . array ( mdr_annual [ 'PDI' ]) . reshape ( - 1 , 1 ) X [ 'Year' ] = np . array ( X . index ) model = sm . OLS ( Y , sm . add_constant ( X [ 'Year' ])) . fit () plt . plot ( PDI ) plt . plot ( model . fittedvalues ) plt . title ( \"Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta\" ) Text(0.5, 1.0, 'Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta') Usando as covari\u00e1veis do dataset, podemos fazer um modelo onde achamos rela\u00e7\u00e3o com a press\u00e3o ao nivel do mar (slp), a humidade (rhum) e a \"cloudin. # X['sst'] = np.log(X['sst']) ( xtrain , xtest , ytrain , ytest ) = train_test_split ( X . drop ( columns = [ \"sst\" , \"wspd\" , \"vwnd\" , \"Year\" ]), np . log ( Y )) model = sm . OLS ( ytrain , xtrain ) . fit () ypred = model . predict ( xtest ) print ( \"R2 nos dados separados para teste:\" , r2_score ( ytest , ypred )) print ( model . summary ()) R2 nos dados separados para teste : 0 . 2268374488496503 OLS Regression Results ============================================================================== Dep . Variable : y R - squared : 0 . 781 Model : OLS Adj . R - squared : 0 . 766 Method : Least Squares F - statistic : 51 . 26 Date : Mon , 31 Aug 2020 Prob ( F - statistic ): 2 . 98 e - 14 Time : 10 : 22 : 52 Log - Likelihood : 14 . 019 No . Observations : 47 AIC : - 20 . 04 Df Residuals : 43 BIC : - 12 . 64 Df Model : 3 Covariance Type : nonrobust ============================================================================== coef std err t P >| t | [ 0 . 025 0 . 975 ] ------------------------------------------------------------------------------ const - 484 . 2535 146 . 318 - 3 . 310 0 . 002 - 779 . 332 - 189 . 175 rhum 0 . 7547 0 . 079 9 . 497 0 . 000 0 . 594 0 . 915 slp 0 . 4106 0 . 143 2 . 871 0 . 006 0 . 122 0 . 699 cldc 1 . 9827 0 . 229 8 . 641 0 . 000 1 . 520 2 . 445 ============================================================================== Omnibus : 4 . 476 Durbin - Watson : 2 . 046 Prob ( Omnibus ): 0 . 107 Jarque - Bera ( JB ): 3 . 413 Skew : - 0 . 468 Prob ( JB ): 0 . 182 Kurtosis : 3 . 932 Cond . No . 5 . 42 e + 06 ============================================================================== Warnings : [ 1 ] Standard Errors assume that the covariance matrix of the errors is correctly specified . [ 2 ] The condition number is large , 5 . 42 e + 06 . This might indicate that there are strong multicollinearity or other numerical problems . Pode parecer um pouco contraintuitivo a princ\u00edpio n\u00e3o achar rela\u00e7\u00e3o significativa da temperatura do mar e da intensidade (na verdade, a destrutibilidade, o PDI) dos furac\u00f5es. Uma explica\u00e7\u00e3o plausivel para isso \u00e9 que os dados de temperatura conseguem ser explicados pelas outras vari\u00e1veis: model = sm . OLS ( X [ 'sst' ], X . drop ( columns = [ 'vwnd' , 'wspd' , 'Year' , 'sst' ])) . fit () print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: sst R-squared: 0.749 Model: OLS Adj. R-squared: 0.736 Method: Least Squares F-statistic: 58.61 Date: Mon, 31 Aug 2020 Prob (F-statistic): 1.08e-17 Time: 10:22:52 Log-Likelihood: 53.106 No. Observations: 63 AIC: -98.21 Df Residuals: 59 BIC: -89.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -177.9485 73.852 -2.410 0.019 -325.726 -30.171 rhum 0.2643 0.040 6.528 0.000 0.183 0.345 slp 0.1764 0.072 2.446 0.017 0.032 0.321 cldc 1.3740 0.113 12.204 0.000 1.149 1.599 ============================================================================== Omnibus: 0.296 Durbin-Watson: 0.473 Prob(Omnibus): 0.862 Jarque-Bera (JB): 0.479 Skew: -0.023 Prob(JB): 0.787 Kurtosis: 2.575 Cond. No. 5.52e+06 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 5.52e+06. This might indicate that there are strong multicollinearity or other numerical problems. Time series Analysis tsPDI = pd . DataFrame ( Y , index = [ dt . datetime ( x , 1 , 1 ) for x in mdr_annual . index ], columns = [ \"PDI\" ]) tsPDI . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PDI 1951-01-01 2.7846 1952-01-01 2.3445 1953-01-01 2.2639 1954-01-01 2.4730 1955-01-01 2.4041 Vamos tratar agora a trajet\u00f3ria do PDI como uma s\u00e9rie temporal. Inicialmente, vamos ajustar um modelo autorregressivo de ordem 2: mod = sm . tsa . ARIMA ( tsPDI , order = ( 2 , 0 , 0 )) modfit = mod . fit () # modfit.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit.resid, ax = axs[0, 0]) # plot_pacf(modfit.resid, ax = axs[0, 1]) # sm.qqplot(modfit.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit.resid, ax = axs[1, 1]) # plt.show()# modfit.plot_diagnostics() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia das previs\u00f5es com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Modelando o log da s\u00e9rie: tsLogPDI = tsPDI . apply ( np . log ) #sm.tsa.stattools.adfuller(tsLogPDI['PDI']) ainda n\u00e3o \u00e9 estacionaria a 5% de significancia mod2 = sm . tsa . ARIMA ( tsLogPDI , order = ( 2 , 0 , 0 )) modfit2 = mod2 . fit () # modfit2.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit2.resid, ax = axs[0, 0]) # plot_pacf(modfit2.resid, ax = axs[0, 1]) # sm.qqplot(modfit2.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit2.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: sims = np . zeros (( 1000 , step )) for i in range ( 1000 ): sims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( sims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 30 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 15 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () # plt.plot(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) # sm.tsa.stattools.adfuller(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) tsTr = np . log ( np . diff ( np . log ( tsPDI [ 'PDI' ])) + 1 ) # terceiro modelo: log(delta(log(PDI))+1) como ARMA(2,1) mod3 = sm . tsa . ARIMA ( tsTr , order = ( 2 , 0 , 1 )) modfit3 = mod3 . fit () # modfit3.summary() # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit3.resid, ax = axs[0, 0]) # plot_pacf(modfit3.resid, ax = axs[0, 1]) # sm.qqplot(modfit3.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit3.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsTr ) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = range ( len ( tsTr ), len ( tsTr ) + step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsTr ) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia da transforma\u00e7\u00e3o predita com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es da transforma\u00e7\u00e3o' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = last + np . cumsum ( dsims [ i ]) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 50 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do log(PDI)' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = np . exp ( last + np . cumsum ( dsims [ i ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Count Medir destrutibilidade do furac\u00e3o n\u00e3o \u00e9 coisa trivial; n\u00e3o basta olhar o PDI visto que os nossos dados de PDI podem estar errados por medi\u00e7\u00f5es inconsistentes de velocidade at\u00e9 os anos 90, onde come\u00e7amos a ter um comportamento an\u00f4malo do ENSO; al\u00e9m da din\u00e2mica sazonal de correntes termosal\u00ednicas gerarem sazonalidade nas temperaturas entre dec\u00e1das (Atlantic Multidecadal Oscilation, AMO). Outras alternativas comumente consideradas s\u00e3o a de contagem de furac\u00f5es no ano e de tamanho de cada furac\u00e3o; cada uma delas com seus problemas particulares. Abaixo, fazemos um modelo simples de GLM para contagem usando como preditores as covari data = pd . read_csv ( 'Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data = data [[ 'ID' , 'Name' , 'Date' , 'Year' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ]] def category ( mw ): if mw >= 137 : return \"Category 5\" elif mw >= 113 : return \"Category 4\" elif mw >= 96 : return \"Category 3\" elif mw >= 83 : return \"Category 2\" elif mw >= 64 : return \"Category 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" cat_id = data . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) data . loc [:, 'Category' ] = data . ID . apply ( lambda x : cat_id [ x ]) # data.groupby('Category')['ID'].count().plot.bar(); major_df = data [ data . Category . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] major_df . groupby ( 'Year' )[ 'ID' ] . count () . plot () #sm.tsa.stattools.adfuller(count) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ec16a60> count = major_df . groupby ( \"Year\" )[ \"ID\" ] . count () X = pd . pivot_table ( major_df , index = \"Year\" , values = [ \"sst\" , \"rhum\" , \"wspd\" , \"slp\" , \"cldc\" ]) X [ 'Y' ] = X . index . values countModel = sm . GLM ( count , sm . add_constant ( X ), family = sm . families . NegativeBinomial ()) countFit = countModel . fit () print ( countFit . summary ()) #descomente para ver o sumario do ajuste plt . plot ( count ) plt . plot ( countFit . predict ( sm . add_constant ( X ))) plt . show () Generalized Linear Model Regression Results ============================================================================== Dep. Variable: ID No. Observations: 60 Model: GLM Df Residuals: 54 Model Family: NegativeBinomial Df Model: 5 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -347.61 Date: Mon, 31 Aug 2020 Deviance: 26.626 Time: 10:22:58 Pearson chi2: 26.3 No. Iterations: 12 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -146.2316 147.049 -0.994 0.320 -434.443 141.980 cldc 0.1977 0.589 0.336 0.737 -0.956 1.351 rhum 0.0182 0.067 0.271 0.786 -0.113 0.150 slp 0.1338 0.142 0.940 0.347 -0.145 0.413 sst 0.0952 0.132 0.723 0.470 -0.163 0.353 wspd 0.0182 0.067 0.271 0.786 -0.113 0.150 Y 0.0048 0.008 0.602 0.547 -0.011 0.020 ==============================================================================","title":"Destructivity"},{"location":"PowerDissipationIndex/#power-dissipation-index-pdi-analysis","text":"","title":"Power Dissipation Index (PDI) Analysis"},{"location":"PowerDissipationIndex/#definitions","text":"PDI is an index that represents the destructive power of a storm combining together, intensity, duration, and frequency. References: Emanuel, 2005 and Emanuel, 2007 In the references, Kerry Emanuel defines the index as: PDI\\equiv\\int_0^{\\tau}V^3_{max}dt~, where V_{max} is the maximum sustained wind speed, and \\tau is the lifetime of the storm event.","title":"Definitions"},{"location":"PowerDissipationIndex/#the-pdi-dataset","text":"We're gonna use the PDI calculated by National Oceanic & Atmospheric Administration (NOAA) which data is avaible at Our World in Data . It covers the North Atlantic, Caribbean and Gulf of Mexico storms. The data has been smoothed through a five-year weighted average plotted at the center, in order to remove interannual variability. We're gonna o the same smooth with our climate dataset of Atlantic MDR. import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score import scipy.stats as stats from math import * from statsmodels.graphics.tsaplots import plot_acf , plot_pacf from statsmodels.tsa.holtwinters import ExponentialSmoothing , HoltWintersResults from statsmodels.tsa.stattools import adfuller from sklearn.model_selection import TimeSeriesSplit import datetime as dt sns . set () % matplotlib inline raw_pdi = pd . read_csv ( 'Datasets/cyclone-power-dissipation-index.csv' ) raw_pdi . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Entity Code Year Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) 0 North Atlantic NaN 1951 2.7846 1 North Atlantic NaN 1952 2.3445 2 North Atlantic NaN 1953 2.2639 3 North Atlantic NaN 1954 2.4730 4 North Atlantic NaN 1955 2.4041 raw_pdi . dtypes Entity object Code float64 Year int64 Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA) float64 dtype: object PDI = raw_pdi [[ 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' ]] . rename ( columns = { 'Year' : 'Year' , 'Cyclone Power Dissipation Index (PDI) (HUDRAT, NOAA)' : 'PDI' }) PDI = PDI . set_index ( 'Year' ) fig , axs = plt . subplots ( 2 , 2 , figsize = ( 15 , 9 )) axs [ 0 , 0 ] . set_title ( \"Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 0 ] . set_ylabel ( \"PDI\" , fontsize = 16 ) axs [ 0 , 0 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 0 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 0 ] . plot ( PDI . index , PDI . PDI , lw = 4 ); axs [ 0 , 1 ] . set_title ( \"Logarithm of Power Dissipation Index 1951 - 2013\" , fontsize = 18 ) axs [ 0 , 1 ] . set_ylabel ( \"log(PDI)\" , fontsize = 16 ) axs [ 0 , 1 ] . set_xlabel ( \"Year\" , fontsize = 16 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) axs [ 0 , 1 ] . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) axs [ 0 , 1 ] . plot ( PDI . index , np . log ( PDI . PDI ), lw = 4 ) sns . distplot ( PDI . PDI , ax = axs [ 1 , 0 ]) sns . distplot ( np . log ( PDI . PDI ), ax = axs [ 1 , 1 ]) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ef52a30>","title":"The PDI Dataset"},{"location":"PowerDissipationIndex/#atlantic-mdr-climate-data","text":"Let's do the same smoothing operation in our \"atlantic_mdr\" dataset. We're gonna apply a 1\u20133\u20134\u20133\u20131 weighted average Emanuel, 2007 on the data, to get out of interannual variability. atlantic_mdr = pd . read_csv ( './Datasets/atlantic_mdr.csv' ) def smooth ( col ): n = len ( col ) new_col = np . zeros ([ n , 1 ]) w = np . array ([[ 1 , 3 , 4 , 3 , 1 ]]) for i in range ( 2 , n - 2 ): new_col [ i ] = w . dot ( np . array ( col [ i - 2 : i + 3 ]) . reshape ( - 1 , 1 )) / 12 return new_col . ravel () # atlantic_mdr = atlantic_mdr[(atlantic_mdr.Month>=8) & (atlantic_mdr.Month<=10)] mdr_annual = atlantic_mdr . groupby ( 'Year' ) . agg ({ 'sst' : np . mean , 'rhum' : np . mean , 'wspd' : np . mean , 'slp' : np . mean , 'vwnd' : np . mean , 'cldc' : np . mean }) for col in mdr_annual . columns : mdr_annual . loc [:, col ] = smooth ( mdr_annual [ col ]) mdr_annual = mdr_annual . loc [ 1951 : 2013 ,:] # mdr_annual mdr_annual [ 'PDI' ] = np . array ( PDI . PDI ) . reshape ( - 1 , 1 ) # mdr_annual","title":"Atlantic MDR Climate Data"},{"location":"PowerDissipationIndex/#analysing-correlation","text":"# corr = mdr_annual.corr() # corr.style.background_gradient(cmap='coolwarm') # corr = df.corr() # corr.style.background_gradient(cmap='coolwarm') f , axs = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( mdr_annual . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 0 ]); df = mdr_annual . copy () df [ 'PDI' ] = np . log ( df [ 'PDI' ]) df = df . rename ( columns = { \"PDI\" : \"logPDI\" }) sns . set_context ( \"notebook\" , font_scale = 1.25 , rc = { \"lines.linewidth\" : 2.5 }) sns . heatmap ( df . corr () . round ( 2 ), cmap = 'coolwarm' , annot = True , annot_kws = { \"size\" : 13 }, ax = axs [ 1 ]); plt . show () fig , ax = plt . subplots ( figsize = ( 10 , 6 )) plt . title ( \"North Atlantic MDR SST vs PDI\" , fontsize = 18 ) plt . xlabel ( \"Year\" , fontsize = 14 ) ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . tick_params ( axis = 'both' , which = 'minor' , labelsize = 12 ) ax2 = ax . twinx () ax . plot ( mdr_annual . sst , lw = 4 , label = \"SST\" ); ax . set_ylabel ( \"Sea Surface Temperature\" , fontsize = 14 ) ax2 . plot ( mdr_annual . PDI , lw = 4 , label = \"PDI\" , color = 'orange' ); ax2 . set_ylabel ( \"Power Dissipation Index\" , fontsize = 14 ) fig . legend ( loc = \"upper left\" , fontsize = 14 , bbox_to_anchor = ( 0 , 1 ), bbox_transform = ax . transAxes ); # sns.scatterplot(x=np.log(mdr_annual['sst']), y=np.log(mdr_annual['PDI'])) # sns.scatterplot(x=X.sst, y=mdr_annual['PDI'])","title":"Analysing Correlation"},{"location":"PowerDissipationIndex/#simple-linear-model","text":"Quanto \u00e0 trajet\u00f3ria do PDI, \u00e9 observ\u00e1vel uma leve tend\u00eancia positiva. # Aparentemente o PDI cresceu: X = mdr_annual . drop ( columns = \"PDI\" ) X = sm . add_constant ( X ) Y = np . array ( mdr_annual [ 'PDI' ]) . reshape ( - 1 , 1 ) X [ 'Year' ] = np . array ( X . index ) model = sm . OLS ( Y , sm . add_constant ( X [ 'Year' ])) . fit () plt . plot ( PDI ) plt . plot ( model . fittedvalues ) plt . title ( \"Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta\" ) Text(0.5, 1.0, 'Evolu\u00e7\u00e3o do PDI com o tempo e a tend\u00eancia de crescimento com uma reta') Usando as covari\u00e1veis do dataset, podemos fazer um modelo onde achamos rela\u00e7\u00e3o com a press\u00e3o ao nivel do mar (slp), a humidade (rhum) e a \"cloudin. # X['sst'] = np.log(X['sst']) ( xtrain , xtest , ytrain , ytest ) = train_test_split ( X . drop ( columns = [ \"sst\" , \"wspd\" , \"vwnd\" , \"Year\" ]), np . log ( Y )) model = sm . OLS ( ytrain , xtrain ) . fit () ypred = model . predict ( xtest ) print ( \"R2 nos dados separados para teste:\" , r2_score ( ytest , ypred )) print ( model . summary ()) R2 nos dados separados para teste : 0 . 2268374488496503 OLS Regression Results ============================================================================== Dep . Variable : y R - squared : 0 . 781 Model : OLS Adj . R - squared : 0 . 766 Method : Least Squares F - statistic : 51 . 26 Date : Mon , 31 Aug 2020 Prob ( F - statistic ): 2 . 98 e - 14 Time : 10 : 22 : 52 Log - Likelihood : 14 . 019 No . Observations : 47 AIC : - 20 . 04 Df Residuals : 43 BIC : - 12 . 64 Df Model : 3 Covariance Type : nonrobust ============================================================================== coef std err t P >| t | [ 0 . 025 0 . 975 ] ------------------------------------------------------------------------------ const - 484 . 2535 146 . 318 - 3 . 310 0 . 002 - 779 . 332 - 189 . 175 rhum 0 . 7547 0 . 079 9 . 497 0 . 000 0 . 594 0 . 915 slp 0 . 4106 0 . 143 2 . 871 0 . 006 0 . 122 0 . 699 cldc 1 . 9827 0 . 229 8 . 641 0 . 000 1 . 520 2 . 445 ============================================================================== Omnibus : 4 . 476 Durbin - Watson : 2 . 046 Prob ( Omnibus ): 0 . 107 Jarque - Bera ( JB ): 3 . 413 Skew : - 0 . 468 Prob ( JB ): 0 . 182 Kurtosis : 3 . 932 Cond . No . 5 . 42 e + 06 ============================================================================== Warnings : [ 1 ] Standard Errors assume that the covariance matrix of the errors is correctly specified . [ 2 ] The condition number is large , 5 . 42 e + 06 . This might indicate that there are strong multicollinearity or other numerical problems . Pode parecer um pouco contraintuitivo a princ\u00edpio n\u00e3o achar rela\u00e7\u00e3o significativa da temperatura do mar e da intensidade (na verdade, a destrutibilidade, o PDI) dos furac\u00f5es. Uma explica\u00e7\u00e3o plausivel para isso \u00e9 que os dados de temperatura conseguem ser explicados pelas outras vari\u00e1veis: model = sm . OLS ( X [ 'sst' ], X . drop ( columns = [ 'vwnd' , 'wspd' , 'Year' , 'sst' ])) . fit () print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: sst R-squared: 0.749 Model: OLS Adj. R-squared: 0.736 Method: Least Squares F-statistic: 58.61 Date: Mon, 31 Aug 2020 Prob (F-statistic): 1.08e-17 Time: 10:22:52 Log-Likelihood: 53.106 No. Observations: 63 AIC: -98.21 Df Residuals: 59 BIC: -89.64 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -177.9485 73.852 -2.410 0.019 -325.726 -30.171 rhum 0.2643 0.040 6.528 0.000 0.183 0.345 slp 0.1764 0.072 2.446 0.017 0.032 0.321 cldc 1.3740 0.113 12.204 0.000 1.149 1.599 ============================================================================== Omnibus: 0.296 Durbin-Watson: 0.473 Prob(Omnibus): 0.862 Jarque-Bera (JB): 0.479 Skew: -0.023 Prob(JB): 0.787 Kurtosis: 2.575 Cond. No. 5.52e+06 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 5.52e+06. This might indicate that there are strong multicollinearity or other numerical problems.","title":"Simple Linear Model"},{"location":"PowerDissipationIndex/#time-series-analysis","text":"tsPDI = pd . DataFrame ( Y , index = [ dt . datetime ( x , 1 , 1 ) for x in mdr_annual . index ], columns = [ \"PDI\" ]) tsPDI . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PDI 1951-01-01 2.7846 1952-01-01 2.3445 1953-01-01 2.2639 1954-01-01 2.4730 1955-01-01 2.4041 Vamos tratar agora a trajet\u00f3ria do PDI como uma s\u00e9rie temporal. Inicialmente, vamos ajustar um modelo autorregressivo de ordem 2: mod = sm . tsa . ARIMA ( tsPDI , order = ( 2 , 0 , 0 )) modfit = mod . fit () # modfit.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit.resid, ax = axs[0, 0]) # plot_pacf(modfit.resid, ax = axs[0, 1]) # sm.qqplot(modfit.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit.resid, ax = axs[1, 1]) # plt.show()# modfit.plot_diagnostics() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) axs [ 0 ] . set_title ( 'M\u00e9dia das previs\u00f5es com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () Modelando o log da s\u00e9rie: tsLogPDI = tsPDI . apply ( np . log ) #sm.tsa.stattools.adfuller(tsLogPDI['PDI']) ainda n\u00e3o \u00e9 estacionaria a 5% de significancia mod2 = sm . tsa . ARIMA ( tsLogPDI , order = ( 2 , 0 , 0 )) modfit2 = mod2 . fit () # modfit2.summary() /home/gambitura/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency AS-JAN will be used. warnings.warn('No frequency information was' # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit2.resid, ax = axs[0, 0]) # plot_pacf(modfit2.resid, ax = axs[0, 1]) # sm.qqplot(modfit2.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit2.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit2 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: sims = np . zeros (( 1000 , step )) for i in range ( 1000 ): sims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( sims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 30 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 15 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show () # plt.plot(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) # sm.tsa.stattools.adfuller(np.log(np.diff(np.log(mdr_annual['PDI'])) + 1)) tsTr = np . log ( np . diff ( np . log ( tsPDI [ 'PDI' ])) + 1 ) # terceiro modelo: log(delta(log(PDI))+1) como ARMA(2,1) mod3 = sm . tsa . ARIMA ( tsTr , order = ( 2 , 0 , 1 )) modfit3 = mod3 . fit () # modfit3.summary() # fig, axs = plt.subplots(2,2, figsize = (10,4)) # plot_acf(modfit3.resid, ax = axs[0, 0]) # plot_pacf(modfit3.resid, ax = axs[0, 1]) # sm.qqplot(modfit3.resid, dist=stats.norm, line=\"s\", ax = axs[1, 0]) # sns.distplot(modfit3.resid, ax = axs[1, 1]) # plt.show() fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsTr ) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = range ( len ( tsTr ), len ( tsTr ) + step ) axs [ 0 ] . plot ( x , forecast [ 0 ]) axs [ 0 ] . fill_between ( x , forecast [ 2 ][:, 0 ], forecast [ 2 ][:, 1 ], color = 'k' , alpha =. 2 ) #simulating some cenarios: sims = np . zeros (( 500 , step )) for i in range ( 500 ): sims [ i ] = np . random . normal ( forecast [ 0 ], forecast [ 1 ]) axs [ 1 ] . plot ( tsTr ) for i in range ( 40 ): axs [ 1 ] . plot ( x , sims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia da transforma\u00e7\u00e3o predita com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es da transforma\u00e7\u00e3o' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsLogPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = last + np . cumsum ( dsims [ i ]) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsLogPDI [ 'PDI' ]) for i in range ( 50 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do log(PDI) predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do log(PDI)' ) plt . show () fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) # With CI: axs [ 0 ] . plot ( tsPDI [ 'PDI' ]) # plt.plot(tsPDI['PDI'], axs = axs[0]) step = 10 forecast = modfit3 . forecast ( step ) x = pd . date_range ( dt . datetime ( 2013 , 1 , 1 ), freq = \"A\" , periods = step ) #simulating some cenarios: dsims = np . zeros (( 1000 , step )) for i in range ( 1000 ): dsims [ i ] = np . exp ( np . random . normal ( forecast [ 0 ], forecast [ 1 ])) - 1 last = tsLogPDI [ \"PDI\" ] . values [ - 1 ] for i in range ( len ( dsims )): dsims [ i ] = np . exp ( last + np . cumsum ( dsims [ i ])) # computing some CIs: CIs = np . zeros (( step , 3 )) for i in range ( step ): CIs [ i ] = np . quantile ( dsims [:, i ], [ 0.025 , 0.5 , 0.975 ]) axs [ 0 ] . plot ( x , CIs [:, 1 ]) axs [ 0 ] . fill_between ( x , CIs [:, 0 ], CIs [:, 2 ], color = 'k' , alpha =. 2 ) axs [ 1 ] . plot ( tsPDI [ 'PDI' ]) for i in range ( 40 ): axs [ 1 ] . plot ( x , dsims [ i ], color = \"k\" , alpha =. 1 ) #teste de sanidade axs [ 0 ] . set_title ( 'M\u00e9dia do PDI predito com intervalo de confian\u00e7a (a = 0.05)' ) axs [ 1 ] . set_title ( 'Simula\u00e7\u00f5es do PDI' ) plt . show ()","title":"Time series Analysis"},{"location":"PowerDissipationIndex/#count","text":"Medir destrutibilidade do furac\u00e3o n\u00e3o \u00e9 coisa trivial; n\u00e3o basta olhar o PDI visto que os nossos dados de PDI podem estar errados por medi\u00e7\u00f5es inconsistentes de velocidade at\u00e9 os anos 90, onde come\u00e7amos a ter um comportamento an\u00f4malo do ENSO; al\u00e9m da din\u00e2mica sazonal de correntes termosal\u00ednicas gerarem sazonalidade nas temperaturas entre dec\u00e1das (Atlantic Multidecadal Oscilation, AMO). Outras alternativas comumente consideradas s\u00e3o a de contagem de furac\u00f5es no ano e de tamanho de cada furac\u00e3o; cada uma delas com seus problemas particulares. Abaixo, fazemos um modelo simples de GLM para contagem usando como preditores as covari data = pd . read_csv ( 'Datasets/data_atl_merged2.csv' , parse_dates = [ 'Date' ]) data = data [[ 'ID' , 'Name' , 'Date' , 'Year' , 'Time' , 'Event' , 'Status' , 'Latitude' , 'Longitude' , 'Maximum Wind' , 'sst' , 'rhum' , 'wspd' , 'slp' , 'cldc' ]] def category ( mw ): if mw >= 137 : return \"Category 5\" elif mw >= 113 : return \"Category 4\" elif mw >= 96 : return \"Category 3\" elif mw >= 83 : return \"Category 2\" elif mw >= 64 : return \"Category 1\" elif mw >= 34 : return \"Tropical Storm\" else : return \"Tropical Depression\" cat_id = data . groupby ( 'ID' )[ 'Maximum Wind' ] . max () . apply ( category ) data . loc [:, 'Category' ] = data . ID . apply ( lambda x : cat_id [ x ]) # data.groupby('Category')['ID'].count().plot.bar(); major_df = data [ data . Category . apply ( lambda x : 0 if x [ - 1 ] == 'm' else int ( x [ - 1 ])) >= 3 ] major_df . groupby ( 'Year' )[ 'ID' ] . count () . plot () #sm.tsa.stattools.adfuller(count) <matplotlib.axes._subplots.AxesSubplot at 0x7f0a8ec16a60> count = major_df . groupby ( \"Year\" )[ \"ID\" ] . count () X = pd . pivot_table ( major_df , index = \"Year\" , values = [ \"sst\" , \"rhum\" , \"wspd\" , \"slp\" , \"cldc\" ]) X [ 'Y' ] = X . index . values countModel = sm . GLM ( count , sm . add_constant ( X ), family = sm . families . NegativeBinomial ()) countFit = countModel . fit () print ( countFit . summary ()) #descomente para ver o sumario do ajuste plt . plot ( count ) plt . plot ( countFit . predict ( sm . add_constant ( X ))) plt . show () Generalized Linear Model Regression Results ============================================================================== Dep. Variable: ID No. Observations: 60 Model: GLM Df Residuals: 54 Model Family: NegativeBinomial Df Model: 5 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -347.61 Date: Mon, 31 Aug 2020 Deviance: 26.626 Time: 10:22:58 Pearson chi2: 26.3 No. Iterations: 12 Covariance Type: nonrobust ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const -146.2316 147.049 -0.994 0.320 -434.443 141.980 cldc 0.1977 0.589 0.336 0.737 -0.956 1.351 rhum 0.0182 0.067 0.271 0.786 -0.113 0.150 slp 0.1338 0.142 0.940 0.347 -0.145 0.413 sst 0.0952 0.132 0.723 0.470 -0.163 0.353 wspd 0.0182 0.067 0.271 0.786 -0.113 0.150 Y 0.0048 0.008 0.602 0.547 -0.011 0.020 ==============================================================================","title":"Count"}]}